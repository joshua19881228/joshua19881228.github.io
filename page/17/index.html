<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Do not aim for success if you want it; just do what you love and believe in, and it will come naturally.">
<meta property="og:type" content="website">
<meta property="og:title" content="Joshua&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/page/17/index.html">
<meta property="og:site_name" content="Joshua&#39;s Blog">
<meta property="og:description" content="Do not aim for success if you want it; just do what you love and believe in, and it will come naturally.">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Joshua LI">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/page/17/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Joshua's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Joshua's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-my-jumble-of-computer-vision">

    <a href="/2016/08/25/Computer_Vision/2016-08-25-my-jumble-of-computer-vision/" rel="section"><i class=" fa-fw"></i>My Jumble of Computer Vision</a>

  </li>
        <li class="menu-item menu-item-an-introduction-to-cnn-based-object-detection">

    <a href="/2017/06/13/Computer_Vision/2017-06-13-An-Introduction-to-CNN-based-Object-Detection/" rel="section"><i class=" fa-fw"></i>An Introduction to CNN based Object Detection</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/09/23/Life_Discovery/Miscellaneous/2016-09-23-Miscellaneous/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar-icon.png">
      <meta itemprop="name" content="Joshua LI">
      <meta itemprop="description" content="Do not aim for success if you want it; just do what you love and believe in, and it will come naturally.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Joshua's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/09/23/Life_Discovery/Miscellaneous/2016-09-23-Miscellaneous/" class="post-title-link" itemprop="url">8 Reasons People Who Like Spending Time Alone Are Smarter and Stronger</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-09-23 00:00:00" itemprop="dateCreated datePublished" datetime="2016-09-23T00:00:00+08:00">2016-09-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-08-19 17:11:04" itemprop="dateModified" datetime="2022-08-19T17:11:04+08:00">2022-08-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Life-Discovery/" itemprop="url" rel="index"><span itemprop="name">Life Discovery</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Today I read a very interesting essay about solitude. The ideas of the essay are quite appealing for me because I myself prefer to be alone rather than hanging out with people. Sometimes I even prefer to travel alone without any companions, though it is not a wise choice. The essay can be found <a href="http://www.lifehack.org/453521/8-reasons-people-who-like-spending-time-alone-are-smarter-and-stronger" target="_blank" rel="noopener">here</a>.</p>
<blockquote>
<p>Today’s society encourages us to spend as much time as possible with other people; even when we are alone, we are texting, emailing, phoning, and Skyping each other. We are always expected to be doing something or going somewhere, but there are actually lots of benefits to spending time alone.</p>
<p>Solitude isn’t very popular in our constantly connected world, and often people who spend time alone are assumed to be lonely or sad. However, this is rarely the case; lots people enjoy spending time alone because it benefits them psychologically. Spending time alone is actually good for us, and it gives us the chance to relax and recharge.</p>
<ol>
<li><p>Spending Time Alone Will Make You More Confident</p>
<p>Unconfident people often rely on others to help with decisions, but spending time alone encourages you to make decisions for yourself. When you are alone, you can ignore other people’s opinions and ideas so that you can really focus on your own thoughts. You get the opportunity to weigh up all of the pros and cons so that you make the best possible decision, which helps to inspire confidence within ourselves.</p>
</li>
<li><p>Spending Time Alone Will Boost Your Productivity</p>
<p>When we are around other people, we often become distracted from our goals and priorities. When we are alone, we get the chance to really think about what matters to us, from work to family to money. This helps us to decide our goals and it also motivates us to work towards achieving our goals.</p>
</li>
<li><p>Spending Time Alone Helps You to be Creative</p>
<p>Everyone is creative in different ways, but when we are around other people, we are more likely to do what the rest of the group is doing. When we are alone, outside influences are removed and we can do exactly as we please. Some people enjoy drawing or painting, and others might enjoy cooking, reading, writing, or making music. There are lots of different ways to be creative, and when we are alone we get the chance to explore our individual interests and abilities.</p>
</li>
<li><p>Spending Time Alone Will Clear Your Mind</p>
<p>Our society is filled with information and we often overload on the endless stream of information coming from work, social media, and our friends and family. Sometimes it is essential to take a break from the stream of information so that we can think about our lives and assess everything that is happening. Spending time alone allows us to clear our minds, which makes us happier and more relaxed.</p>
</li>
<li><p>Spending Time Alone Will Help You to Solve Problems</p>
<p>The best solutions often come to us when we are alone and reflecting on our problems. Spending time alone helps us to work through a problem as we get the chance to really think about it. We can think about what caused the problem, as well as all the different ways we can do to solve the problem. We also get the opportunity to think about what we really want, which helps us to think of effective solutions.</p>
</li>
<li><p>Spending Time Alone Will Help You to Get Things Done</p>
<p>Most people have a list of things that they need to do, but it is difficult to tick anything off the list when you are always with people. When you are alone, you get uninterrupted time to work on your to-do list, and we often achieve a lot more when we are alone as there are no distractions. Even if it isn’t fun to work through the list, you will feel happy and productive when you finish working.</p>
</li>
<li><p>Spending Time Alone Relieves Stress and Anxiety</p>
<p>Lots of people today suffer from stress and anxiety, and spending time alone helps us to de-stress and relax. When we are alone we don’t have to listen to other people’s problems or issues; we can just relax and do as we please.</p>
</li>
<li><p>Spending Time Alone Encourages You to be More Independent</p>
<p>One of the main benefits to spending time alone is that it inspires us to be more independent. When we are alone we can focus on personal progress, problem solving, and enjoying ourselves, which all encourage independence and self-love.</p>
</li>
</ol>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/09/20/Using_Linux/2016-09-20-Note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar-icon.png">
      <meta itemprop="name" content="Joshua LI">
      <meta itemprop="description" content="Do not aim for success if you want it; just do what you love and believe in, and it will come naturally.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Joshua's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/09/20/Using_Linux/2016-09-20-Note/" class="post-title-link" itemprop="url">Some Note of Setting up Caffe</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-09-20 00:00:00" itemprop="dateCreated datePublished" datetime="2016-09-20T00:00:00+08:00">2016-09-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-08-19 17:11:04" itemprop="dateModified" datetime="2022-08-19T17:11:04+08:00">2022-08-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Using-Linux/" itemprop="url" rel="index"><span itemprop="name">Using Linux</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Previously I have written some <a href="https://joshua19881228.github.io/2015-08-30-Setting-Up-Caffe/" target="_blank" rel="noopener">notes</a> of how to set up environment for <a href="http://caffe.berkeleyvision.org/" target="_blank" rel="noopener">Caffe</a> on Ubuntu 14.04. My strategy is DIY. Though the methods mentioned in those notes worked well at that time, they may be out of date now. Since I am quite a rookie in Linux OS, I need this additional note to bypass the problems I met recently.</p>
<p><strong>Concerning setting link directories</strong></p>
<p>New a configuration file in the directory of <code>/etc/ld.so.conf.d/</code> and the file name must be end up with <code>.conf</code>. In the file is the directories that need to be added to the link path. Finally use command <code>sudo ldconfig</code> to make it effective. Here is an example to add link path of OpenBLAS to the environment. A new file named as <code>openblas.conf</code> is created and saved in <code>/etc/ld.so.conf.d/</code>. In the file a path is written <code>/home/joshua/LIBS/openblas/lib</code>. Then type <code>sudo ldconfig</code> in a terminal window.</p>
<p><strong>Concerning installing NVIDIA GPU drivers</strong></p>
<p>A very easy way to install NVIDIA GPU drivers is using the tools called “Software&amp;Updates” in “System Settings” of Ubuntu. In “Software&amp;Updates” there is a tag “Additional Drivers”, which can be used to select possible GPU drivers. After reboot the new settings can take effect.</p>
<p><strong>Concerning making Caffe</strong></p>
<p>After <code>sudo apt-get update</code> and <code>sudo apt-get upgrade</code>, I found that Caffe can not make. The error came from TIFF. I solved the problem by <code>conda remove libtiff</code>.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/09/08/Life_Discovery/Miscellaneous/2016-09-08-Miscellaneous/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar-icon.png">
      <meta itemprop="name" content="Joshua LI">
      <meta itemprop="description" content="Do not aim for success if you want it; just do what you love and believe in, and it will come naturally.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Joshua's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/09/08/Life_Discovery/Miscellaneous/2016-09-08-Miscellaneous/" class="post-title-link" itemprop="url">读《解忧杂货店》有感</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-09-08 00:00:00" itemprop="dateCreated datePublished" datetime="2016-09-08T00:00:00+08:00">2016-09-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-08-19 17:11:04" itemprop="dateModified" datetime="2022-08-19T17:11:04+08:00">2022-08-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Life-Discovery/" itemprop="url" rel="index"><span itemprop="name">Life Discovery</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>东野圭吾的《解忧杂货店》里有这样一段话：</p>
<blockquote>
<p>不管是骚扰还是恶作剧，写这些信给浪矢杂货店的人，和普通的咨询者在本质上是一样的。他们都是内心破了一个洞，重要的东西正从那个破洞逐渐流失。证据就是，这样的人也一定会来拿回信，他会来查看牛奶箱。因为他很想知道，浪矢爷爷会怎样回复自己的信。你想想看，就算是瞎编的烦恼，要一口气想出三十个也不简单。既然费了这么多心思，怎么可能不想知道答案？所以我不但要写回信，而且要好好思考后再写。人的心声是绝对不能无视的。</p>
</blockquote>
<p>一直被这最后一句话打动着，“人的心声是绝对不能无视的”，我们绝对应该好好倾听一下自己的心声，问问自己到底想要些什么。这一点可能特别困难，因为就像话剧《李白》里，当人生如意、仕途坦荡时，诗人外罩锦袍，内套道袍，写出的诗句是“人生得意须尽欢，莫使金樽空对月。天生我材必有用，千金散尽还复来”。相反当入世不顺、无奈被贬之时，诗人则道袍在外，锦袍与内，写出的诗句是“君不见吴中张翰称达生，秋风忽忆江东行。且乐生前一杯酒，何须身后千载名？”。都是喝酒但是，一个是踌躇满志的得意酒，一个是悻悻作罢的失意酒。连诗仙李白都是这么一个“俗人”，更何况我们这些芸芸呢。找到自己的内心真的很难，有多少时候我们有了小小的成就，便发出一种“世界全是我的”的感叹，而遇到些许困难，又是一副悲天悯人的模样。不知道浪矢爷爷说的那个洞，到底能不能真的被填上。真希望有个浪矢爷爷来帮我们答疑解惑，</p>
<blockquote>
<p>很多时候，咨询的人心里已经有了答案，来咨询只是想确认自己的决定是对的。所以有些人读过回信后，会再次写信过来，大概就是因为回答的内容和他的想法不一样吧。</p>
</blockquote>
<p>说得太对了，当我们挣扎着做了一个决定时，总是希望别人也有同样的想法，并给予自己支持。现实也的确如此，当我们已经做了一个决定时，我们再去咨询别人，别人大多也会顺着我们的决心说，不管是鼓励还是安慰，总是要说一些顺耳的话。但也许我们的决定并不一定是正确的，那些给予我们正确意见的声音，被我们选择性的过滤掉了。我们到底该听谁的意见呢，这可太难了，我们想顺着自己的心意行事，但是又害怕做了错事，别人真的那么在乎我们的烦恼吗？他们的意见一定正确吗？我们都不知道。因此，获取我们真的是应该在做决定时抛一枚硬币，就按照我们期望的那一面去做吧。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/09/06/Computer_Vision/Reading_Note/2016-09-06-Reading-note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar-icon.png">
      <meta itemprop="name" content="Joshua LI">
      <meta itemprop="description" content="Do not aim for success if you want it; just do what you love and believe in, and it will come naturally.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Joshua's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/09/06/Computer_Vision/Reading_Note/2016-09-06-Reading-note/" class="post-title-link" itemprop="url">What makes ImageNet good for transfer learning?</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-09-06 00:00:00" itemprop="dateCreated datePublished" datetime="2016-09-06T00:00:00+08:00">2016-09-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-08-19 17:11:04" itemprop="dateModified" datetime="2022-08-19T17:11:04+08:00">2022-08-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Computer-Vision/" itemprop="url" rel="index"><span itemprop="name">Computer Vision</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>TITLE</strong>: What makes ImageNet good for transfer learning?</p>
<p><strong>AUTHOR</strong>: Minyoung Huh, Pulkit Agrawal, Alexei A. Efros</p>
<p><strong>ASSOCIATION</strong>: Berkeley Artificial Intelligence Research (BAIR) Laboratory, UC Berkeley</p>
<p><strong>FROM</strong>: <a href="http://arxiv.org/abs/1608.08614" target="_blank" rel="noopener">http://arxiv.org/abs/1608.08614</a></p>
<h3 id="CONTRIBUTIONS"><a href="#CONTRIBUTIONS" class="headerlink" title="CONTRIBUTIONS"></a>CONTRIBUTIONS</h3><p>Several questions about how the dataset affects the training of CNN is discussed, including</p>
<ul>
<li>Is more pre-training data always better? How does feature quality depend on the number of training examples per class? </li>
<li>Does adding more object classes improve performance? </li>
<li>For the same data budget, how should the data be split into classes?</li>
<li>Is fine-grained recognition necessary for learning good features?</li>
<li>Given the same number of training classes, is it better to have coarse classes or fine-grained classes? </li>
<li>Which is better: more classes or more examples per class?</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><blockquote>
<p>The following is a summary of the main findings:</p>
<ol>
<li><strong>How many pre-training ImageNet examples are sufficient for transfer learning?</strong> Pre-training with only half the ImageNet data (500 images per class instead of 1000)results in only a small drop in transfer learning performance (1.5 mAP drop on PASCAL-DET). This drop is much smaller than the drop on the ImageNet classification task itself.</li>
<li><strong>How many pre-training ImageNet classes are sufficient for transfer learning?</strong> Pre-training with an order of magnitude fewer classes (127 classes instead of 1000) results in only a small drop in transfer learning performance (drop of 2.8 mAP on PASCAL-DET). Quite interestingly, we also found that for some transfer tasks, pre-training with fewer number of classes leads to better performance.</li>
<li><strong>How important is fine-grained recognition for learning good features for transfer learning?</strong> The above experiment also suggests that transferable features are learnt even when a CNN is pre-trained with a set of classes that do not require fine-grained discrimination.</li>
<li><strong>Given the same budget of pre-training images, should we have more classes or more images per class?</strong> Training with fewer classes but more images per class performs slightly better than training with more classes but fewer images per class.</li>
<li><strong>Is more data always helpful?</strong> We found that training using 771 ImageNet classes that excludes all PASCAL VOC classes, achieves nearly the same performance on PASCALDET as training on complete ImageNet. Further experiments confirm that blindly adding more training data does not always lead to better performance and can sometimes hurt performance.</li>
</ol>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/08/30/Computer_Vision/Reading_Note/2016-08-30-Reading-note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar-icon.png">
      <meta itemprop="name" content="Joshua LI">
      <meta itemprop="description" content="Do not aim for success if you want it; just do what you love and believe in, and it will come naturally.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Joshua's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/08/30/Computer_Vision/Reading_Note/2016-08-30-Reading-note/" class="post-title-link" itemprop="url">PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-08-30 00:00:00" itemprop="dateCreated datePublished" datetime="2016-08-30T00:00:00+08:00">2016-08-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-08-19 17:11:04" itemprop="dateModified" datetime="2022-08-19T17:11:04+08:00">2022-08-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Computer-Vision/" itemprop="url" rel="index"><span itemprop="name">Computer Vision</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>TITLE</strong>: PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection</p>
<p><strong>AUTHER</strong>: Kye-Hyeon Kim, Yeongjae Cheon, Sanghoon Hong, Byungseok Roh, Minje Park</p>
<p><strong>ASSOCIATION</strong>: Intel Imaging and Camera Technology</p>
<p><strong>FROM</strong>: <a href="http://arxiv.org/abs/1608.08021" target="_blank" rel="noopener">arXiv:1608.08021</a></p>
<h3 id="CONTRIBUTIONS"><a href="#CONTRIBUTIONS" class="headerlink" title="CONTRIBUTIONS"></a>CONTRIBUTIONS</h3><p>An efficient object detector based on CNN is proposed, which has the following advantages:</p>
<ul>
<li>Computational cost: 7.9GMAC for feature extraction with 1065x640 input (cf. ResNet-101: 80.5GMAC1)</li>
<li>Runtime performance: 750ms/image (1.3FPS) on Intel i7-6700K CPU with a single core; 46ms/image (21.7FPS) on NVIDIA Titan X GPU</li>
<li>Accuracy: 81.8% mAP on VOC-2007; 82.5% mAP on VOC-2012 (2nd place)</li>
</ul>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>The author utilizes the pipline of Faster-RCNN, which is “CNN feature extraction + region proposal + RoI classification”. The author claims that feature extraction part needs to be redesigned, since region proposal part is not computationally expensive and classification part can be efficiently compressed with common techniques like truncated SVD. And the principle is “less channels with more layers” and adoption of some building blocks including concatenated ReLU, Inception, and HyperNet. The structure of the network is as follows:</p>
<p><img class="img-responsive center-block" src="https://raw.githubusercontent.com/joshua19881228/my_blogs/master/Computer_Vision/Reading_Note/figures/structure.jpg" alt="" width="640"/></p>
<p><strong>Some Details</strong></p>
<ol>
<li>Concatenated rectified linear unit (C.ReLU) is applied to the early stage of the CNNs (i.e., first several layers from the network input) to reduce the number of computations by half without losing accuracy. In my understanding, the C.ReLU encourages the network to learn Gabor-like filters and helps to accelerate the forward-propagation. If the output of the C.ReLu is 64, its convolution layer only needs 32-channel outputs. And it may harm the performance if it is used to the later stage of the CNNs, because it keeps the negative responses as activated signal, which means that a mad brain is trained.</li>
<li>Inception is applied to the remaining of the feature generation sub-network. An Inception module produces output activations of different sizes of receptive fields, so that increases the variety of receptive field sizes in the previous layer. All the design policies can be found in this <a href="http://joshua881228.webfactional.com/blog_reading-note-rethinking-the-inception-architecture-for-computer-vision_136/" target="_blank" rel="noopener">related work</a>.</li>
<li>The author adopted the idea of multi-scale representation like HyperNet that combines several intermediate outputs so that multiple levels of details and non-linearities can be considered simultaneously. Direct concatenation of all abstraction layers may produce redundant information with much higher compute requirement and layers which are too early for object proposal and classification would be little help. The author combines 1) the last layer and 2) two intermediate layers whose scales are 2x and 4x of the last layer, respectively.</li>
<li>Residual structure is also used in this network, which helps to train very deep CNNs.</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/08/25/Computer_Vision/2016-08-25-my-jumble-of-computer-vision/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar-icon.png">
      <meta itemprop="name" content="Joshua LI">
      <meta itemprop="description" content="Do not aim for success if you want it; just do what you love and believe in, and it will come naturally.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Joshua's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/08/25/Computer_Vision/2016-08-25-my-jumble-of-computer-vision/" class="post-title-link" itemprop="url">My Jumble of Computer Vision</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-08-25 00:00:00" itemprop="dateCreated datePublished" datetime="2016-08-25T00:00:00+08:00">2016-08-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-08-19 17:11:04" itemprop="dateModified" datetime="2022-08-19T17:11:04+08:00">2022-08-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Computer-Vision/" itemprop="url" rel="index"><span itemprop="name">Computer Vision</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>I am going to maintain this page to record a few things about computer vision that I have read, am doing, or will have a look at. Previously I’d like to write short notes of the papers that I have read. It is a good way to remember and understand the ideas of the authors. But gradually I found that I forget much portion of what I had learnt because in addition to paper I also derive knowledges from others’ blogs, online courses and reports, not recording them at all. Besides, I need a place to keep a list of what I should have a look at but do not at the time when I discover them. This page will be much like a catalog.</p>
<h2 id="PAPERS-AND-PROJECTS"><a href="#PAPERS-AND-PROJECTS" class="headerlink" title="PAPERS AND PROJECTS"></a>PAPERS AND PROJECTS</h2><h3 id="OBJECT-SALIENCY-DETECTION"><a href="#OBJECT-SALIENCY-DETECTION" class="headerlink" title="OBJECT/SALIENCY DETECTION"></a>OBJECT/SALIENCY DETECTION</h3><ul>
<li>EfficientDet: Scalable and Efficient Object Detection (<a href="https://arxiv.org/abs/1911.09070" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/google/automl/tree/master/efficientdet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>YOLOv4: Optimal Speed and Accuracy of Object Detection (<a href="https://arxiv.org/abs/2004.10934" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/AlexeyAB/darknet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Learning Data Augmentation Strategies for Object Detection (<a href="https://arxiv.org/abs/1906.11172" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/tensorflow/tpu/tree/master/models/official/detection" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Light-Weight RetinaNet for Object Detection (<a href="https://arxiv.org/abs/1905.10011" target="_blank" rel="noopener">PDF</a>)</li>
<li>Objects as Points (<a href="https://arxiv.org/abs/1904.07850" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/xingyizhou/CenterNet" target="_blank" rel="noopener">Code/Projects</a>)</li>
<li>Augmentation for small object detection (<a href="https://arxiv.org/abs/1902.07296" target="_blank" rel="noopener">PDF</a>)</li>
<li>ThunderNet: Towards Real-time Generic Object Detection (<a href="https://arxiv.org/abs/1903.11752" target="_blank" rel="noopener">PDF</a>)</li>
<li>Pyramid Mask Text Detector (<a href="https://arxiv.org/abs/1903.11800" target="_blank" rel="noopener">PDF</a>)</li>
<li>Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving (<a href="https://arxiv.org/abs/1904.04620" target="_blank" rel="noopener">PDF</a>)</li>
<li>CornerNet: Detecting Objects as Paired Keypoints (<a href="https://arxiv.org/abs/1808.01244" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/princeton-vl/CornerNet" target="_blank" rel="noopener">Code/Project</a>, <a href="https://joshua19881228.github.io/2019-01-20-CornerNet/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Scale-Aware Trident Networks for Object Detection (<a href="https://arxiv.org/abs/1901.01892" target="_blank" rel="noopener">PDF</a>)</li>
<li>Acquisition of Localization Confidence for Accurate Object Detectinon (<a href="https://arxiv.org/abs/1807.11590" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/vacancy/PreciseRoIPooling" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>A Single Shot Text Detector with Scale-adaptive Anchors (<a href="https://arxiv.org/abs/1807.01884" target="_blank" rel="noopener">PDF</a>)</li>
<li>Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation (<a href="https://arxiv.org/abs/1807.01438" target="_blank" rel="noopener">PDF</a>)</li>
<li>Object detection at 200 Frames Per Second (<a href="https://arxiv.org/abs/1805.06361" target="_blank" rel="noopener">PDF</a>, )</li>
<li>DetNet: A Backbone network for Object Detection (<a href="https://arxiv.org/abs/1804.06215" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2018-05-01-DetNet/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Zero-Shot Object Detection (<a href="https://arxiv.org/abs/1804.04340" target="_blank" rel="noopener">PDF</a>)</li>
<li>Unsupervised Discovery of Object Landmarks as Structural Representations (<a href="https://arxiv.org/abs/1804.04412" target="_blank" rel="noopener">PDF</a>, <a href="http://www.ytzhang.net/projects/lmdis-rep/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Cascade R-CNN: Delving into High Quality Object Detection (<a href="https://arxiv.org/abs/1712.00726" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/zhaoweicai/cascade-rcnn" target="_blank" rel="noopener">PROJECT/CODE</a>)</li>
<li>Path Aggregation Network for Instance Segmentation (<a href="https://arxiv.org/abs/1803.01534" target="_blank" rel="noopener">PDF</a>)</li>
<li>ClickBAIT-v2: Training an Object Detector in Real-Time (<a href="https://arxiv.org/abs/1803.10358" target="_blank" rel="noopener">PDF</a>)</li>
<li>Single-Shot Bidirectional Pyramid Networks for High-Quality Object Detection (<a href="https://arxiv.org/abs/1803.08208" target="_blank" rel="noopener">PDF</a>)</li>
<li>Complex-YOLO: Real-time 3D Object Detection on Point Clouds (<a href="https://arxiv.org/abs/1803.06199" target="_blank" rel="noopener">PDF</a>)</li>
<li>Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concepts (<a href="https://arxiv.org/abs/1803.06049" target="_blank" rel="noopener">PDF</a>)</li>
<li>Domain Adaptive Faster R-CNN for Object Detection in the Wild (<a href="https://arxiv.org/abs/1803.03243" target="_blank" rel="noopener">PDF</a>)</li>
<li>Chinese Text in the Wild (<a href="https://arxiv.org/abs/1803.00085" target="_blank" rel="noopener">PDF</a>, <a href="https://ctwdataset.github.io/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>TSSD: Temporal Single-Shot Detector Based on Attention and LSTM for Robotic Intelligent Perception (<a href="https://arxiv.org/abs/1803.00197" target="_blank" rel="noopener">PDF</a>)</li>
<li>Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for Real-time Embedded Object Detection (<a href="https://arxiv.org/abs/1802.06488" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2018-03-04-Tiny_SSD/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Object Detection in Videos by Short and Long Range Object Linking (<a href="https://arxiv.org/abs/1801.09823" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning a Rotation Invariant Detector with Rotatable Bounding Box (<a href="https://arxiv.org/abs/1711.09405" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com//liulei01/DRBox" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Detecting Curve Text in the Wild: New Dataset and New Solution (<a href="https://arxiv.org/abs/1712.02170" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com//Yuliang-Liu/Curve-Text-Detector#curve-text-detector" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Single Shot Text Detector with Regional Attention (<a href="https://arxiv.org/abs/1709.00138" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com//BestSonny/SSTD" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Single-Shot Refinement Neural Network for Object Detection (<a href="https://arxiv.org/abs/1711.06897" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com//sfzhang15/RefineDet" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2018-01-10-Single-Shot-Refinement-Neural-Network/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>$S^3$FD: Single Shot Scale-invariant Face Detector (<a href="https://arxiv.org/abs/1708.05237" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/sfzhang15/SFD" target="_blank" rel="noopener">Code/Project</a>, <a href="https://joshua19881228.github.io/2018-01-20-S-3_Face_Detector/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>MegDet: A Large Mini-Batch Object Detector (<a href="https://arxiv.org/abs/1711.07240" target="_blank" rel="noopener">PDF</a>)</li>
<li>Light-Head R-CNN: In Defense of Two-Stage Object Detector (<a href="https://arxiv.org/abs/1711.07264" target="_blank" rel="noopener">PDF</a>)</li>
<li>Interpretable R-CNN (<a href="https://arxiv.org/abs/1711.05226" target="_blank" rel="noopener">PDF</a>)</li>
<li>Cascade Region Proposal and Global Context for Deep Object Detection (<a href="https://arxiv.org/abs/1710.10749" target="_blank" rel="noopener">PDF</a>)</li>
<li>PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection (<a href="http://arxiv.org/abs/1608.08021" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/sanghoon/pva-faster-rcnn" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2016-08-30-Reading-note/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks (<a href="http://arxiv.org/abs/1512.04143" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-06-01-ION/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Object Detection from Video Tubelets with Convolutional Neural Networks (<a href="http://arxiv.org/abs/1604.04053" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-05-25-TCNN/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>R-FCN: Object Detection via Region-based Fully Convolutional Networks (<a href="http://arxiv.org/abs/1605.06409" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/daijifeng001/r-fcn" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2016-05-23-RFCN/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>SSD: Single Shot MultiBox Detector (<a href="http://arxiv.org/abs/1512.02325v2" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2016-04-26-SSD/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Pushing the Limits of Deep CNNs for Pedestrian Detection (<a href="http://lib-arxiv-008.serverfarm.cornell.edu/abs/1603.04525" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-03-29-CFM/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Object Detection by Labeling Superpixels(<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/ext/3B_072_ext.pdf" target="_blank" rel="noopener">PDF</a>, <a href="http://joshua881228.webfactional.com/blog_reading-note-object-detection-by-labeling-superpixels_74/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Crafting GBD-Net for Object Detection (<a href="https://arxiv.org/abs/1610.02579" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/craftGBD/craftGBD" target="_blank" rel="noopener">Projct/Code</a>)<br>code for CUImage and CUVideo, the object detection champion of ImageNet 2016.</li>
<li>Fused DNN: A deep neural network fusion approach to fast and robust pedestrian detection (<a href="https://arxiv.org/abs/1610.03466" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-11-21-FusedDNN/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Training Region-based Object Detectors with Online Hard Example Mining (<a href="https://arxiv.org/abs/1604.03540" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-11-03-OHEM/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Detecting People in Artwork with CNNs (<a href="https://arxiv.org/abs/1610.08871" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/BathVisArtData/PeopleArt" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Deeply supervised salient object detection with short connections (<a href="https://arxiv.org/abs/1611.04849" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning to detect and localize many objects from few examples (<a href="https://arxiv.org/abs/1611.05664" target="_blank" rel="noopener">PDF</a>)</li>
<li>Multi-Scale Saliency Detection using Dictionary Learning (<a href="https://arxiv.org/abs/1611.06307" target="_blank" rel="noopener">PDF</a>)</li>
<li>Straight to Shapes: Real-time Detection of Encoded Shapes (<a href="https://arxiv.org/abs/1611.07932" target="_blank" rel="noopener">PDF</a>)</li>
<li>Weakly Supervised Cascaded Convolutional Networks (<a href="https://arxiv.org/abs/1611.08258" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-12-06-WCCN/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Speed/accuracy trade-offs for modern convolutional object detectors (<a href="https://arxiv.org/abs/1611.10012" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-12-02-DetectorCompare/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Object Detection via End-to-End Integration of Aspect Ratio and Context Aware Part-based Models and Fully Convolutional Networks (<a href="https://arxiv.org/abs/1612.00534" target="_blank" rel="noopener">PDF</a>)</li>
<li>Feature Pyramid Networks for Object Detection (<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-12-15-FPN/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>COCO-Stuff: Thing and Stuff Classes in Context (<a href="https://arxiv.org/abs/1612.03716" target="_blank" rel="noopener">PDF</a>)</li>
<li>Finding Tiny Faces (<a href="https://arxiv.org/abs/1612.04402v1" target="_blank" rel="noopener">PDF</a>)</li>
<li>Beyond Skip Connections: Top-Down Modulation for Object Detection (<a href="https://arxiv.org/abs/1612.06851" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-12-21-TDM/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>YOLO9000: Better, Faster, Stronger (<a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="noopener">PDF</a>, <a href="http://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2017-01-11-YOLO9000/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Quantitative Analysis of Automatic Image Cropping Algorithms: A Dataset and Comparative Study (<a href="https://arxiv.org/abs/1701.01480" target="_blank" rel="noopener">PDF</a>)</li>
<li>To Boost or Not to Boost? On the Limits of Boosted Trees for Object Detection (<a href="https://arxiv.org/abs/1701.01692" target="_blank" rel="noopener">PDF</a>)</li>
<li>Pixel Objectness (<a href="https://arxiv.org/abs/1701.05349" target="_blank" rel="noopener">PDF</a>, <a href="http://vision.cs.utexas.edu/projects/pixelobjectness/" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2017-01-22-PixelObjectness/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>DSSD: Deconvolutional Single Shot Detector (<a href="https://arxiv.org/abs/1701.06659" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2017-02-10-DSSD/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>A Fast and Compact Salient Score Regression Network Based on Fully Convolutional Network (<a href="https://arxiv.org/abs/1702.00615" target="_blank" rel="noopener">PDF</a>)</li>
<li>Wide-Residual-Inception Networks for Real-time Object Detection (<a href="https://arxiv.org/abs/1702.01243" target="_blank" rel="noopener">PDF</a>)</li>
<li>Zoom Out-and-In Network with Recursive Training for Object Proposal (<a href="https://arxiv.org/abs/1702.05711" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/hli2020/zoom_network" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Improving Object Detection with Region Similarity Learning (<a href="https://arxiv.org/abs/1703.00234" target="_blank" rel="noopener">PDF</a>)</li>
<li>Tree-Structured Reinforcement Learning for Sequential Object Localization (<a href="https://arxiv.org/abs/1703.02710" target="_blank" rel="noopener">PDF</a>)</li>
<li>Weakly Supervised Object Localization Using Things and Stuff Transfer (<a href="https://arxiv.org/abs/1703.08000" target="_blank" rel="noopener">PDF</a>)</li>
<li>Unsupervised learning from video to detect foreground objects in single images (<a href="https://arxiv.org/abs/1703.10901" target="_blank" rel="noopener">PDF</a>)</li>
<li>A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection (<a href="http://www.cs.cmu.edu/~xiaolonw/papers/CVPR2017_Adversarial_Det.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/xiaolonw/adversarial-frcnn" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>A Learning non-maximum suppression (<a href="https://arxiv.org/abs/1705.02950" target="_blank" rel="noopener">PDF</a>)</li>
<li>Real Time Image Saliency for Black Box Classifiers (<a href="https://arxiv.org/abs/1705.07857" target="_blank" rel="noopener">PDF</a>)</li>
<li>An Efficient Approach for Object Detection and Tracking of Objects in a Video with Variable Background (<a href="https://arxiv.org/abs/1706.02672" target="_blank" rel="noopener">PDF</a>)</li>
<li>RON: Reverse Connection with Objectness Prior Networks for Object Detection (<a href="https://arxiv.org/abs/1707.01691" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/taokong/RON" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Deformable Part-based Fully Convolutional Network for Object Detection (<a href="https://arxiv.org/abs/1707.06175" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2017-07-28-DP-FCN/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Recurrent Scale Approximation for Object Detection in CNN (<a href="https://arxiv.org/abs/1707.09531" target="_blank" rel="noopener">PDF</a>)</li>
<li>DSOD: Learning Deeply Supervised Object Detectors from Scratch (<a href="https://arxiv.org/abs/1708.01241" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/szq0214/DSOD" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2017-08-10-DSOD/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN (<a href="https://arxiv.org/abs/1708.01956" target="_blank" rel="noopener">PDF</a>)</li>
<li>Focal Loss for Dense Object Detection (<a href="https://arxiv.org/abs/1708.02002" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning Uncertain Convolutional Features for Accurate Saliency Detection (<a href="https://arxiv.org/abs/1708.02031" target="_blank" rel="noopener">PDF</a>)</li>
<li>Optimizing Region Selection for Weakly Supervised Object Detection (<a href="https://arxiv.org/abs/1708.01723" target="_blank" rel="noopener">PDF</a>)</li>
<li>Kill Two Birds With One Stone: Boosting Both Object Detection Accuracy and Speed With adaptive Patch-of-Interest Composition (<a href="https://arxiv.org/abs/1708.03795" target="_blank" rel="noopener">PDF</a>)</li>
<li>Flow-Guided Feature Aggregation for Video Object Detection (<a href="https://arxiv.org/abs/1703.10025" target="_blank" rel="noopener">PDF</a>)</li>
<li>BlitzNet: A Real-Time Deep Network for Scene Understanding (<a href="BlitzNet: A Real-Time Deep Network for Scene Understanding">PDF</a>, <a href="http://thoth.inrialpes.fr/research/blitznet/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>RON: Reverse Connection with Objectness Prior Networks for Object Detection (<a href="https://arxiv.org/abs/1707.01691" target="_blank" rel="noopener">PDF</a>)</li>
<li>Soft Proposal Networks for Weakly Supervised Object Localization (<a href="https://arxiv.org/pdf/1709.01829.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/ZhouYanzhao/SPN/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Feature-Fused SSD: Fast Detection for Small Objects (<a href="https://arxiv.org/abs/1709.05054" target="_blank" rel="noopener">PDF</a>)</li>
<li>Light Cascaded Convolutional Neural Networks for Accurate Player Detection (<a href="https://arxiv.org/abs/1709.10230" target="_blank" rel="noopener">PDF</a>)</li>
<li>Personalized Saliency and its Prediction (<a href="https://arxiv.org/abs/1710.03011" target="_blank" rel="noopener">PDF</a>)</li>
<li>WeText: Scene Text Detection under Weak Supervision (<a href="https://arxiv.org/abs/1710.04826" target="_blank" rel="noopener">PDF</a>)</li>
<li>VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition (<a href="https://arxiv.org/abs/1710.06288" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/SeokjuLee/VPGNet" target="_blank" rel="noopener">Project/Code</a>)</li>
</ul>
<h3 id="SEGMENTATION-PARSING"><a href="#SEGMENTATION-PARSING" class="headerlink" title="SEGMENTATION/PARSING"></a>SEGMENTATION/PARSING</h3><ul>
<li>CenterMask: single shot instance segmentation with point representation (<a href="https://arxiv.org/abs/2004.04446" target="_blank" rel="noopener">PDF</a>)</li>
<li>Background Matting: The World is Your Green Screen (<a href="https://arxiv.org/abs/2004.00626" target="_blank" rel="noopener">PDF</a>, <a href="http://grail.cs.washington.edu/projects/background-matting/" target="_blank" rel="noopener">Project/Code</a>, <a href="https://github.com/senguptaumd/Background-Matting" target="_blank" rel="noopener">Github</a>)</li>
<li>Towards Real-Time Automatic Portrait Matting on Mobile Devices (<a href="https://arxiv.org/abs/1904.03816v1" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/hyperconnect/MMNet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Panoptic Feature Pyramid Networks (<a href="https://arxiv.org/abs/1901.02446" target="_blank" rel="noopener">PDF</a>)</li>
<li>Fast Neural Architecture Search of Compact Semantic Segmentation Models via Auxiliary Cells (<a href="https://arxiv.org/abs/1810.10804" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Learning for Semantic Segmentation on Minimal Hardware (<a href="https://arxiv.org/abs/1807.05597" target="_blank" rel="noopener">PDF</a>)</li>
<li>TernausNetV2: Fully Convolutional Network for Instance Segmentation (<a href="https://arxiv.org/abs/1806.00844" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/ternaus/TernausNetV2" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Stacked U-Nets: A No-Frills Approach to Natural Image Segmentation (<a href="https://arxiv.org/abs/1804.10343" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/shahsohil/sunets" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Deep Object Co-Segmentation (<a href="https://arxiv.org/abs/1804.06423" target="_blank" rel="noopener">PDF</a>)</li>
<li>Fusing Hierarchical Convolutional Features for Human Body Segmentation and Clothing Fashion Classification (<a href="https://arxiv.org/abs/1803.03415" target="_blank" rel="noopener">PDF</a>)</li>
<li>ShuffleSeg: Real-time Semantic Segmentation Network (<a href="https://arxiv.org/abs/1803.03816" target="_blank" rel="noopener">PDF</a>)</li>
<li>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation (<a href="https://arxiv.org/abs/1802.02611" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/tensorflow/models/tree/master/research/deeplab" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Learning random-walk label propagation for weakly-supervised semantic segmentation (<a href="https://arxiv.org/abs/1802.00470" target="_blank" rel="noopener">PDF</a>)</li>
<li>Panoptic Segmentation (<a href="https://arxiv.org/abs/1801.00868" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2018-01-08-Panoptic_Segmentation/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Learning to Segment Every Thing (<a href="https://arxiv.org/abs/1711.10370" target="_blank" rel="noopener">PDF</a>, <a href="http://ronghanghu.com/seg_every_thing/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Deep Extreme Cut: From Extreme Points to Object Segmentation (<a href="https://arxiv.org/abs/1711.09081" target="_blank" rel="noopener">PDF</a>)</li>
<li>Instance-aware Semantic Segmentation via Multi-task Network Cascades (<a href="http://arxiv.org/abs/1512.04412" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/daijifeng001/MNC" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation (<a href="http://arxiv.org/abs/1606.02147" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-08-23-Reading-note/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Learning Deconvolution Network for Semantic Segmentation (<a href="http://arxiv.org/abs/1505.04366" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-07-04-DeconvSemanticSegmentation/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Semantic Object Parsing with Graph LSTM (<a href="http://arxiv.org/abs/1603.07063" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-05-30-GLSTM/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding (<a href="http://arxiv.org/abs/1511.02680" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2015-11-10-SegNet/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Learning to Segment Moving Objects in Videos (<a href="http://arxiv.org/abs/1412.6504" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2015-11-04-Learning-to-Segment-Moving-Objects-in-Videos/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li><p>Deep Structured Features for Semantic Segmentation (<a href="http://arxiv.org/abs/1609.07916v1" target="_blank" rel="noopener">PDF</a>)</p>
<blockquote>
<p>We propose a highly structured neural network architecture for semantic segmentation of images that combines i) a Haar wavelet-based tree-like convolutional neural network (CNN), ii) a random layer realizing a radial basis function kernel approximation, and iii) a linear classifier. While stages i) and ii) are completely pre-specified, only the linear classifier is learned from data. Thanks to its high degree of structure, our architecture has a very small memory footprint and thus fits onto low-power embedded and mobile platforms. We apply the proposed architecture to outdoor scene and aerial image semantic segmentation and show that the accuracy of our architecture is competitive with conventional pixel classification CNNs. Furthermore, we demonstrate that the proposed architecture is data efficient in the sense of matching the accuracy of pixel classification CNNs when trained on a much smaller data set.</p>
</blockquote>
</li>
<li><p>CNN-aware Binary Map for General Semantic Segmentation (<a href="https://arxiv.org/abs/1609.09220" target="_blank" rel="noopener">PDF</a>)</p>
</li>
<li>Learning to Refine Object Segments (<a href="https://arxiv.org/abs/1603.08695" target="_blank" rel="noopener">PDF</a>)</li>
<li>Clockwork Convnets for Video Semantic Segmentation(<a href="https://arxiv.org/abs/1608.03609" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/shelhamer/clockwork-fcn" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Convolutional Gated Recurrent Networks for Video Segmentation (<a href="https://arxiv.org/abs/1611.05435" target="_blank" rel="noopener">PDF</a>)</li>
<li>Efficient Convolutional Neural Network with Binary Quantization Layer (<a href="https://arxiv.org/abs/1611.06764" target="_blank" rel="noopener">PDF</a>)</li>
<li>One-Shot Video Object Segmentation (<a href="https://arxiv.org/abs/1611.05198" target="_blank" rel="noopener">PDF</a>)</li>
<li>Fully Convolutional Instance-aware Semantic Segmentation (<a href="https://arxiv.org/abs/1611.07709" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/daijifeng001/TA-FCN" target="_blank" rel="noopener">Projcet/Code</a>, <a href="https://joshua19881228.github.io/2016-11-28-InstanceFCN/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Semantic Segmentation using Adversarial Networks (<a href="https://arxiv.org/abs/1611.08408" target="_blank" rel="noopener">PDF</a>)</li>
<li>Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes (<a href="https://arxiv.org/abs/1611.08323" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Watershed Transform for Instance Segmentation (<a href="https://arxiv.org/abs/1611.08303" target="_blank" rel="noopener">PDF</a>)</li>
<li>InstanceCut: from Edges to Instances with MultiCut (<a href="https://arxiv.org/abs/1611.08272" target="_blank" rel="noopener">PDF</a>)</li>
<li>The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation (<a href="https://arxiv.org/abs/1611.09326" target="_blank" rel="noopener">PDF</a>)</li>
<li>Improving Fully Convolution Network for Semantic Segmentation (<a href="https://arxiv.org/abs/1611.08986" target="_blank" rel="noopener">PDF</a>)</li>
<li>Video Scene Parsing with Predictive Feature Learning (<a href="https://arxiv.org/abs/1612.00119" target="_blank" rel="noopener">PDF</a>)</li>
<li>Training Bit Fully Convolutional Network for Fast Semantic Segmentation (<a href="https://arxiv.org/abs/1612.00212" target="_blank" rel="noopener">PDF</a>)</li>
<li>Pyramid Scene Parsing Network (<a href="https://arxiv.org/abs/1612.01105" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2017-09-19-PSPNet/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Mining Pixels: Weakly Supervised Semantic Segmentation Using Image Labels (<a href="https://arxiv.org/abs/1612.02101" target="_blank" rel="noopener">PDF</a>)</li>
<li>FastMask: Segment Object Multi-scale Candidates in One Shot (<a href="https://arxiv.org/abs/1612.08843" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/voidrank/FastMask" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2017-04-18-FastMask/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>A New Convolutional Network-in-Network Structure and Its Applications in Skin Detection, Semantic Segmentation, and Artifact Reduction (<a href="https://arxiv.org/abs/1701.06190" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2017-01-24-NINApplication/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>FusionSeg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos (<a href="https://arxiv.org/abs/1701.05384" target="_blank" rel="noopener">PDF</a>)</li>
<li>Visual Saliency Prediction Using a Mixture of Deep Neural Networks (<a href="https://arxiv.org/abs/1702.00372" target="_blank" rel="noopener">PDF</a>)</li>
<li>PixelNet: Representation of the pixels, by the pixels, and for the pixels (<a href="https://arxiv.org/abs/1702.06506" target="_blank" rel="noopener">PDF</a>, <a href="http://www.cs.cmu.edu/~aayushb/pixelNet/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Super-Trajectory for Video Segmentation (<a href="https://arxiv.org/abs/1702.08634" target="_blank" rel="noopener">PDF</a>)</li>
<li>Understanding Convolution for Semantic Segmentation (<a href="https://arxiv.org/abs/1702.08502" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2017-03-04-DUC-HDC/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Adversarial Examples for Semantic Image Segmentation (<a href="https://arxiv.org/abs/1703.01101" target="_blank" rel="noopener">PDF</a>)</li>
<li>Large Kernel Matters — Improve Semantic Segmentation by Global Convolutional Network (<a href="https://arxiv.org/abs/1703.02719" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Image Matting (<a href="https://arxiv.org/abs/1703.03872" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2017-03-16-Matting/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Mask R-CNN (<a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/jasjeetIM/Mask-RCNN" target="_blank" rel="noopener">Caffe Implementation</a>, <a href="https://github.com/TuSimple/mx-maskrcnn" target="_blank" rel="noopener">TuSimple Implementation on MXNet</a>, <a href="https://github.com/matterport/Mask_RCNN" target="_blank" rel="noopener">TensorFlow Implementation</a>, <a href="https://joshua19881228.github.io/2017-05-02-MaskRCNN/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Predicting Deeper into the Future of Semantic Segmentation (<a href="https://arxiv.org/abs/1703.07684" target="_blank" rel="noopener">PDF</a>)</li>
<li>Convolutional Oriented Boundaries: From Image Segmentation to High-Level Tasks (<a href="https://arxiv.org/abs/1701.04658" target="_blank" rel="noopener">PDF</a>, <a href="http://www.vision.ee.ethz.ch/~cvlsegmentation/cob/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>One-Shot Video Object Segmentation (<a href="https://arxiv.org/abs/1611.05198" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/kmaninis/OSVOS-caffe" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Semantic Instance Segmentation via Deep Metric Learning (<a href="https://arxiv.org/abs/1703.10277" target="_blank" rel="noopener">PDF</a>)</li>
<li>Not All Pixels Are Equal: Difficulty-aware Semantic Segmentation via Deep Layer Cascade (<a href="https://arxiv.org/abs/1704.01344" target="_blank" rel="noopener">PDF</a>)</li>
<li>Semantically-Guided Video Object Segmentation (<a href="https://arxiv.org/abs/1704.01926" target="_blank" rel="noopener">PDF</a>)</li>
<li>Recurrent Multimodal Interaction for Referring Image Segmentation (<a href="https://arxiv.org/abs/1703.07939" target="_blank" rel="noopener">PDF</a>)</li>
<li>Loss Max-Pooling for Semantic Image Segmentation (<a href="https://arxiv.org/abs/1704.02966" target="_blank" rel="noopener">PDF</a>)</li>
<li>Reformulating Level Sets as Deep Recurrent Neural Network Approach to Semantic Segmentation (<a href="https://arxiv.org/abs/1704.03593" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning Video Object Segmentation with Visual Memory (<a href="https://arxiv.org/abs/1704.05737" target="_blank" rel="noopener">PDF</a>)</li>
<li>A Review on Deep Learning Techniques Applied to Semantic Segmentation (<a href="https://arxiv.org/abs/1704.06857" target="_blank" rel="noopener">PDF</a>)</li>
<li>BiSeg: Simultaneous Instance Segmentation and Semantic Segmentation with Fully Convolutional Networks (<a href="https://arxiv.org/abs/1706.02135" target="_blank" rel="noopener">PDF</a>)</li>
<li>Rethinking Atrous Convolution for Semantic Image Segmentation (<a href="https://arxiv.org/abs/1706.05587" target="_blank" rel="noopener">PDF</a>)</li>
<li>Discriminative Localization in CNNs for Weakly-Supervised Segmentation of Pulmonary Nodules (<a href="https://arxiv.org/abs/1707.01086" target="_blank" rel="noopener">PDF</a>)</li>
<li>Superpixel-based semantic segmentation trained by statistical process control (<a href="https://arxiv.org/abs/1706.10071" target="_blank" rel="noopener">PDF</a>)</li>
<li>The Devil is in the Decoder (<a href="https://arxiv.org/abs/1707.05847" target="_blank" rel="noopener">PDF</a>)</li>
<li>Semantic Segmentation with Reverse Attention (<a href="https://arxiv.org/abs/1707.06426" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning Deconvolution Network for Semantic Segmentation (<a href="https://arxiv.org/abs/1505.04366" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/HyeonwooNoh/DeconvNet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Depth Adaptive Deep Neural Network for Semantic Segmentation (<a href="https://arxiv.org/abs/1708.01818" target="_blank" rel="noopener">PDF</a>)</li>
<li>Semantic Instance Segmentation with a Discriminative Loss Function (<a href="https://arxiv.org/abs/1708.02551" target="_blank" rel="noopener">PDF</a>)</li>
<li>A Cost-Sensitive Visual Question-Answer Framework for Mining a Deep And-OR Object Semantics from Web Images (<a href="https://arxiv.org/abs/1708.03911" target="_blank" rel="noopener">PDF</a>)</li>
<li>ICNet for Real-Time Semantic Segmentation on High-Resolution Images (<a href="https://arxiv.org/abs/1704.08545" target="_blank" rel="noopener">PDF</a>, <a href="https://hszhao.github.io/projects/icnet/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Pyramid Scene Parsing Network (<a href="https://arxiv.org/abs/1612.01105" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/hszhao/PSPNet" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2017-09-19-PSPNet/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Learning to Segment Instances in Videos with Spatial Propagation Network (<a href="http://davischallenge.org/challenge2017/papers/DAVIS-Challenge-6th-Team.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/JingchunCheng/Seg-with-SPN" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Learning Affinity via Spatial Propagation Networks (<a href="https://arxiv.org/abs/1710.01020" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/Liusifei/caffe-spn" target="_blank" rel="noopener">Project/Code</a>)</li>
</ul>
<h3 id="TRACKING"><a href="#TRACKING" class="headerlink" title="TRACKING"></a>TRACKING</h3><ul>
<li>Tracking Objects as Points (<a href="https://arxiv.org/abs/2004.01177" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/xingyizhou/CenterTrack" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Deeper and Wider Siamese Networks for Real-Time Visual Tracking (<a href="https://arxiv.org/abs/1901.01660" target="_blank" rel="noopener">PDF</a>)</li>
<li>Multiple People Tracking Using Hierarchical Deep Tracklet Re-identification (<a href="https://arxiv.org/abs/1811.04091" target="_blank" rel="noopener">PDF</a>)</li>
<li>Fully-Convolutional Siamese Networks for Object Tracking (<a href="https://arxiv.org/abs/1606.09549" target="_blank" rel="noopener">PDF</a>)</li>
<li>Joint Flow: Temporal Flow Fields for Multi Person Tracking (<a href="https://arxiv.org/abs/1805.04596" target="_blank" rel="noopener">PDF</a>)</li>
<li>Trajectory Factory: Tracklet Cleaving and Re-connection by Deep Siamese Bi-GRU for Multiple Object Tracking (<a href="https://arxiv.org/abs/1804.04555" target="_blank" rel="noopener">PDF</a>)</li>
<li>Machine Learning Methods for Solving Assignment Problems in Multi-Target Tracking (<a href="https://arxiv.org/abs/1802.06897" target="_blank" rel="noopener">PDF</a>)</li>
<li>Multi-Target, Multi-Camera Tracking by Hierarchical Clustering: Recent Progress on DukeMTMC Project (<a href="https://arxiv.org/abs/1712.09531" target="_blank" rel="noopener">PDF</a>)</li>
<li>Detect-and-Track: Efficient Pose Estimation in Videos (<a href="https://arxiv.org/abs/1712.09184" target="_blank" rel="noopener">PDF</a>)</li>
<li>Track, then Decide: Category-Agnostic Vision-based Multi-Object Tracking (<a href="https://arxiv.org/abs/1712.07920" target="_blank" rel="noopener">PDF</a>)</li>
<li>Spatially Supervised Recurrent Convolutional Neural Networks for Visual Object Tracking (<a href="http://arxiv.org/abs/1607.05781" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-07-21-ROLO/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Joint Tracking and Segmentation of Multiple Targets (<a href="http://milanton.de/files/cvpr2015/cvpr2015-ext-abstr-anton.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2015-11-05-Joint-Tracking-and-Segmentation-of-Multiple-Targets/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Deep Tracking on the Move: Learning to Track the World from a Moving Vehicle using Recurrent Neural Networks (<a href="https://arxiv.org/abs/1609.09365" target="_blank" rel="noopener">PDF</a>)</li>
<li>Convolutional Regression for Visual Tracking (<a href="https://arxiv.org/abs/1611.04215" target="_blank" rel="noopener">PDF</a>)</li>
<li>Kernelized Correlation Filters(<a href="http://www.robots.ox.ac.uk/%7Ejoao/circulant/" target="_blank" rel="noopener">Project</a> <a href="https://github.com/foolwood/KCF" target="_blank" rel="noopener">CODE1</a> <a href="https://github.com/vojirt/kcf" target="_blank" rel="noopener">CODE2</a>)</li>
<li>Online Visual Multi-Object Tracking via Labeled Random Finite Set Filtering (<a href="https://arxiv.org/abs/1611.06011" target="_blank" rel="noopener">PDF</a>)</li>
<li>SANet: Structure-Aware Network for Visual Tracking (<a href="https://arxiv.org/abs/1611.06878" target="_blank" rel="noopener">PDF</a>)</li>
<li>Semantic tracking: Single-target tracking with inter-supervised convolutional networks (<a href="https://arxiv.org/abs/1611.06395" target="_blank" rel="noopener">PDF</a>)</li>
<li>On The Stability of Video Detection and Tracking (<a href="https://arxiv.org/abs/1611.06467" target="_blank" rel="noopener">PDF</a>)</li>
<li>Dual Deep Network for Visual Tracking (<a href="https://arxiv.org/abs/1612.06053" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Motion Features for Visual Tracking (<a href="https://arxiv.org/abs/1612.06615" target="_blank" rel="noopener">PDF</a>)</li>
<li>Robust and Real-time Deep Tracking Via Multi-Scale Domain Adaptation (<a href="https://arxiv.org/abs/1701.00561" target="_blank" rel="noopener">PDF</a>, <a href="https://bitbucket.org/xinke_wang/msdat/src" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Instance Flow Based Online Multiple Object Tracking (<a href="https://arxiv.org/abs/1703.01289" target="_blank" rel="noopener">PDF</a>)</li>
<li>PathTrack: Fast Trajectory Annotation with Path Supervision (<a href="https://arxiv.org/abs/1703.02437" target="_blank" rel="noopener">PDF</a>)</li>
<li>Good Features to Correlate for Visual Tracking (<a href="https://arxiv.org/abs/1704.06326" target="_blank" rel="noopener">PDF</a>)</li>
<li>Re3 : Real-Time Recurrent Regression Networks for Object Tracking (<a href="https://arxiv.org/abs/1705.06368" target="_blank" rel="noopener">PDF</a>)</li>
<li>Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning (<a href="https://drive.google.com/file/d/0B34VXh5mZ22cZUs2Umc1cjlBMFU/view" target="_blank" rel="noopener">PDF</a>, <a href="https://sites.google.com/view/cvpr2017-adnet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Simple Online and Realtime Tracking with a Deep Association Metric (<a href="https://arxiv.org/abs/1703.07402" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning Policies for Adaptive Tracking with Deep Feature Cascades (<a href="https://arxiv.org/abs/1708.02973" target="_blank" rel="noopener">PDF</a>)</li>
<li>Recurrent Filter Learning for Visual Tracking (<a href="https://arxiv.org/abs/1708.03874" target="_blank" rel="noopener">PDF</a>)</li>
<li>Tracking Persons-of-Interest via Unsupervised Representation Adaptation (<a href="https://arxiv.org/abs/1710.02139" target="_blank" rel="noopener">PDF</a>)</li>
<li>Detect to Track and Track to Detect (<a href="https://arxiv.org/abs/1710.03958" target="_blank" rel="noopener">PDF</a>, <a href="https://www.robots.ox.ac.uk/~vgg/research/detect-track/" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2017-10-25-D2TT2D/" target="_blank" rel="noopener">Reading Note</a>)</li>
</ul>
<h3 id="POSE-ESTIMATION"><a href="#POSE-ESTIMATION" class="headerlink" title="POSE ESTIMATION"></a>POSE ESTIMATION</h3><ul>
<li>Human Pose Estimation with Spatial Contextual Information (<a href="https://arxiv.org/abs/1901.01760" target="_blank" rel="noopener">PDF</a>)</li>
<li>Rethinking on Multi-Stage Networks for Human Pose Estimation (<a href="https://arxiv.org/abs/1901.00148" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning to Estimate 3D Human Pose and Shape from a Single Color Image (<a href="https://arxiv.org/abs/1805.04092" target="_blank" rel="noopener">PDF</a>, <a href="https://www.seas.upenn.edu/~pavlakos/projects/humanshape/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Ordinal Depth Supervision for 3D Human Pose Estimation (<a href="https://arxiv.org/abs/1805.04095" target="_blank" rel="noopener">PDF</a>, <a href="https://www.seas.upenn.edu/~pavlakos/projects/ordinal/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Simple Baselines for Human Pose Estimation and Tracking (<a href="https://arxiv.org/abs/1804.06208" target="_blank" rel="noopener">PDF</a>)</li>
<li>End-to-end Recovery of Human Shape and Pose (<a href="https://arxiv.org/abs/1712.06584" target="_blank" rel="noopener">PDF</a>, <a href="https://akanazawa.github.io/hmr/" target="_blank" rel="noopener">PROJECT/CODE</a>, <a href="https://github.com/akanazawa/hmr" target="_blank" rel="noopener">Code</a>)</li>
<li>PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model (<a href="https://arxiv.org/abs/1803.08225" target="_blank" rel="noopener">PDF</a>)</li>
<li>DensePose: Dense Human Pose Estimation In The Wild (<a href="https://arxiv.org/abs/1802.00434" target="_blank" rel="noopener">PDF</a>, <a href="http://densepose.org/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Cascaded Pyramid Network for Multi-Person Pose Estimation (<a href="https://arxiv.org/abs/1711.07319" target="_blank" rel="noopener">PDF</a>)</li>
<li>Chained Predictions Using Convolutional Neural Networks (<a href="http://arxiv.org/abs/1605.02346" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-05-24-chainedPredictions/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>CRF-CNN: Modeling Structured Information in Human Pose Estimation (<a href="https://arxiv.org/abs/1611.00468" target="_blank" rel="noopener">PDF</a>)</li>
<li>Convolutional Pose Machines (<a href="https://arxiv.org/abs/1602.00134" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/shihenw/convolutional-pose-machines-release" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2016-12-27-CPM/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields (<a href="https://arxiv.org/abs/1611.08050" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/ZheC/Multi-Person-Pose-Estimation" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2016-12-29-PAF/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Towards Accurate Multi-person Pose Estimation in the Wild (<a href="https://arxiv.org/abs/1701.01779" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2017-01-19-TAMPEW/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Adversarial PoseNet: A Structure-aware Convolutional Network for Human Pose Estimation (<a href="https://arxiv.org/abs/1705.00389" target="_blank" rel="noopener">PDF</a>)</li>
<li>Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose (<a href="https://arxiv.org/abs/1611.07828" target="_blank" rel="noopener">PDF</a>, <a href="https://www.seas.upenn.edu/~pavlakos/projects/volumetric/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Learning Feature Pyramids for Human Pose Estimation (<a href="https://arxiv.org/abs/1708.01101" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/bearpaw/PyraNet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Joint Multi-Person Pose Estimation and Semantic Part Segmentation (<a href="https://arxiv.org/abs/1708.03383" target="_blank" rel="noopener">PDF</a>)</li>
<li>DeepPrior++: Improving Fast and Accurate 3D Hand Pose Estimation (<a href="https://arxiv.org/abs/1708.08325" target="_blank" rel="noopener">PDF</a>)</li>
<li>Lifting from the Deep: Convolutional 3D Pose Estimation from a Single Image (<a href="https://arxiv.org/abs/1701.00295" target="_blank" rel="noopener">PDF</a>)</li>
<li>Human Pose Regression by Combining Indirect Part Detection and Contextual Information (<a href="https://arxiv.org/abs/1710.02322" target="_blank" rel="noopener">PDF</a>)</li>
<li>Dual Path Networks for Multi-Person Human Pose Estimation (<a href="https://arxiv.org/abs/1710.10192" target="_blank" rel="noopener">PDF</a>)</li>
</ul>
<h3 id="ACTION-RECOGNITION-EVENT-DETECTION-VIDEO"><a href="#ACTION-RECOGNITION-EVENT-DETECTION-VIDEO" class="headerlink" title="ACTION RECOGNITION/EVENT DETECTION/VIDEO"></a>ACTION RECOGNITION/EVENT DETECTION/VIDEO</h3><ul>
<li>Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition (<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_Two-Stream_Adaptive_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_CVPR_2019_paper.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/lshiwjx/2s-AGCN" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes (<a href="https://arxiv.org/abs/1802.10062" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/leeyeehoo/CSRNet" target="_blank" rel="noopener">Project/Code</a>, <a href="https://github.com/wkcn/CSRNet-mx" target="_blank" rel="noopener">MxNet Version</a>, <a href="https://joshua19881228.github.io/2019-04-14-CSRNet/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>SlowFast Networks for Video Recognition (<a href="https://arxiv.org/abs/1812.03982" target="_blank" rel="noopener">PDF</a>)</li>
<li>PHD-GIFs: Personalized Highlight Detection for Automatic GIF Creation (<a href="https://arxiv.org/abs/1804.06604" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/gyglim/personalized-highlights-dataset" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Superframes, A Temporal Video Segmentation (<a href="https://arxiv.org/abs/1804.06642" target="_blank" rel="noopener">PDF</a>)</li>
<li>Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation (<a href="https://arxiv.org/abs/1804.06055" target="_blank" rel="noopener">PDF</a>)</li>
<li>2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning (<a href="https://arxiv.org/abs/1802.09232" target="_blank" rel="noopener">PDF</a>)</li>
<li>Real-Time End-to-End Action Detection with Two-Stream Networks (<a href="https://arxiv.org/abs/1802.08362" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning Video-Story Composition via Recurrent Neural Network (<a href="https://arxiv.org/abs/1801.10281" target="_blank" rel="noopener">PDF</a>)</li>
<li>Real-world Anomaly Detection in Surveillance Videos (<a href="https://arxiv.org/abs/1801.04264" target="_blank" rel="noopener">PDF</a>)</li>
<li>Fully-Coupled Two-Stream Spatiotemporal Networks for Extremely Low Resolution Action Recognition (<a href="https://arxiv.org/abs/1801.03983" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward (<a href="https://arxiv.org/abs/1801.00054" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com//KaiyangZhou/vsumm-reinforce" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Making a long story short: A Multi-Importance Semantic for Fast-Forwarding Egocentric Videos (<a href="https://arxiv.org/abs/1711.03473" target="_blank" rel="noopener">PDF</a>)</li>
<li>Attentional Pooling for Action Recognition (<a href="https://arxiv.org/abs/1711.01467" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/rohitgirdhar/AttentionalPoolingAction" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Pooling the Convolutional Layers in Deep ConvNets for Action Recognition (<a href="http://arxiv.org/abs/1511.02126" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2015-11-15-PoolConvNet/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Two-Stream Convolutional Networks for Action Recognition in Videos (<a href="http://arxiv.org/abs/1406.2199" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2015-11-06-Two-Stream/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>YouTube-8M: A Large-Scale Video Classification Benchmark (<a href="https://arxiv.org/abs/1609.08675" target="_blank" rel="noopener">PDF</a>, <a href="http://research.google.com/youtube8m" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Spatiotemporal Residual Networks for Video Action Recognition (<a href="https://arxiv.org/abs/1611.02155" target="_blank" rel="noopener">PDF</a>)</li>
<li>An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data (<a href="https://arxiv.org/abs/1611.06067" target="_blank" rel="noopener">PDF</a>)</li>
<li>Fast Video Classification via Adaptive Cascading of Deep Models (<a href="https://arxiv.org/abs/1611.06453" target="_blank" rel="noopener">PDF</a>)</li>
<li>Video Pixel Networks (<a href="https://arxiv.org/abs/1610.00527" target="_blank" rel="noopener">PDF</a>)</li>
<li>Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection (<a href="https://arxiv.org/abs/1610.00307" target="_blank" rel="noopener">PDF</a>)</li>
<li>EM-Based Mixture Models Applied to Video Event Detection (<a href="https://arxiv.org/abs/1610.02923" target="_blank" rel="noopener">PDF</a>)</li>
<li>Video Captioning and Retrieval Models with Semantic Attention (<a href="https://arxiv.org/abs/1610.02947" target="_blank" rel="noopener">PDF</a>)</li>
<li>Title Generation for User Generated Videos (<a href="https://arxiv.org/abs/1608.07068" target="_blank" rel="noopener">PDF</a>)</li>
<li>Review of Action Recognition and Detection Methods (<a href="https://arxiv.org/abs/1610.06906" target="_blank" rel="noopener">PDF</a>)</li>
<li>RECURRENT MIXTURE DENSITY NETWORK FOR SPATIOTEMPORAL VISUAL ATTENTION (<a href="http://openreview.net/pdf?id=SJRpRfKxx" target="_blank" rel="noopener">PDF</a>)</li>
<li>Self-Supervised Video Representation Learning With Odd-One-Out Networks (<a href="https://arxiv.org/abs/1611.06646" target="_blank" rel="noopener">PDF</a>)</li>
<li>Recurrent Memory Addressing for describing videos (<a href="https://arxiv.org/abs/1611.06492" target="_blank" rel="noopener">PDF</a>)</li>
<li>Online Real time Multiple Spatiotemporal Action Localisation and Prediction on a Single Platform (<a href="https://arxiv.org/abs/1611.08563" target="_blank" rel="noopener">PDF</a>)</li>
<li>Real-Time Video Highlights for Yahoo Esports (<a href="https://arxiv.org/abs/1611.08780" target="_blank" rel="noopener">PDF</a>)</li>
<li>Surveillance Video Parsing with Single Frame Supervision (<a href="https://arxiv.org/abs/1611.09587" target="_blank" rel="noopener">PDF</a>)</li>
<li>Anomaly Detection in Video Using Predictive Convolutional Long Short-Term Memory Networks (<a href="https://arxiv.org/abs/1612.00390" target="_blank" rel="noopener">PDF</a>)</li>
<li>Action Recognition with Dynamic Image Networks (<a href="https://arxiv.org/abs/1612.00738" target="_blank" rel="noopener">PDF</a>)</li>
<li>ActionFlowNet: Learning Motion Representation for Action Recognition (<a href="https://arxiv.org/abs/1612.03052" target="_blank" rel="noopener">PDF</a>)</li>
<li>Video Propagation Networks (<a href="https://arxiv.org/abs/1612.05478" target="_blank" rel="noopener">PDF</a>)</li>
<li>Detecting events and key actors in multi-person videos (<a href="https://arxiv.org/abs/1511.02917" target="_blank" rel="noopener">PDF</a>)</li>
<li>A Pursuit of Temporal Accuracy in General Activity Detection (<a href="https://arxiv.org/abs/1703.02716" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2017-03-15-TAG/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos (<a href="https://arxiv.org/abs/1703.10664" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deceiving Google’s Cloud Video Intelligence API Built for Summarizing Videos (<a href="https://arxiv.org/abs/1703.09793" target="_blank" rel="noopener">PDF</a>)</li>
<li>Incremental Tube Construction for Human Action Detection (<a href="https://arxiv.org/abs/1704.01358" target="_blank" rel="noopener">PDF</a>)</li>
<li>Unsupervised Action Proposal Ranking through Proposal Recombination (<a href="https://arxiv.org/abs/1704.00758" target="_blank" rel="noopener">PDF</a>)</li>
<li>CERN: Confidence-Energy Recurrent Network for Group Activity Recognition (<a href="https://arxiv.org/abs/1704.03058" target="_blank" rel="noopener">PDF</a>)</li>
<li>Forecasting Human Dynamics from Static Images (<a href="https://arxiv.org/abs/1704.03432" target="_blank" rel="noopener">PDF</a>)</li>
<li>Interpretable 3D Human Action Analysis with Temporal Convolutional Networks (<a href="https://arxiv.org/abs/1704.04516" target="_blank" rel="noopener">PDF</a>)</li>
<li>Training object class detectors with click supervision (<a href="https://arxiv.org/abs/1704.06228" target="_blank" rel="noopener">PDF</a>)</li>
<li>Skeleton-based Action Recognition with Convolutional Neural Networks (<a href="https://arxiv.org/abs/1704.07595" target="_blank" rel="noopener">PDF</a>)</li>
<li>Online growing neural gas for anomaly detection in changing surveillance scenes (<a href="https://www.researchgate.net/publication/309101586_Online_growing_neural_gas_for_anomaly_detection_in_changing_surveillance_scenes" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning Person Trajectory Representations for Team Activity Analysis (<a href="https://arxiv.org/abs/1706.00893" target="_blank" rel="noopener">PDF</a>)</li>
<li>Concurrence-Aware Long Short-Term Sub-Memories for Person-Person Action Recognition (<a href="https://arxiv.org/abs/1706.00931" target="_blank" rel="noopener">PDF</a>)</li>
<li>Video Imagination from a Single Image with Transformation Generation (<a href="https://arxiv.org/abs/1706.04124" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/gitpub327/VideoImagination" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Optimizing Deep CNN-Based Queries over Video Streams at Scale (<a href="https://arxiv.org/abs/1703.02529" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/stanford-futuredata/noscope" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2017-07-10-NoScope/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Extreme Low Resolution Activity Recognition with Multi-Siamese Embedding Learning (<a href="https://arxiv.org/abs/1708.00999" target="_blank" rel="noopener">PDF</a>)</li>
<li>Predicting Human Activities Using Stochastic Grammar (<a href="https://arxiv.org/abs/1708.00945" target="_blank" rel="noopener">PDF</a>)</li>
<li>Discriminative convolutional Fisher vector network for action recognition (<a href="https://arxiv.org/abs/1707.06119" target="_blank" rel="noopener">PDF</a>)</li>
<li>Extreme Low Resolution Activity Recognition with Multi-Siamese Embedding Learning (<a href="https://arxiv.org/abs/1708.00999" target="_blank" rel="noopener">PDF</a>)</li>
<li>Exploiting Semantic Contextualization for Interpretation of Human Activity in Videos (<a href="https://arxiv.org/abs/1708.03725" target="_blank" rel="noopener">PDF</a>)</li>
<li>Lattice Long Short-Term Memory for Human Action Recognition (<a href="https://arxiv.org/abs/1708.03958" target="_blank" rel="noopener">PDF</a>)</li>
<li>Kinship Verification from Videos using Spatio-Temporal Texture Features and Deep Learning (<a href="https://arxiv.org/abs/1708.04069" target="_blank" rel="noopener">PDF</a>)</li>
<li>Fast-Forward Video Based on Semantic Extraction (<a href="https://arxiv.org/abs/1708.04160" target="_blank" rel="noopener">PDF</a>)</li>
<li>Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks (<a href="https://arxiv.org/abs/1708.04299" target="_blank" rel="noopener">PDF</a>)</li>
<li>ConvNet Architecture Search for Spatiotemporal Feature Learning (<a href="https://arxiv.org/abs/1708.05038" target="_blank" rel="noopener">PDF</a>, <a href="http://vlg.cs.dartmouth.edu/c3d/" target="_blank" rel="noopener">Project/Code</a>, <a href="https://github.com/facebook/C3D" target="_blank" rel="noopener">Github</a>)</li>
<li>Fully Context-Aware Video Prediction (<a href="https://arxiv.org/abs/1710.08518" target="_blank" rel="noopener">PDF</a>)</li>
</ul>
<h3 id="FACE"><a href="#FACE" class="headerlink" title="FACE"></a>FACE</h3><ul>
<li>BlazeFace: Sub-millisecond Neural Face Detection on Mobile GPUs (<a href="https://arxiv.org/abs/1907.05047" target="_blank" rel="noopener">PDF</a>)</li>
<li>A Dataset and Benchmark for Large-scale Multi-modal Face Anti-spoofing (<a href="https://arxiv.org/abs/1812.00408" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/AlexanderParkin/ChaLearn_liveness_challenge" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Learning towards Minimum Hyperspherical Energy (<a href="https://arxiv.org/abs/1805.09298" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/wy1iu/sphereface-plus" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Consensus-Driven Propagation in Massive Unlabeled Data for Face Recognition (<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Xiaohang_Zhan_Consensus-Driven_Propagation_in_ECCV_2018_paper.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/XiaohangZhan/cdp/" target="_blank" rel="noopener">Code/Project</a>)</li>
<li>Arbitrary Facial Attribute Editing: Only Change What You Want (<a href="https://arxiv.org/abs/1711.10678" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/LynnHo/AttGAN-Tensorflow" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Anchor Cascade for Efficient Face Detection (<a href="https://arxiv.org/abs/1805.03363" target="_blank" rel="noopener">PDF</a>)</li>
<li>Real-Time Rotation-Invariant Face Detection with Progressive Calibration Networks (<a href="https://arxiv.org/abs/1804.06039" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2018-04-27-Real-Time-Face-Detection/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>MobileFaceNets: Efficient CNNs for Accurate Real-time Face Verification on Mobile Devices (<a href="https://arxiv.org/abs/1804.07573" target="_blank" rel="noopener">PDF</a>)</li>
<li>Survey of Face Detection on Low-quality Images (<a href="https://arxiv.org/abs/1804.07362" target="_blank" rel="noopener">PDF</a>)</li>
<li>PyramidBox: A Context-assisted Single Shot Face Detector (<a href="https://arxiv.org/abs/1803.07737" target="_blank" rel="noopener">PDF</a>)</li>
<li>SFace: An Efficient Network for Face Detection in Large Scale Variations (<a href="SFace: An Efficient Network for Face Detection in Large Scale Variations">PDF</a>)</li>
<li>Deep Facial Expression Recognition: A Survey (<a href="https://arxiv.org/abs/1804.08348" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Face Recognition: A Survey (<a href="https://arxiv.org/abs/1804.06655" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Semantic Face Deblurring (<a href="https://arxiv.org/abs/1803.03345" target="_blank" rel="noopener">PDF</a>, <a href="https://sites.google.com/site/ziyishenmi/cvpr18_face_deblur" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Evaluation of Dense 3D Reconstruction from 2D Face Images in the Wild (<a href="https://arxiv.org/abs/1803.05536" target="_blank" rel="noopener">PDF</a>)</li>
<li>SSH: Single Stage Headless Face Detector (<a href="https://arxiv.org/abs/1708.03979" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/mahyarnajibi/SSH" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Detecting and counting tiny faces (<a href="https://arxiv.org/abs/1801.06504" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/alexattia/ExtendedTinyFaces" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Training Deep Face Recognition Systems with Synthetic Data (<a href="https://arxiv.org/abs/1802.05891" target="_blank" rel="noopener">PDF</a>)</li>
<li>Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification (<a href="http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf" target="_blank" rel="noopener">PDF</a>, <a href="http://gendershades.org/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks (<a href="https://arxiv.org/abs/1604.02878" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/kpzhang93/MTCNN_face_detection_alignment" target="_blank" rel="noopener">Project/Code</a>, <a href="https://github.com/foreverYoungGitHub/MTCNN" target="_blank" rel="noopener">Code Caffe</a>)</li>
<li>Deep Architectures for Face Attributes (<a href="http://arxiv.org/abs/1609.09018" target="_blank" rel="noopener">PDF</a>)</li>
<li>Face Detection with End-to-End Integration of a ConvNet and a 3D Model (<a href="https://www.arxiv.org/abs/1606.00850" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-09-29-Reading-note/" target="_blank" rel="noopener">Reading Note</a>, <a href="https://github.com/tfwu/FaceDetection-ConvNet-3D" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>A CNN Cascade for Landmark Guided Semantic Part Segmentation (<a href="https://arxiv.org/pdf/1609.09642.pdf" target="_blank" rel="noopener">PDF</a>, <a href="http://www.cs.nott.ac.uk/~psxasj/papers/jackson2016guided/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Kernel Selection using Multiple Kernel Learning and Domain Adaptation in Reproducing Kernel Hilbert Space, for Face Recognition under Surveillance Scenario (<a href="https://arxiv.org/abs/1610.00660" target="_blank" rel="noopener">PDF</a>)</li>
<li>An All-In-One Convolutional Neural Network for Face Analysis (<a href="https://arxiv.org/abs/1611.00851" target="_blank" rel="noopener">PDF</a>)</li>
<li>Fast Face-swap Using Convolutional Neural Networks (<a href="https://arxiv.org/abs/1611.09577" target="_blank" rel="noopener">PDF</a>)</li>
<li>Cross-Age Reference Coding for Age-Invariant Face Recognition and Retrieval (<a href="http://bcsiriuschen.github.io/CARC/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>CMS-RCNN: Contextual Multi-Scale Region-based CNN for Unconstrained Face Detection (<a href="https://arxiv.org/abs/1606.05413" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Face Synthesis from Facial Identity Features (<a href="https://arxiv.org/abs/1701.04851" target="_blank" rel="noopener">PDF</a>)</li>
<li>DeepFace: Face Generation using Deep Learning (<a href="https://arxiv.org/abs/1701.01876" target="_blank" rel="noopener">PDF</a>)</li>
<li>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns (<a href="http://www.openu.ac.il/home/hassner/projects/cnn_emotions/LeviHassnerICMI15.pdf" target="_blank" rel="noopener">PDF</a>, <a href="http://www.openu.ac.il/home/hassner/projects/cnn_emotions/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>EmotioNet Challenge: Recognition of facial expressions of emotion in the wild (<a href="https://arxiv.org/abs/1703.01210" target="_blank" rel="noopener">PDF</a>)</li>
<li>Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation (<a href="https://arxiv.org/abs/1703.10131" target="_blank" rel="noopener">PDF</a>)</li>
<li>Semi and Weakly Supervised Semantic Segmentation Using Generative Adversarial Network (<a href="https://arxiv.org/abs/1703.09695" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Alignment Network: A convolutional neural network for robust face alignment (<a href="https://arxiv.org/abs/1706.01789" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/MarekKowalski/DeepAlignmentNetwork" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Scale-Aware Face Detection (<a href="https://arxiv.org/abs/1706.09876" target="_blank" rel="noopener">PDF</a>)</li>
<li>SSH: Single Stage Headless Face Detector (<a href="https://arxiv.org/abs/1708.03979" target="_blank" rel="noopener">PDF</a>)</li>
<li>AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild (<a href="https://arxiv.org/abs/1708.03985" target="_blank" rel="noopener">PDF</a>)</li>
<li>SphereFace: Deep Hypersphere Embedding for Face Recognition (<a href="https://arxiv.org/abs/1704.08063" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/wy1iu/sphereface" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Age Group and Gender Estimation in the Wild with Deep RoR Architecture (<a href="https://arxiv.org/abs/1710.02985" target="_blank" rel="noopener">PDF</a>)</li>
<li>Island Loss for Learning Discriminative Features in Facial Expression Recognition (<a href="https://arxiv.org/abs/1710.03144" target="_blank" rel="noopener">PDF</a>)</li>
<li>Temporal Non-Volume Preserving Approach to Facial Age-Progression and Age-Invariant Face Recognition (<a href="https://arxiv.org/abs/1703.08617" target="_blank" rel="noopener">PDF</a>)</li>
</ul>
<h3 id="OPTICAL-FLOW"><a href="#OPTICAL-FLOW" class="headerlink" title="OPTICAL FLOW"></a>OPTICAL FLOW</h3><ul>
<li>LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation (<a href="https://arxiv.org/abs/1805.07036" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/twhui/LiteFlowNet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>DeepFlow: Large displacement optical flow with deep matching (<a href="https://hal.inria.fr/hal-00873592" target="_blank" rel="noopener">PDF</a>, <a href="http://lear.inrialpes.fr/src/deepflow/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Guided Optical Flow Learning (<a href="https://arxiv.org/abs/1702.02295" target="_blank" rel="noopener">PDF</a>)</li>
</ul>
<h3 id="IMAGE-PROCESSING"><a href="#IMAGE-PROCESSING" class="headerlink" title="IMAGE PROCESSING"></a>IMAGE PROCESSING</h3><ul>
<li>R2D2: Repeatable and Reliable Detector and Descriptor (<a href="https://arxiv.org/abs/1906.06195" target="_blank" rel="noopener">PDF</a>)</li>
<li>CartoonGAN: Generative Adversarial Networks for Photo Cartoonization (<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/2205.html" target="_blank" rel="noopener">PDF</a>)</li>
<li>Image Inpainting for Irregular Holes Using Partial Convolutions (<a href="https://arxiv.org/abs/1804.07723" target="_blank" rel="noopener">PDF</a>)</li>
<li>Neural Aesthetic Image Reviewer (<a href="https://arxiv.org/abs/1802.10240" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2018-03-05-Neural_Aesthetic_Image_Reviewer/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Automatic Image Cropping for Visual Aesthetic Enhancement Using Deep Neural Networks and Cascaded Regression (<a href="https://arxiv.org/abs/1712.09048" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning Intelligent Dialogs for Bounding Box Annotation (<a href="https://arxiv.org/abs/1712.08087" target="_blank" rel="noopener">PDF</a>)</li>
<li>Real-time video stabilization and mosaicking for monitoring and surveillance (<a href="http://ieeexplore.ieee.org/document/7886813/?reload=true" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com//a-jahani/Real-time-Video-Mosaic" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Learning Recursive Filter for Low-Level Vision via a Hybrid Neural Network (<a href="http://faculty.ucmerced.edu/mhyang/papers/eccv16_rnn_filter.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/Liusifei/caffe-lowlevel" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding(<a href="https://arxiv.org/abs/1510.00149" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/songhan/Deep-Compression-AlexNet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>A Learned Representation For Artistic Style(<a href="https://arxiv.org/abs/1610.07629" target="_blank" rel="noopener">PDF</a>)</li>
<li>Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification (<a href="http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/data/colorization_sig2016.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/satoshiiizuka/siggraph2016_colorization" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Pixel Recurrent Neural Networks (<a href="https://arxiv.org/abs/1601.06759" target="_blank" rel="noopener">PDF</a>)</li>
<li>Conditional Image Generation with PixelCNN Decoders (<a href="https://arxiv.org/abs/1606.05328" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/dritchie/pixelCNN" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>RAISR: Rapid and Accurate Image Super Resolution (<a href="https://arxiv.org/abs/1606.01299" target="_blank" rel="noopener">PDF</a>)</li>
<li>Photo-Quality Evaluation based on Computational Aesthetics: Review of Feature Extraction Techniques (<a href="https://arxiv.org/abs/1612.06259" target="_blank" rel="noopener">PDF</a>)</li>
<li>Fast color transfer from multiple images (<a href="https://arxiv.org/abs/1612.08927" target="_blank" rel="noopener">PDF</a>)</li>
<li>Bringing Impressionism to Life with Neural Style Transfer in Come Swim (<a href="https://arxiv.org/abs/1701.04928" target="_blank" rel="noopener">PDF</a>)</li>
<li>PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications (<a href="https://arxiv.org/abs/1701.05517" target="_blank" rel="noopener">PDF</a>, (Project/CODE)[<a href="https://github.com/openai/pixel-cnn" target="_blank" rel="noopener">https://github.com/openai/pixel-cnn</a>])</li>
<li>Deep Photo Style Transfer (<a href="https://arxiv.org/abs/1703.07511" target="_blank" rel="noopener">PDF</a>)</li>
<li>A Neural Representation of Sketch Drawings (<a href="https://arxiv.org/abs/1704.03477" target="_blank" rel="noopener">PDF</a>)</li>
<li>Visual Attribute Transfer through Deep Image Analogy (<a href="https://arxiv.org/abs/1705.01088" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Semantics-Aware Photo Adjustment (<a href="https://arxiv.org/abs/1706.08260" target="_blank" rel="noopener">PDF</a>)</li>
<li>Diversified Texture Synthesis with Feed-forward Networks (<a href="https://arxiv.org/abs/1703.01664" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/Yijunmaverick/MultiTextureSynthesis" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Real-Time Neural Style Transfer for Videos (<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Real-Time_Neural_Style_CVPR_2017_paper.pdf" target="_blank" rel="noopener">PDF</a>)</li>
<li>Creatism: A deep-learning photographer capable of creating professional work (<a href="https://arxiv.org/abs/1707.03491" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Image Harmonization (<a href="https://arxiv.org/abs/1703.00069" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/wasidennis/DeepHarmonization" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Neural Color Transfer between Images (<a href="https://arxiv.org/abs/1710.00756" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deeper, Broader and Artier Domain Generalization (<a href="https://arxiv.org/abs/1710.03077" target="_blank" rel="noopener">PDF</a>)</li>
</ul>
<h3 id="3D-DEPTH-POINT-CLOUD"><a href="#3D-DEPTH-POINT-CLOUD" class="headerlink" title="3D/DEPTH/POINT CLOUD"></a>3D/DEPTH/POINT CLOUD</h3><ul>
<li>The Perfect Match: 3D Point Cloud Matching with Smoothed Densities (<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Gojcic_The_Perfect_Match_3D_Point_Cloud_Matching_With_Smoothed_Densities_CVPR_2019_paper.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/zgojcic/3DSmoothNet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling (<a href="https://arxiv.org/abs/1804.04610" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/xingyuansun/pix3d" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Depth from Videos in the Wild: Unsupervised Monocular Depth Learning from Unknown Cameras (<a href="https://arxiv.org/abs/1904.04998" target="_blank" rel="noopener">PDF</a>)</li>
</ul>
<h3 id="CNN-AND-DEEP-LEARNING"><a href="#CNN-AND-DEEP-LEARNING" class="headerlink" title="CNN AND DEEP LEARNING"></a>CNN AND DEEP LEARNING</h3><ul>
<li>ResNeSt: Split-Attention Networks (<a href="https://arxiv.org/abs/2004.08955" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/zhanghang1989/ResNeSt" target="_blank" rel="noopener">Project/Code</a>, <a href="/2020/05/04/Computer_Vision/Reading_Note/2020-05-04-ResNeSt/">Reading Note</a>)</li>
<li>Meta-Learning in Neural Networks: A Survey (<a href="https://arxiv.org/abs/2004.05439" target="_blank" rel="noopener">PDF</a>, )</li>
<li>Generalizing from a Few Examples: A Survey on Few-Shot Learning (<a href="https://arxiv.org/abs/1904.05046" target="_blank" rel="noopener">PDF</a>)</li>
<li>NBDT: Neural-Backed Decision Trees (<a href="https://arxiv.org/abs/2004.00221" target="_blank" rel="noopener">PDF</a>, <a href="http://nbdt.alvinwan.com/" target="_blank" rel="noopener">Project/Code</a>, <a href="https://github.com/alvinwan/neural-backed-decision-trees" target="_blank" rel="noopener">Github</a>, <a href="https://joshua19881228.github.io/2020/05/01/Computer_Vision/Reading_Note/2020-05-01-NBDT/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Interpretable CNNs (<a href="https://arxiv.org/abs/1901.02413" target="_blank" rel="noopener">PDF</a>)</li>
<li>Bag of Tricks for Image Classification with Convolutional Neural Networks (<a href="https://arxiv.org/abs/1812.01187" target="_blank" rel="noopener">PDF</a>)</li>
<li>How Does Batch Normalization Help Optimization? (<a href="https://arxiv.org/abs/1805.11604" target="_blank" rel="noopener">PDF</a>, <a href="https://www.youtube.com/watch?v=ZOabsYbmBRM" target="_blank" rel="noopener">VIDEO</a>)</li>
<li><a href="https://arxiv.org/abs/1805.07883" target="_blank" rel="noopener">https://arxiv.org/abs/1805.07883</a> (<a href="https://arxiv.org/abs/1805.07883" target="_blank" rel="noopener">PDF</a>)</li>
<li>Rethinking ImageNet Pre-training (<a href="https://arxiv.org/abs/1811.08883" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning From Positive and Unlabeled Data: A Survey (<a href="https://arxiv.org/abs/1811.04820" target="_blank" rel="noopener">PDF</a>)</li>
<li>Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks (<a href="https://arxiv.org/pdf/1810.12348.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/hujie-frank/GENet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>DropBlock: A regularization method for convolutional networks (<a href="https://arxiv.org/abs/1810.12890" target="_blank" rel="noopener">PDF</a>)</li>
<li>Differentiable Abstract Interpretation for Provably Robust Neural Networks (<a href="https://www.sri.inf.ethz.ch/papers/icml18-diffai.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/eth-sri/diffai" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Adding One Neuron Can Eliminate All Bad Local Minima (<a href="https://arxiv.org/abs/1805.08671" target="_blank" rel="noopener">PDF</a>)</li>
<li>Step Size Matters in Deep Learning (<a href="https://arxiv.org/abs/1805.08890" target="_blank" rel="noopener">PDF</a>)</li>
<li>Do Better ImageNet Models Transfer Better? (<a href="https://arxiv.org/abs/1805.08974" target="_blank" rel="noopener">PDF</a>)</li>
<li>Robust Classification with Convolutional Prototype Learning (<a href="https://arxiv.org/abs/1805.03438" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/YangHM/Convolutional-Prototype-Learning" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Fast Feature Extraction with CNNs with Pooling Layers (<a href="https://arxiv.org/abs/1805.03096" target="_blank" rel="noopener">PDF</a>)</li>
<li>Network Transplanting (<a href="https://arxiv.org/abs/1804.10272" target="_blank" rel="noopener">PDF</a>)</li>
<li>An Information-Theoretic View for Deep Learning (<a href="https://arxiv.org/abs/1804.09060" target="_blank" rel="noopener">PDF</a>)</li>
<li>Understanding Individual Neuron Importance Using Information Theory (<a href="https://arxiv.org/abs/1804.06679" target="_blank" rel="noopener">PDF</a>)</li>
<li>Understanding Convolutional Neural Network Training with Information Theory (<a href="https://arxiv.org/abs/1804.06537" target="_blank" rel="noopener">PDF</a>)</li>
<li>The unreasonable effectiveness of the forget gate (<a href="https://arxiv.org/abs/1804.04849" target="_blank" rel="noopener">PDF</a>)</li>
<li>Discovering Hidden Factors of Variation in Deep Networks (<a href="https://arxiv.org/abs/1412.6583" target="_blank" rel="noopener">PDF</a>)</li>
<li>Regularizing Deep Networks by Modeling and Predicting Label Structure (<a href="https://arxiv.org/abs/1804.02009" target="_blank" rel="noopener">PDF</a>)</li>
<li>Hierarchical Novelty Detection for Visual Object Recognition (<a href="https://arxiv.org/abs/1804.00722" target="_blank" rel="noopener">PDF</a>)</li>
<li>Guide Me: Interacting with Deep Networks (<a href="https://arxiv.org/abs/1803.11544" target="_blank" rel="noopener">PDF</a>)</li>
<li>Studying Invariances of Trained Convolutional Neural Networks (<a href="https://arxiv.org/abs/1803.05963" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Residual Networks and Weight Initialization (<a href="https://arxiv.org/abs/1709.02956" target="_blank" rel="noopener">PDF</a>)</li>
<li>WNGrad: Learn the Learning Rate in Gradient Descent (<a href="https://arxiv.org/abs/1803.02865" target="_blank" rel="noopener">PDF</a>)</li>
<li>Understanding the Loss Surface of Neural Networks for Binary Classification (<a href="https://arxiv.org/abs/1803.00909" target="_blank" rel="noopener">PDF</a>)</li>
<li>Tell Me Where to Look: Guided Attention Inference Network (<a href="https://arxiv.org/abs/1802.10171" target="_blank" rel="noopener">PDF</a>)</li>
<li>Convolutional Neural Networks with Alternately Updated Clique (<a href="https://arxiv.org/abs/1802.10419" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/iboing/CliqueNet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Visual Interpretability for Deep Learning: a Survey (<a href="https://arxiv.org/abs/1802.00614" target="_blank" rel="noopener">PDF</a>)</li>
<li>Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey (<a href="https://arxiv.org/abs/1801.00553" target="_blank" rel="noopener">PDF</a>)</li>
<li>CNNs are Globally Optimal Given Multi-Layer Support (<a href="https://arxiv.org/abs/1712.02501" target="_blank" rel="noopener">PDF</a>)</li>
<li>Take it in your stride: Do we need striding in CNNs? (<a href="https://arxiv.org/abs/1712.02502" target="_blank" rel="noopener">PDF</a>)</li>
<li>Gradients explode - Deep Networks are shallow - ResNet explained (<a href="https://arxiv.org/abs/1712.05577" target="_blank" rel="noopener">PDF</a>)</li>
<li>Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates (<a href="https://arxiv.org/abs/1708.07120" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/lnsmith54/super-convergence" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Data Distillation: Towards Omni-Supervised Learning (<a href="https://arxiv.org/abs/1712.04440" target="_blank" rel="noopener">PDF</a>)</li>
<li>Peephole: Predicting Network Performance Before Training (<a href="https://arxiv.org/abs/1712.03351" target="_blank" rel="noopener">PDF</a>)</li>
<li>AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks (<a href="https://arxiv.org/abs/1712.02029" target="_blank" rel="noopener">PDF</a>)</li>
<li>Gradual Tuning: a better way of Fine Tuning the parameters of a Deep Neural Network (<a href="https://arxiv.org/abs/1711.10177" target="_blank" rel="noopener">PDF</a>)</li>
<li>CondenseNet: An Efficient DenseNet using Learned Group Convolutions (<a href="http://www.cs.cornell.edu/~gaohuang/papers/condensenet.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com//ShichenLiu/CondenseNet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Population Based Training of Neural Networks (<a href="https://arxiv.org/abs/1711.09846" target="_blank" rel="noopener">PDF</a>)</li>
<li>Knowledge Concentration: Learning 100K Object Classifiers in a Single CNN (<a href="https://arxiv.org/abs/1711.07607" target="_blank" rel="noopener">PDF</a>)</li>
<li>Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions (<a href="https://arxiv.org/abs/1711.08141" target="_blank" rel="noopener">PDF</a>)</li>
<li>Unleashing the Potential of CNNs for Interpretable Few-Shot Learning (<a href="https://arxiv.org/abs/1711.08277" target="_blank" rel="noopener">PDF</a>)</li>
<li>Non-local Neural Networks (<a href="https://arxiv.org/abs/1711.07971" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/facebookresearch/video-nonlocal-net" target="_blank" rel="noopener">Caffe2</a>)</li>
<li>Log-DenseNet: How to Sparsify a DenseNet (<a href="https://arxiv.org/abs/1711.00002g" target="_blank" rel="noopener">PDF</a>)</li>
<li>Don’t Decay the Learning Rate, Increase the Batch Size (<a href="https://arxiv.org/abs/1711.00489" target="_blank" rel="noopener">PDF</a>)</li>
<li>Guarding Against Adversarial Domain Shifts with Counterfactual Regularization (<a href="https://arxiv.org/abs/1710.11469" target="_blank" rel="noopener">PDF</a>)</li>
<li>UberNet: Training a ‘Universal’ Convolutional Neural Network for Low-, Mid-, and High-Level Vision using Diverse Datasets and Limited Memory (<a href="http://arxiv.org/abs/1609.02132" target="_blank" rel="noopener">PDF</a>, <a href="http://cvn.ecp.fr/ubernet/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li><p>What makes ImageNet good for transfer learning? (<a href="http://arxiv.org/abs/1608.08614" target="_blank" rel="noopener">PDF</a>, <a href="http://minyounghuh.com/papers/analysis/" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2016-09-06-Reading-note/" target="_blank" rel="noopener">Reading Note</a>)</p>
<blockquote>
<p>The tremendous success of features learnt using the ImageNet classification task on a wide range of transfer tasks begs the question: what are the intrinsic properties of the ImageNet dataset that are critical for learning good, general-purpose features? This work provides an empirical investigation of various facets of this question: Is more pre-training data always better? How does feature quality depend on the number of training examples per class? Does adding more object classes improve performance? For the same data budget, how should the data be split into classes? Is fine-grained recognition necessary for learning good features? Given the same number of training classes, is it better to have coarse classes or fine-grained classes? Which is better: more classes or more examples per class?</p>
</blockquote>
</li>
<li><p>Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units (<a href="http://arxiv.org/abs/1603.05201" target="_blank" rel="noopener">PDF</a>)</p>
</li>
<li>Densely Connected Convolutional Networks (<a href="http://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/liuzhuang13/DenseNet" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2016-11-24-DenseNet/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li><p>Decoupled Neural Interfaces using Synthetic Gradients (<a href="https://arxiv.org/pdf/1608.05343.pdf" target="_blank" rel="noopener">PDF</a>)</p>
<blockquote>
<p>Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modeled sub-graph will produce using only local information. In particular we focus on modeling error gradients: by using the modeled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously.</p>
</blockquote>
</li>
<li><p>Rethinking the Inception Architecture for Computer Vision (<a href="http://arxiv.org/abs/1512.00567" target="_blank" rel="noopener">PDF</a>, <a href="http://joshua881228.webfactional.com/blog_reading-note-rethinking-the-inception-architecture-for-computer-vision_136/" target="_blank" rel="noopener">Reading Note</a>)</p>
<p>In this paper, several network designing choices are discussed, including <em>factorizing convolutions into smaller kernels and asymmetric kernels</em>, <em>utility of auxiliary classifiers</em> and <em>reducing grid size using convolution stride rather than pooling</em>.</p>
</li>
<li><p>Factorized Convolutional Neural Networks (<a href="http://arxiv.org/abs/1608.04337" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-08-19-Reading-note/" target="_blank" rel="noopener">Reading Note</a>)</p>
</li>
<li>Do semantic parts emerge in Convolutional Neural Networks? (<a href="http://arxiv.org/abs/1607.03738" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-08-09-Reading-Note/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>A Critical Review of Recurrent Neural Networks for Sequence Learning (<a href="https://arxiv.org/abs/1506.00019" target="_blank" rel="noopener">PDF</a>)</li>
<li>Image Compression with Neural Networks (<a href="https://github.com/tensorflow/models/tree/master/compression" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Graph Convolutional Networks (<a href="http://tkipf.github.io/graph-convolutional-networks/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Understanding intermediate layers using linear classifier probes (<a href="https://arxiv.org/abs/1610.01644" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-10-28-linear-classifier-probe/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Learning What and Where to Draw (<a href="http://www.scottreed.info/files/nips2016.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/reedscot/nips2016" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>On the interplay of network structure and gradient convergence in deep learning (<a href="https://arxiv.org/abs/1511.05297" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Learning with Separable Convolutions (<a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="noopener">PDF</a>)</li>
<li>Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization (<a href="https://arxiv.org/abs/1610.02391" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/ramprs/grad-cam/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Optimization of Convolutional Neural Network using Microcanonical Annealing Algorithm (<a href="https://arxiv.org/abs/1610.02306" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Pyramidal Residual Networks (<a href="https://arxiv.org/abs/1610.02915" target="_blank" rel="noopener">PDF</a>)</li>
<li>Impatient DNNs - Deep Neural Networks with Dynamic Time Budgets (<a href="https://arxiv.org/abs/1610.02850" target="_blank" rel="noopener">PDF</a>)</li>
<li>Uncertainty in Deep Learning (<a href="http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf" target="_blank" rel="noopener">PDF</a>, <a href="http://mlg.eng.cam.ac.uk/yarin/blog_2248.html" target="_blank" rel="noopener">Project/Code</a>)<br>This is the PhD Thesis of Yarin Gal.</li>
<li>Tensorial Mixture Models (<a href="https://arxiv.org/abs/1610.04167" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/HUJI-Deep/TMM" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks (<a href="https://arxiv.org/abs/1602.03616" target="_blank" rel="noopener">PDF</a>)</li>
<li>Why Deep Neural Networks? (<a href="https://arxiv.org/abs/1610.04161" target="_blank" rel="noopener">PDF</a>)</li>
<li>Local Similarity-Aware Deep Feature Embedding (<a href="https://arxiv.org/abs/1610.08904" target="_blank" rel="noopener">PDF</a>)</li>
<li>A Review of 40 Years of Cognitive Architecture Research: Focus on Perception, Attention, Learning and Applications (<a href="https://arxiv.org/abs/1610.08602" target="_blank" rel="noopener">PDF</a>)</li>
<li>Professor Forcing: A New Algorithm for Training Recurrent Networks (<a href="https://arxiv.org/abs/1610.09038" target="_blank" rel="noopener">PDF</a>)</li>
<li>On the expressive power of deep neural networks(<a href="https://arxiv.org/abs/1606.05336" target="_blank" rel="noopener">PDF</a>)</li>
<li>What Is the Best Practice for CNNs Applied to Visual Instance Retrieval? (<a href="https://arxiv.org/abs/1611.01640" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Convolutional Neural Network Design Patterns (<a href="https://arxiv.org/abs/1611.00847" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/iPhysicist/CNNDesignPatterns" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Tricks from Deep Learning (<a href="https://arxiv.org/abs/1611.03777" target="_blank" rel="noopener">PDF</a>)</li>
<li>A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models (<a href="https://arxiv.org/abs/1611.03852" target="_blank" rel="noopener">PDF</a>)</li>
<li>Multi-Shot Mining Semantic Part Concepts in CNNs (<a href="https://arxiv.org/abs/1611.04246" target="_blank" rel="noopener">PDF</a>)</li>
<li>Aggregated Residual Transformations for Deep Neural Networks (<a href="https://arxiv.org/abs/1611.05431" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-11-17-ResNeXt/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>PolyNet: A Pursuit of Structural Diversity in Very Deep Networks (<a href="https://arxiv.org/abs/1611.05725" target="_blank" rel="noopener">PDF</a>)</li>
<li>On the Exploration of Convolutional Fusion Networks for Visual Recognition (<a href="https://arxiv.org/abs/1611.05503" target="_blank" rel="noopener">PDF</a>)</li>
<li>ResFeats: Residual Network Based Features for Image Classification (<a href="https://arxiv.org/abs/1611.06656" target="_blank" rel="noopener">PDF</a>)</li>
<li>Object Recognition with and without Objects (<a href="https://arxiv.org/abs/1611.06596" target="_blank" rel="noopener">PDF</a>)</li>
<li>LCNN: Lookup-based Convolutional Neural Network (<a href="https://arxiv.org/abs/1611.06473" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-11-23-LCNN/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Inductive Bias of Deep Convolutional Networks through Pooling Geometry (<a href="https://arxiv.org/abs/1605.06743" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/HUJI-Deep/inductive-pooling" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Wider or Deeper: Revisiting the ResNet Model for Visual Recognition (<a href="https://arxiv.org/abs/1611.10080" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2016-12-07-WiderResNet/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Multi-Scale Context Aggregation by Dilated Convolutions (<a href="https://arxiv.org/abs/1511.07122" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/fyu/dilation" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Large-Margin Softmax Loss for Convolutional Neural Networks (<a href="https://arxiv.org/abs/1612.02295" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/luoyetx/mx-lsoftmax" target="_blank" rel="noopener">mxnet Code</a>, <a href="https://github.com/wy1iu/LargeMargin_Softmax_Loss" target="_blank" rel="noopener">Caffe Code</a>)</li>
<li>Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics (<a href="https://arxiv.org/abs/1612.07767" target="_blank" rel="noopener">PDF</a>)</li>
<li>Feedback Networks (<a href="https://arxiv.org/abs/1612.09508" target="_blank" rel="noopener">PDF</a>)</li>
<li>Visualizing Residual Networks (<a href="https://arxiv.org/abs/1701.02362" target="_blank" rel="noopener">PDF</a>)</li>
<li>Convolutional Oriented Boundaries: From Image Segmentation to High-Level Tasks (<a href="https://arxiv.org/abs/1701.04658" target="_blank" rel="noopener">PDF</a>, <a href="http://www.vision.ee.ethz.ch/~cvlsegmentation/cob/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Understanding trained CNNs by indexing neuron selectivity (<a href="https://arxiv.org/abs/1702.00382" target="_blank" rel="noopener">PDF</a>)</li>
<li>Benchmarking State-of-the-Art Deep Learning Software Tools (<a href="https://arxiv.org/abs/1608.07249" target="_blank" rel="noopener">PDF</a>, <a href="http://dlbench.comp.hkbu.edu.hk/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models (<a href="https://arxiv.org/abs/1702.03275" target="_blank" rel="noopener">PDF</a>)</li>
<li>Visualizing Deep Neural Network Decisions: Prediction Difference Analysis (<a href="https://arxiv.org/abs/1702.04595" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/lmzintgraf/DeepVis-PredDiff" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>ShaResNet: reducing residual network parameter number by sharing weights (<a href="https://arxiv.org/abs/1702.08782" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Forest: Towards An Alternative to Deep Neural Networks (<a href="https://arxiv.org/abs/1702.08835" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/kingfengji/gcForest" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>All You Need is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation (<a href="https://arxiv.org/abs/1703.01827" target="_blank" rel="noopener">PDF</a>)</li>
<li>Genetic CNN (<a href="https://arxiv.org/abs/1703.01513" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deformable Convolutional Networks (<a href="https://arxiv.org/abs/1703.06211" target="_blank" rel="noopener">PDF</a>)</li>
<li>Quality Resilient Deep Neural Networks (<a href="https://arxiv.org/abs/1703.08119" target="_blank" rel="noopener">PDF</a>)</li>
<li>How ConvNets model Non-linear Transformations (<a href="https://arxiv.org/abs/1702.07664" target="_blank" rel="noopener">PDF</a>)</li>
<li>Active Convolution: Learning the Shape of Convolution for Image Classification (<a href="https://arxiv.org/abs/1703.09076" target="_blank" rel="noopener">PDF</a>)</li>
<li>Multi-Scale Dense Convolutional Networks for Efficient Prediction (<a href="https://arxiv.org/abs/1703.09844" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/gaohuang/MSDNet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Coordinating Filters for Faster Deep Neural Networks (<a href="https://arxiv.org/abs/1703.09746" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/wenwei202/caffe/tree/sfm" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>A Genetic Programming Approach to Designing Convolutional Neural Network Architectures (<a href="https://arxiv.org/abs/1704.00764" target="_blank" rel="noopener">PDF</a>)</li>
<li>On Generalization and Regularization in Deep Learning (<a href="https://arxiv.org/abs/1704.01312" target="_blank" rel="noopener">PDF</a>)</li>
<li>Interpretable Explanations of Black Boxes by Meaningful Perturbation (<a href="https://arxiv.org/abs/1704.03296" target="_blank" rel="noopener">PDF</a>)</li>
<li>Energy Propagation in Deep Convolutional Neural Networks (<a href="https://arxiv.org/abs/1704.03636" target="_blank" rel="noopener">PDF</a>)</li>
<li>Introspection: Accelerating Neural Network Training By Learning Weight Evolution (<a href="https://arxiv.org/abs/1704.04959" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deeply-Supervised Nets (<a href="https://arxiv.org/abs/1409.5185" target="_blank" rel="noopener">PDF</a>)</li>
<li>Speeding up Convolutional Neural Networks By Exploiting the Sparsity of Rectifier Units (<a href="https://arxiv.org/abs/1704.07724" target="_blank" rel="noopener">PDF</a>)</li>
<li>Inception Recurrent Convolutional Neural Network for Object Recognition (<a href="https://arxiv.org/abs/1704.07709" target="_blank" rel="noopener">PDF</a>)</li>
<li>Residual Attention Network for Image Classification (<a href="https://arxiv.org/abs/1704.06904" target="_blank" rel="noopener">PDF</a>)</li>
<li>The Landscape of Deep Learning Algorithms (<a href="https://arxiv.org/abs/1705.07038" target="_blank" rel="noopener">PDF</a>)</li>
<li>Pixel Deconvolutional Networks (<a href="https://arxiv.org/abs/1705.06820" target="_blank" rel="noopener">PDF</a>)</li>
<li>Dilated Residual Networks (<a href="https://arxiv.org/abs/1705.09914" target="_blank" rel="noopener">PDF</a>)</li>
<li>A Kernel Redundancy Removing Policy for Convolutional Neural Network (<a href="https://arxiv.org/abs/1705.10748" target="_blank" rel="noopener">PDF</a>)</li>
<li>Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour (<a href="https://arxiv.org/abs/1706.02677" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning Spatial Regularization with Image-level Supervisions for Multi-label Image Classification (<a href="https://arxiv.org/abs/1702.05891" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/zhufengx/SRN_multilabel/" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2017-07-06-SRN/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>VisualBackProp: efficient visualization of CNNs (<a href="https://arxiv.org/abs/1611.05418" target="_blank" rel="noopener">PDF</a>)</li>
<li>Pruning Convolutional Neural Networks for Resource Efficient Inference (<a href="https://arxiv.org/abs/1611.06440" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/jacobgil/pytorch-pruning" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Zero-Shot Learning - A Comprehensive Evaluation of the Good, the Bad and the Ugly (<a href="https://arxiv.org/abs/1707.00600" target="_blank" rel="noopener">PDF</a>, <a href="http://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/zero-shot-learning/zero-shot-learning-the-good-the-bad-and-the-ugly/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices (<a href="https://arxiv.org/abs/1707.01083" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/camel007/Caffe-ShuffleNet" target="_blank" rel="noopener">Caffe Implementation</a>)</li>
<li>Submanifold Sparse Convolutional Networks (<a href="https://arxiv.org/abs/1706.01307" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/facebookresearch/SparseConvNet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Dual Path Networks (<a href="https://arxiv.org/abs/1707.01629" target="_blank" rel="noopener">PDF</a>)</li>
<li>ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression (<a href="https://arxiv.org/abs/1707.06342" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/Roll920/ThiNet" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2017-08-03-ThiNet/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Memory-Efficient Implementation of DenseNets (<a href="https://arxiv.org/abs/1707.06990" target="_blank" rel="noopener">PDF</a>)</li>
<li>Residual Attention Network for Image Classification (<a href="https://arxiv.org/abs/1704.06904" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/buptwangfei/residual-attention-network" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>An Effective Training Method For Deep Convolutional Neural Network (<a href="https://arxiv.org/abs/1708.01666" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning to Transfer (<a href="https://arxiv.org/abs/1708.05629" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning Efficient Convolutional Networks through Network Slimming (<a href="https://arxiv.org/abs/1708.06519" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/liuzhuang13/slimming" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates (<a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/lnsmith54/super-convergence" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Hierarchical loss for classification (<a href="https://arxiv.org/abs/1709.01062" target="_blank" rel="noopener">PDF</a>)</li>
<li>Convolutional Gaussian Processes (<a href="https://arxiv.org/abs/1709.01894" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/markvdw/convgp" target="_blank" rel="noopener">Code/Project</a>)</li>
<li>Interpretable Convolutional Neural Networks (<a href="https://arxiv.org/abs/1710.00935" target="_blank" rel="noopener">PDF</a>)</li>
<li>What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? (<a href="https://arxiv.org/abs/1703.04977" target="_blank" rel="noopener">PDF</a>)</li>
<li>Porcupine Neural Networks: (Almost) All Local Optima are Global (<a href="https://arxiv.org/abs/1710.02196" target="_blank" rel="noopener">PDF</a>)</li>
<li>Generalization in Deep Learning (<a href="https://arxiv.org/abs/1710.05468" target="_blank" rel="noopener">PDF</a>)</li>
<li>A systematic study of the class imbalance problem in convolutional neural networks (<a href="https://arxiv.org/abs/1710.05381" target="_blank" rel="noopener">PDF</a>)</li>
<li>Interpretable Transformations with Encoder-Decoder Networks (<a href="https://arxiv.org/abs/1710.07307" target="_blank" rel="noopener">PDF</a>, <a href="http://visual.cs.ucl.ac.uk/pubs/interpTransform/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>One pixel attack for fooling deep neural networks (<a href="https://arxiv.org/abs/1710.08864" target="_blank" rel="noopener">PDF</a>)</li>
</ul>
<h3 id="SINGLE-SHOT-UNSUPERVISED-LEARNING"><a href="#SINGLE-SHOT-UNSUPERVISED-LEARNING" class="headerlink" title="SINGLE-SHOT/UNSUPERVISED LEARNING"></a>SINGLE-SHOT/UNSUPERVISED LEARNING</h3><ul>
<li>Zero-Shot Object Detection by Hybrid Region Embedding (<a href="https://arxiv.org/abs/1805.06157" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/syb7573330/im2avatar" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Deep Triplet Ranking Networks for One-Shot Recognition (<a href="https://arxiv.org/abs/1804.07275" target="_blank" rel="noopener">PDF</a>)</li>
<li>Avatar-Net: Multi-scale Zero-shot Style Transfer by Feature Decoration (<a href="https://arxiv.org/abs/1805.03857" target="_blank" rel="noopener">PDF</a>)</li>
</ul>
<h3 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h3><ul>
<li>A Survey on GANs for Anomaly Detection (<a href="https://arxiv.org/abs/1906.11632" target="_blank" rel="noopener">PDF</a>)</li>
<li>Outfit Generation and Style Extraction via Bidirectional LSTM and Autoencoder (<a href="https://arxiv.org/abs/1807.03133" target="_blank" rel="noopener">PDF</a>)</li>
<li>Pioneer Networks: Progressively Growing Generative Autoencoder (<a href="https://arxiv.org/abs/1807.03026" target="_blank" rel="noopener">PDF</a>)</li>
<li>Transferring GANs: generating images from limited data (<a href="https://arxiv.org/abs/1805.01677" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/yaxingwang/Transferring-GANs" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Painting Generation Using Conditional Generative Adversarial Net (<a href="http://adeel.io/sncgan/mlp_cw4.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/AdeelMufti/SNcGAN" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>MGGAN: Solving Mode Collapse using Manifold Guided Training (<a href="https://arxiv.org/abs/1804.04391" target="_blank" rel="noopener">PDF</a>)</li>
<li>Multimodal Unsupervised Image-to-Image Translation (<a href="https://arxiv.org/abs/1804.04732" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/nvlabs/MUNIT" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and Beyond (<a href="https://arxiv.org/abs/1804.02047" target="_blank" rel="noopener">PDF</a>)</li>
<li>Face Aging with Contextual Generative Adversarial Nets (<a href="https://arxiv.org/abs/1802.00237" target="_blank" rel="noopener">PDF</a>, <a href="https://www.arxiv-vanity.com/papers/1802.00237/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Deformable GANs for Pose-based Human Image Generation (<a href="https://arxiv.org/abs/1801.00055" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com//AliaksandrSiarohin/pose-gan" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>ComboGAN: Unrestrained Scalability for Image Domain Translation (<a href="https://arxiv.org/abs/1712.06909" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com//AAnoosheh/ComboGAN" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Eye In-Painting with Exemplar Generative Adversarial Networks (<a href="https://arxiv.org/abs/1712.03999" target="_blank" rel="noopener">PDF</a>)</li>
<li>Disentangled Person Image Generation (<a href="https://arxiv.org/abs/1712.02621" target="_blank" rel="noopener">PDF</a>)</li>
<li>Fader Networks: Manipulating Images by Sliding Attributes (<a href="https://arxiv.org/abs/1706.00409" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/facebookresearch/FaderNetworks" target="_blank" rel="noopener">Code/Project</a>)</li>
<li>Are GANs Created Equal? A Large-Scale Study (<a href="https://arxiv.org/abs/1711.10337" target="_blank" rel="noopener">PDF</a>)</li>
<li>StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation (<a href="https://arxiv.org/abs/1711.09020" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com//yunjey/StarGAN" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Two Birds with One Stone: Iteratively Learn Facial Attributes with GANs (<a href="https://arxiv.org/abs/1711.06078" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com//punkcure/Iterative-GAN" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Spectral Normalization for Generative Adversarial Networks (<a href="https://openreview.net/pdf?id=B1QRgziT-" target="_blank" rel="noopener">PDF</a>)</li>
<li>XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings (<a href="https://arxiv.org/abs/1711.05139" target="_blank" rel="noopener">PDF</a>)</li>
<li>How Generative Adversarial Nets and its variants Work: An Overview of GAN (<a href="https://arxiv.org/abs/1711.05914" target="_blank" rel="noopener">PDF</a>)</li>
<li>DNA-GAN: Learning Disentangled Representations from Multi-Attribute Images (<a href="https://arxiv.org/abs/1711.05415" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com//Prinsphield/DNA-GAN" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Sobolev GAN (<a href="https://arxiv.org/abs/1711.04894" target="_blank" rel="noopener">PDF</a>)</li>
<li>Data Augmentation Generative Adversarial Networks (<a href="https://arxiv.org/abs/1711.04340" target="_blank" rel="noopener">PDF</a>)</li>
<li>Conditional Autoencoders with Adversarial Information Factorization (<a href="https://arxiv.org/abs/1711.05175" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/ToniCreswell/attribute-cVAEGAN" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Progressive Growing of GANs for Improved Quality, Stability, and Variation (<a href="https://arxiv.org/abs/1710.10196" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/tkarras/progressive_growing_of_gans" target="_blank" rel="noopener">Project/Code</a>, <a href="https://github.com/nashory/progressive-growing-torch" target="_blank" rel="noopener">Torch</a>, <a href="https://github.com//github-pengge/PyTorch-progressive_growing_of_gans" target="_blank" rel="noopener">PyTorch</a>, <a href="https://joshua19881228.github.io/2017-11-12-ProgressiveGrowing/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Bayesian GAN (<a href="https://arxiv.org/abs/1705.09558" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com//andrewgordonwilson/bayesgan/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Metric Learning-based Generative Adversarial Network (<a href="https://arxiv.org/abs/1711.02792" target="_blank" rel="noopener">PDF</a>)</li>
<li>Flexible Prior Distributions for Deep Generative Models (<a href="https://arxiv.org/abs/1710.11383" target="_blank" rel="noopener">PDF</a>)</li>
<li>Data Augmentation in Classification using GAN (<a href="https://arxiv.org/abs/1711.00648" target="_blank" rel="noopener">PDF</a>)</li>
<li>Semantically Decomposing the Latent Spaces of Generative Adversarial Networks (<a href="https://arxiv.org/abs/1705.07904" target="_blank" rel="noopener">PDF</a>)</li>
<li>Multi-View Data Generation Without View Supervision (<a href="https://arxiv.org/abs/1711.00305" target="_blank" rel="noopener">PDF</a>)</li>
<li>StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks (<a href="https://arxiv.org/abs/1710.10916" target="_blank" rel="noopener">PDF</a>)</li>
<li>Generative Adversarial Networks (<a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">PDF</a>)</li>
<li>Stacked Generative Adversarial Networks (<a href="https://arxiv.org/abs/1612.04357" target="_blank" rel="noopener">PDF</a>)</li>
<li>Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks (<a href="https://arxiv.org/abs/1612.05424" target="_blank" rel="noopener">PDF</a>)</li>
<li>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (<a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks (<a href="https://arxiv.org/abs/1506.05751" target="_blank" rel="noopener">PDF</a>)</li>
<li>NIPS 2016 Tutorial: Generative Adversarial Networks (<a href="https://arxiv.org/abs/1701.00160" target="_blank" rel="noopener">PDF</a>)</li>
<li>Wasserstein GAN (<a href="https://arxiv.org/abs/1701.07875" target="_blank" rel="noopener">PDF</a>)</li>
<li>Adversarial Discriminative Domain Adaptation (<a href="https://arxiv.org/abs/1702.05464" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2017-02-22-ADDA/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Generative Adversarial Nets with Labeled Data by Activation Maximization (<a href="https://arxiv.org/abs/1703.02000" target="_blank" rel="noopener">PDF</a>)</li>
<li>Triple Generative Adversarial Nets (<a href="https://arxiv.org/abs/1703.02291" target="_blank" rel="noopener">PDF</a>)</li>
<li>On the Quantative Evaluation of Deep Generative Models (<a href="http://www.cs.cmu.edu/~rsalakhu/talk_Eval.pdf" target="_blank" rel="noopener">PDF</a>)</li>
<li>Adversarial Transformation Networks: Learning to Generate Adversarial Examples (<a href="https://arxiv.org/abs/1703.09387" target="_blank" rel="noopener">PDF</a>)</li>
<li>Improved Training of Wasserstein GANs (<a href="https://arxiv.org/abs/1704.00028" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/igul222/improved_wgan_trainin" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Generate To Adapt: Aligning Domains using Generative Adversarial Networks (<a href="https://arxiv.org/abs/1704.01705" target="_blank" rel="noopener">PDF</a>)</li>
<li>Adversarial Generator-Encoder Networks (<a href="https://arxiv.org/abs/1704.02304" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/DmitryUlyanov/AGE" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Training Triplet Networks with GAN (<a href="https://arxiv.org/abs/1704.02227" target="_blank" rel="noopener">PDF</a>)</li>
<li>Multi-Agent Diverse Generative Adversarial Networks (<a href="https://arxiv.org/abs/1704.02906" target="_blank" rel="noopener">PDF</a>)</li>
<li>GP-GAN: Towards Realistic High-Resolution Image Blending (<a href="https://arxiv.org/abs/1703.07195" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/wuhuikai/GP-GAN" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>BEGAN: Boundary Equilibrium Generative Adversarial Networks (<a href="https://www.arxiv.org/abs/1703.10717" target="_blank" rel="noopener">PDF</a>)</li>
<li>MAGAN: Margin Adaptation for Generative Adversarial Networks (<a href="https://arxiv.org/abs/1704.03817" target="_blank" rel="noopener">PDF</a>)</li>
<li>Pose Guided Person Image Generation (<a href="https://arxiv.org/abs/1705.09368" target="_blank" rel="noopener">PDF</a>)</li>
<li>On the Effects of Batch and Weight Normalization in Generative Adversarial Networks (<a href="https://arxiv.org/abs/1704.03971" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/stormraiser/GAN-weight-norm" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Aesthetic-Driven Image Enhancement by Adversarial Learning (<a href="https://arxiv.org/abs/1707.05251" target="_blank" rel="noopener">PDF</a>)</li>
<li>VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning (<a href="https://arxiv.org/abs/1705.07761" target="_blank" rel="noopener">PDF</a>, <a href="https://akashgit.github.io/VEEGAN/" target="_blank" rel="noopener">Project/Code</a></li>
<li>MoCoGAN: Decomposing Motion and Content for Video Generation (<a href="https://arxiv.org/abs/1707.04993" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/sergeytulyakov/mocogan" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Generative Adversarial Networks: An Overview ((PDF)[<a href="https://arxiv.org/abs/1710.07035" target="_blank" rel="noopener">https://arxiv.org/abs/1710.07035</a>])</li>
<li>SalGAN: Visual Saliency Prediction with Generative Adversarial Networks (<a href="https://arxiv.org/abs/1701.01081" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/imatge-upc/saliency-salgan-2017" target="_blank" rel="noopener">Project/Code</a>)</li>
</ul>
<h3 id="MACHINE-LEARNING"><a href="#MACHINE-LEARNING" class="headerlink" title="MACHINE LEARNING"></a>MACHINE LEARNING</h3><ul>
<li>Metric Learning with Adaptive Density Discrimination (<a href="https://arxiv.org/abs/1511.05939" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/vithursant/MagnetLoss-PyTorch/" target="_blank" rel="noopener">PyTorch</a>, <a href="https://github.com/pumpikano/tf-magnet-loss" target="_blank" rel="noopener">TF</a>)</li>
<li>Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent (<a href="https://arxiv.org/abs/1711.10456" target="_blank" rel="noopener">PDF</a>)</li>
<li><a href="https://joshua19881228.github.io/2016-08-03-CV-ML/" target="_blank" rel="noopener">计算机视觉与机器学习 【随机森林】</a></li>
<li><a href="https://joshua19881228.github.io/2016-08-01-CV-ML/" target="_blank" rel="noopener">计算机视觉与机器学习 【深度学习中的激活函数】</a></li>
<li><a href="https://www.52ml.net/" target="_blank" rel="noopener">我爱机器学习</a> 机器学习干货站</li>
<li><a href="http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.HomePage" target="_blank" rel="noopener">Bayesian Reasoning and Machine Learning</a></li>
<li>Stochastic Gradient Descent as Approximate Bayesian Inference (<a href="https://arxiv.org/abs/1704.04289" target="_blank" rel="noopener">PDF</a>)</li>
</ul>
<h3 id="LIGHT-WEIGHT-MODEL-EMBEDDED-MOBILE-MODEL-COMPRESSION"><a href="#LIGHT-WEIGHT-MODEL-EMBEDDED-MOBILE-MODEL-COMPRESSION" class="headerlink" title="LIGHT-WEIGHT MODEL/EMBEDDED/MOBILE/MODEL COMPRESSION"></a>LIGHT-WEIGHT MODEL/EMBEDDED/MOBILE/MODEL COMPRESSION</h3><ul>
<li>MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning (<a href="https://arxiv.org/abs/1903.10258" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/liuzechun/MetaPruning" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>PyTorch Network Slimming (<a href="https://arxiv.org/abs/1708.06519" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/yeyun11/pytorch-network-slimming" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Importance Estimation for Neural Network Pruning (<a href="https://arxiv.org/abs/1906.10771" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/NVlabs/Taylor_pruning" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning (<a href="https://arxiv.org/abs/1903.10258" target="_blank" rel="noopener">PDF</a>)</li>
<li>EFFICIENT METHODS AND HARDWARE FOR DEEP LEARNING (<a href="https://stacks.stanford.edu/file/druid:qf934gh3708/EFFICIENT%20METHODS%20AND%20HARDWARE%20FOR%20DEEP%20LEARNING-augmented.pdf" target="_blank" rel="noopener">PDF</a>)</li>
<li>ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware (<a href="https://arxiv.org/abs/1812.00332" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/MIT-HAN-LAB/ProxylessNAS" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>FD-MobileNet: Improved MobileNet with a Fast Downsampling Strategy (<a href="https://arxiv.org/abs/1802.03750" target="_blank" rel="noopener">PDF</a>)</li>
<li>Quantization Mimic: Towards Very Tiny CNN for Object Detection (<a href="https://arxiv.org/abs/1805.02152" target="_blank" rel="noopener">PDF</a>)</li>
<li>Pelee: A Real-Time Object Detection System on Mobile Devices (<a href="https://arxiv.org/abs/1804.06882" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/Robert-JunWang/Pelee" target="_blank" rel="noopener">Project/Code</a>, <a href="https://github.com/ginn24/Pelee-TensorRT" target="_blank" rel="noopener">TensorRT Implemented</a>, <a href="https://joshua19881228.github.io/2018-04-22-Pelee/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>MobileNetV2: Inverted Residuals and Linear Bottlenecks (<a href="https://arxiv.org/abs/1801.04381" target="_blank" rel="noopener">PDF</a>, <a href="https://joshua19881228.github.io/2018-03-07-MobileNetsV2/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>SBNet: Sparse Blocks Network for Fast Inference (<a href="https://arxiv.org/abs/1801.02108" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/uber/sbnet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>IGCV2: Interleaved Structured Sparse Convolutional Neural Networks (<a href="https://arxiv.org/abs/1804.06202" target="_blank" rel="noopener">PDF</a>)</li>
<li>FitNets: Hints for Thin Deep Nets (<a href="https://arxiv.org/abs/1412.6550" target="_blank" rel="noopener">PDF</a>)</li>
<li>Building Efficient ConvNets using Redundant Feature Pruning (<a href="https://arxiv.org/abs/1802.07653" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/babajide07/Redundant-Feature-Pruning-Pytorch-Implementation" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Multi-Scale Dense Networks for Resource Efficient Image Classification (<a href="https://arxiv.org/abs/1703.09844" target="_blank" rel="noopener">PDF</a>)</li>
<li>Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee (<a href="https://arxiv.org/abs/1611.05162" target="_blank" rel="noopener">pdf</a>)</li>
<li>NISP: Pruning Networks using Neuron Importance Score Propagation (<a href="https://arxiv.org/abs/1711.05908" target="_blank" rel="noopener">PDF</a>)</li>
<li>Caffeinated FPGAs: FPGA Framework For Convolutional Neural Networks (<a href="https://arxiv.org/abs/1609.09671" target="_blank" rel="noopener">PDF</a>)</li>
<li>Comprehensive Evaluation of OpenCL-based Convolutional Neural Network Accelerators in Xilinx and Altera FPGAs (<a href="https://arxiv.org/abs/1609.09296" target="_blank" rel="noopener">PDF</a>)</li>
<li>FINN: A Framework for Fast, Scalable Binarized Neural Network Inference (<a href="http://www.idi.ntnu.no/~yamanu/2017-fpga-finn-preprint.pdf" target="_blank" rel="noopener">PDF</a>)</li>
<li>Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices (<a href="https://arxiv.org/abs/1701.00485" target="_blank" rel="noopener">PDF</a>)</li>
<li>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size (<a href="https://arxiv.org/abs/1602.07360" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/DeepScale/SqueezeNet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (<a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/farmingyard/caffe-mobilenet" target="_blank" rel="noopener">Caffe Implementation</a>, <a href="https://joshua19881228.github.io/2017-07-19-MobileNet/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Binarized Convolutional Neural Networks with Separable Filters for Efficient Hardware Acceleration (<a href="https://arxiv.org/abs/1707.04693" target="_blank" rel="noopener">PDF</a>)</li>
<li>Channel Pruning for Accelerating Very Deep Neural Networks (<a href="https://arxiv.org/abs/1707.06168" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/yihui-he/channel-pruning" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Quantized Convolutional Neural Networks for Mobile Devices (<a href="https://arxiv.org/abs/1512.06473" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/jiaxiang-wu/quantized-cnn" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Squeeze-and-Excitation Networks (<a href="https://arxiv.org/abs/1709.01507" target="_blank" rel="noopener">PDF</a>)</li>
<li>Domain-adaptive deep network compression (<a href="https://arxiv.org/abs/1709.01041" target="_blank" rel="noopener">PDF</a>)</li>
<li>Embedded Binarized Neural Networks (<a href="https://arxiv.org/abs/1709.02260" target="_blank" rel="noopener">PDF</a>)</li>
<li>Keynote: Small Neural Nets Are Beautiful: Enabling Embedded Systems with Small Deep-Neural-Network Architectures (<a href="https://arxiv.org/abs/1710.02759" target="_blank" rel="noopener">PDF</a>)</li>
<li>A Survey of Model Compression and Acceleration for Deep Neural Networks ([<a href="https://arxiv.org/abs/1710.09282" target="_blank" rel="noopener">https://arxiv.org/abs/1710.09282</a>])</li>
</ul>
<h3 id="ReID"><a href="#ReID" class="headerlink" title="ReID"></a>ReID</h3><ul>
<li>Video-based Person Re-identification via 3D Convolutional Networks and Non-local Attention (<a href="https://arxiv.org/abs/1807.05073" target="_blank" rel="noopener">PDF</a>)</li>
<li>Attention-Aware Compositional Network for Person Re-identification (<a href="https://arxiv.org/abs/1805.03344" target="_blank" rel="noopener">PDF</a>)</li>
<li>Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification (<a href="https://arxiv.org/abs/1711.07027" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/Simon4Yan/Learning-via-Translation" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Features for Multi-Target Multi-Camera Tracking and Re-Identification (<a href="https://arxiv.org/abs/1803.10859" target="_blank" rel="noopener">PDF</a>)</li>
<li>Video Person Re-identification by Temporal Residual Learning (<a href="https://arxiv.org/abs/1802.07918" target="_blank" rel="noopener">PDF</a>)</li>
<li>Harmonious Attention Network for Person Re-Identification (<a href="https://arxiv.org/abs/1802.08122" target="_blank" rel="noopener">PDF</a>)</li>
<li>In Defense of the Triplet Loss for Person Re-Identification (<a href="https://arxiv.org/abs/1703.07737" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Spatial Feature Reconstruction for Partial Person Re-identification: Alignment-Free Approach (<a href="https://arxiv.org/abs/1801.00881" target="_blank" rel="noopener">PDF</a>)</li>
<li>AlignedReID: Surpassing Human-Level Performance in Person Re-Identification (<a href="https://arxiv.org/abs/1711.08184" target="_blank" rel="noopener">PDF</a>)</li>
<li>A Discriminatively Learned CNN Embedding for Person Re-identification (<a href="https://arxiv.org/abs/1611.05666" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/layumi/2016_person_re-ID" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Learning Deep Neural Networks for Vehicle Re-ID with Visual-spatio-temporal Path Proposals (<a href="https://arxiv.org/abs/1708.03918" target="_blank" rel="noopener">PDF</a>)</li>
<li>Beyond triplet loss: a deep quadruplet network for person re-identification (<a href="https://arxiv.org/abs/1704.01719" target="_blank" rel="noopener">PDF</a>)</li>
<li>Person Re-identification by Local Maximal Occurrence Representation and Metric Learning (<a href="http://www.cbsr.ia.ac.cn/users/scliao/papers/Liao-CVPR15-LOMO-XQDA.pdf" target="_blank" rel="noopener">PDF</a>, <a href="http://www.cbsr.ia.ac.cn/users/scliao/projects/lomo_xqda/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Person Re-identification: Past, Present and Future (<a href="https://arxiv.org/abs/1610.02984" target="_blank" rel="noopener">PDF</a>)</li>
<li>Unsupervised Person Re-identification: Clustering and Fine-tuning (<a href="https://arxiv.org/abs/1705.10444" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-Identification (<a href="https://arxiv.org/abs/1708.02286" target="_blank" rel="noopener">PDF</a>)</li>
<li>Divide and Fuse: A Re-ranking Approach for Person Re-identification (<a href="https://arxiv.org/abs/1708.04169" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification (<a href="https://arxiv.org/abs/1710.06555" target="_blank" rel="noopener">PDF</a>)</li>
<li>HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis (<a href="https://arxiv.org/abs/1709.09930" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/xh-liu/HydraPlus-Net" target="_blank" rel="noopener">Project/Code</a>)</li>
</ul>
<h3 id="FASHION"><a href="#FASHION" class="headerlink" title="FASHION"></a>FASHION</h3><ul>
<li>Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba (<a href="https://arxiv.org/abs/1803.02349" target="_blank" rel="noopener">PDF</a>)</li>
<li>Visually-Aware Fashion Recommendation and Design with Generative Image Models (<a href="Visually-Aware Fashion Recommendation and Design with Generative Image Models">PDF</a>)</li>
<li>Be Your Own Prada: Fashion Synthesis with Structural Coherence (<a href="https://arxiv.org/abs/1710.07346" target="_blank" rel="noopener">PDF</a>, <a href="http://mmlab.ie.cuhk.edu.hk/projects/FashionGAN/" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2017-10-31-DeepFashion/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Style2Vec: Representation Learning for Fashion Items from Style Sets (<a href="https://arxiv.org/abs/1708.04014" target="_blank" rel="noopener">PDF</a>)</li>
<li>Dress like a Star: Retrieving Fashion Products from Videos (<a href="https://arxiv.org/abs/1710.07198" target="_blank" rel="noopener">PDF</a>)</li>
<li>The Conditional Analogy GAN: Swapping Fashion Articles on People Images (<a href="https://arxiv.org/abs/1709.04695" target="_blank" rel="noopener">PDF</a>)</li>
</ul>
<h3 id="OTHER"><a href="#OTHER" class="headerlink" title="OTHER"></a>OTHER</h3><ul>
<li>GaitSet: Regarding Gait as a Set for Cross-View Gait Recognition (<a href="https://arxiv.org/abs/1811.06186" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/AbnerHqC/GaitSet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Deep Clustering for Unsupervised Learning of Visual Features (<a href="https://arxiv.org/abs/1807.05520" target="_blank" rel="noopener">PDF</a>)</li>
<li>Detecting Visual Relationships Using Box Attention (<a href="https://arxiv.org/abs/1807.02136" target="_blank" rel="noopener">PDF</a>)</li>
<li>Zoom-Net: Mining Deep Feature Interactions for Visual Relationship Recognition (<a href="https://arxiv.org/abs/1807.04979" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/gjyin91/ZoomNet" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Learning to See in the Dark(<a href="https://arxiv.org/abs/1805.01934" target="_blank" rel="noopener">PDF</a>)</li>
<li>A Variational U-Net for Conditional Appearance and Shape Generation (<a href="https://arxiv.org/abs/1804.04694" target="_blank" rel="noopener">PDF</a>, <a href="https://compvis.github.io/vunet/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Synthesizing Images of Humans in Unseen Poses (<a href="https://arxiv.org/abs/1804.07739v1" target="_blank" rel="noopener">PDF</a>)</li>
<li>End-to-end weakly-supervised semantic alignment (<a href="https://arxiv.org/abs/1712.06861" target="_blank" rel="noopener">PDF</a>, <a href="http://www.di.ens.fr/willow/research/weakalign/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Dense Optical Flow based Change Detection Network Robust to Difference of Camera Viewpoints (<a href="https://arxiv.org/abs/1712.02941" target="_blank" rel="noopener">PDF</a>)</li>
<li>Dual-Path Convolutional Image-Text Embedding (<a href="https://arxiv.org/abs/1711.05535" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com//layumi/Image-Text-Embedding" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>The Promise and Peril of Human Evaluation for Model Interpretability (<a href="https://arxiv.org/abs/1711.07414" target="_blank" rel="noopener">PDF</a>)</li>
<li>Semantic Image Retrieval via Active Grounding of Visual Situations (<a href="https://arxiv.org/abs/1711.00088" target="_blank" rel="noopener">PDF</a>)</li>
<li>LIFT: Learned Invariant Feature Transform (<a href="https://arxiv.org/abs/1603.09114" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning Aligned Cross-Modal Representations from Weakly Aligned Data (<a href="http://cmplaces.csail.mit.edu/content/paper.pdf" target="_blank" rel="noopener">PDF</a>, <a href="http://cmplaces.csail.mit.edu/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Multi-Task Curriculum Transfer Deep Learning of Clothing Attributes (<a href="https://arxiv.org/abs/1610.03670" target="_blank" rel="noopener">PDF</a>)</li>
<li>End-to-end Learning of Deep Visual Representations for Image Retrieval (<a href="https://arxiv.org/abs/1610.07940" target="_blank" rel="noopener">PDF</a>)</li>
<li>SoundNet: Learning Sound Representations from Unlabeled Video (<a href="http://web.mit.edu/vondrick/soundnet.pdf" target="_blank" rel="noopener">PDF</a>)</li>
<li>Bags of Local Convolutional Features for Scalable Instance Search (<a href="https://arxiv.org/abs/1604.04653" target="_blank" rel="noopener">PDF</a>, <a href="https://imatge-upc.github.io/retrieval-2016-icmr/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Universal Correspondence Network (<a href="http://cvgl.stanford.edu/projects/ucn/choy_nips16.pdf" target="_blank" rel="noopener">PDF</a>, <a href="http://cvgl.stanford.edu/projects/ucn/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Judging a Book By its Cover (<a href="https://arxiv.org/abs/1610.09204" target="_blank" rel="noopener">PDF</a>)</li>
<li>Generalisation and Sharing in Triplet Convnets for Sketch based Visual Search (<a href="https://arxiv.org/abs/1611.05301" target="_blank" rel="noopener">PDF</a>)</li>
<li>Analysis and Optimization of Loss Functions for Multiclass, Top-k, and Multilabel Classification (<a href="https://arxiv.org/abs/1612.03663" target="_blank" rel="noopener">PDF</a>)</li>
<li>Automatic generation of large-scale handwriting fonts via style learning (<a href="http://delivery.acm.org/10.1145/3010000/3005371/a12-lian.pdf?ip=101.36.73.155&amp;id=3005371&amp;acc=OPEN&amp;key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;CFID=878397374&amp;CFTOKEN=60453893&amp;__acm__=1482192254_e947f14035db372a97d831530a7c05b1" target="_blank" rel="noopener">PDF</a>)</li>
<li>Image Retrieval with Deep Local Features and Attention-based Keypoints (<a href="https://arxiv.org/abs/1612.06321" target="_blank" rel="noopener">PDF</a>)</li>
<li>Visual Discovery at Pinterest (<a href="https://arxiv.org/abs/1702.04680" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning to Detect Human-Object Interactions (<a href="https://arxiv.org/abs/1702.05448" target="_blank" rel="noopener">PDF</a>, <a href="http://www-personal.umich.edu/~ywchao/hico/" target="_blank" rel="noopener">Project/Code</a>, <a href="https://joshua19881228.github.io/2017-02-28-HORCNN/" target="_blank" rel="noopener">Reading Note</a>)</li>
<li>Learning Deep Features via Congenerous Cosine Loss for Person Recognition (<a href="https://arxiv.org/abs/1702.06890" target="_blank" rel="noopener">PDF</a>)</li>
<li>Large-Scale Evolution of Image Classifiers (<a href="https://arxiv.org/abs/1703.01041" target="_blank" rel="noopener">PDF</a>)</li>
<li>Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection (<a href="https://arxiv.org/abs/1703.03054" target="_blank" rel="noopener">PDF</a>)</li>
<li>Twitter100k: A Real-world Dataset for Weakly Supervised Cross-Media Retrieval (<a href="https://arxiv.org/abs/1703.06618" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/huyt16/Twitter100k/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Mixture of Counting CNNs: Adaptive Integration of CNNs Specialized to Specific Appearance for Crowd Counting (<a href="https://arxiv.org/abs/1703.09393" target="_blank" rel="noopener">PDF</a>)</li>
<li>Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art (<a href="https://arxiv.org/abs/1704.05519" target="_blank" rel="noopener">PDF</a>, <a href="http://www.cvlibs.net/projects/autonomous_vision_survey/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Learning Features by Watching Objects Move (<a href="https://arxiv.org/abs/1612.06370" target="_blank" rel="noopener">PDF</a>, <a href="https://people.eecs.berkeley.edu/~pathak/unsupervised_video/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>GMS: Grid-based Motion Statistics for Fast, Ultra-robust Feature Correspondence (<a href="http://jwbian.net/Papers/GMS_CVPR17.pdf" target="_blank" rel="noopener">PDF</a>, <a href="http://jwbian.net/gms" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>ResnetCrowd: A Residual Deep Learning Architecture for Crowd Counting, Violent Behaviour Detection and Crowd Density Level Classification (<a href="https://arxiv.org/abs/1705.10698" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning Cross-modal Embeddings for Cooking Recipes and Food Images (<a href="http://im2recipe.csail.mit.edu/im2recipe.pdf" target="_blank" rel="noopener">PDF</a>, <a href="http://im2recipe.csail.mit.edu/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Convolutional neural network architecture for geometric matching (<a href="https://arxiv.org/abs/1703.05593" target="_blank" rel="noopener">PDF</a>, <a href="http://www.di.ens.fr/willow/research/cnngeometric/" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>Semantic Compositional Networks for Visual Captioning (<a href="https://arxiv.org/abs/1611.08002" target="_blank" rel="noopener">PDF</a>, <a href="https://github.com/zhegan27/SCN_for_video_captioning" target="_blank" rel="noopener">Project/Code</a>)</li>
<li>CNN-based Cascaded Multi-task Learning of High-level Prior and Density Estimation for Crowd Counting (<a href="https://arxiv.org/abs/1707.09605" target="_blank" rel="noopener">PDF</a>)</li>
<li>Understanding Black-box Predictions via Influence Functions (<a href="https://arxiv.org/abs/1703.04730" target="_blank" rel="noopener">PDF</a>)</li>
<li>Learning a Repression Network for Precise Vehicle Search (<a href="https://arxiv.org/abs/1708.02386" target="_blank" rel="noopener">PDF</a>)</li>
<li>Visual Graph Mining (<a href="https://arxiv.org/abs/1708.03921" target="_blank" rel="noopener">PDF</a>)</li>
<li>A Deep Multimodal Approach for Cold-start Music Recommendation (<a href="https://arxiv.org/abs/1706.09739" target="_blank" rel="noopener">PDF</a>)</li>
<li>A Multilayer-Based Framework for Online Background Subtraction with Freely Moving Cameras (<a href="https://arxiv.org/abs/1709.01140" target="_blank" rel="noopener">PDF</a>)</li>
<li>A self-organizing neural network architecture for learning human-object interactions (<a href="https://arxiv.org/abs/1710.01916" target="_blank" rel="noopener">PDF</a>)</li>
</ul>
<h2 id="INTERESTING-FINDS"><a href="#INTERESTING-FINDS" class="headerlink" title="INTERESTING FINDS"></a>INTERESTING FINDS</h2><h3 id="RESOURCES-PERSPECTIVES"><a href="#RESOURCES-PERSPECTIVES" class="headerlink" title="RESOURCES/PERSPECTIVES"></a>RESOURCES/PERSPECTIVES</h3><ul>
<li><a href="https://github.com/JeremyBYU/polylidar" target="_blank" rel="noopener">Polylidar</a></li>
<li><a href="https://github.com/mrgloom/Network-Speed-and-Compression" target="_blank" rel="noopener">Network-acceleration</a></li>
<li><a href="https://github.com/niais/Awesome-Skeleton-based-Action-Recognition" target="_blank" rel="noopener">Awesome-Skeleton-based-Action-Recognition</a></li>
<li><a href="https://github.com/crespum/edge-ai" target="_blank" rel="noopener">AI at the edge</a> A curated list of hardware, software, frameworks and other resources for Artificial Intelligence at the edge. Inspired by awesome-dataviz.</li>
<li><a href="https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB" target="_blank" rel="noopener">Ultra-Light-Fast-Generic-Face-Detector-1MB</a> 超轻量级通用人脸检测模型</li>
<li><a href="https://github.com/Zhongdao/Towards-Realtime-MOT" target="_blank" rel="noopener">Towards-Realtime-MOT</a> Joint Detection and Embedding for fast multi-object tracking</li>
<li><a href="https://github.com/songwsx/RFSong-7993" target="_blank" rel="noopener">RFSong-7993</a> 设计的轻量级 RFB 进行行人检测，AP 达到 0.7993，参数量仅有 3.1MB，200 FPS</li>
<li><a href="https://github.com/nerox8664/awesome-computer-vision-models" target="_blank" rel="noopener">Awesome Computer Vision Models</a> This is the list with popular classification and segmentation models related with corresponding evaluation metrics.</li>
<li><a href="https://github.com/wpf535236337/real-time-network" target="_blank" rel="noopener">Real-time network for mobile devices</a> real-time network architecture for mobile devices and semantic segmentation</li>
<li><a href="https://taocpp.github.io/" target="_blank" rel="noopener">The Art of C++</a> A collection of high-quality C++ libraries</li>
<li><a href="https://github.com/dennybritz/reinforcement-learning" target="_blank" rel="noopener">Reinforcement Learning Algorithms</a> Implementation of Reinforcement Learning Algorithms. Python, OpenAI Gym, Tensorflow. Exercises and Solutions to accompany Sutton’s Book and David Silver’s course.</li>
<li><a href="https://www.oreilly.com/library/view/generative-deep-learning/9781492041931/" target="_blank" rel="noopener">Generative Deep Learning</a> Generative modeling is one of the hottest topics in AI. It’s now possible to teach a machine to excel at human endeavors such as painting, writing, and composing music. With this practical book, machine-learning engineers and data scientists will discover how to re-create some of the most impressive examples of generative deep learning models, such as variational autoencoders,generative adversarial networks (GANs), encoder-decoder models and world models.</li>
<li><a href="https://github.com/ZhaoJ9014/High-Performance-Face-Recognition" target="_blank" rel="noopener">High Performance Face Recognition</a> This repository provides several high performance models for unconstrained / large-scale / low-shot face recognition.</li>
<li><a href="https://github.com/xuehaouwa/Awesome-Trajectory-Prediction" target="_blank" rel="noopener">Awesome-Trajectory-Prediction</a> This is a list of useful information about trajectory prediction. Related papers, datasets and codes are included.</li>
<li><a href="https://github.com/wangxiao5791509/Pedestrian-Attribute-Recognition-Paper-List" target="_blank" rel="noopener">Pedestrian Attribute Recognition Paper List</a></li>
<li><a href="https://github.com/HarisIqbal88/PlotNeuralNet" target="_blank" rel="noopener">PlotNeuralNet</a> Latex code for making neural networks diagrams</li>
<li><a href="https://github.com/danistefanovic/build-your-own-x" target="_blank" rel="noopener">Build your own x</a></li>
<li><a href="https://github.com/nmhkahn/pytorch-exercise" target="_blank" rel="noopener">PyTorch Exercise Codes for Deep Learning Researchers</a></li>
<li><a href="https://www.dataquest.io/blog/regex-cheatsheet/" target="_blank" rel="noopener">Python Regular Expressions Cheat Sheet</a></li>
<li><a href="https://github.com/eriklindernoren/PyTorch-GAN" target="_blank" rel="noopener">PyTorch-GAN</a><br>PyTorch implementations of Generative Adversarial Networks.</li>
<li><a href="https://github.com/miguelgfierro/sciblog_support/blob/master/A_Gentle_Introduction_to_Transfer_Learning/Intro_Transfer_Learning.ipynb" target="_blank" rel="noopener">A Gentle Introduction to Transfer Learning for Image Classification</a></li>
<li><a href="https://github.com//dongb5/GAN-Timeline" target="_blank" rel="noopener">GAN Timeline</a><br>A timeline showing the development of Generative Adversarial Networks (GAN).</li>
<li><a href="http://arxiv.org/list/cs.CV/recent" target="_blank" rel="noopener">arXiv(Computer Vision and Pattern Recognition)</a><br>A good place to explore latest papers.</li>
<li><a href="https://github.com/jsbroks/awesome-dataset-tools" target="_blank" rel="noopener">Awesome Dataset Tools</a><br>A curated list of awesome dataset tools</li>
<li><a href="https://github.com/jbhuang0604/awesome-computer-vision" target="_blank" rel="noopener">Awesome Computer Vision</a><br>A curated list of awesome computer vision resources.</li>
<li><a href="https://github.com/kjw0612/awesome-deep-vision" target="_blank" rel="noopener">Awesome Deep Vision</a><br>A curated list of deep learning resources for computer vision.</li>
<li><a href="https://github.com/dmlc/mxnet/blob/master/example/README.md" target="_blank" rel="noopener">Awesome MXNet</a><br>This page contains a curated list of awesome MXnet examples, tutorials and blogs.</li>
<li><a href="https://github.com/jtoy/awesome-tensorflow" target="_blank" rel="noopener">Awesome TensorFlow</a><br>A curated list of awesome TensorFlow experiments, libraries, and projects.</li>
<li><a href="https://github.com/nashory/gans-awesome-applications" target="_blank" rel="noopener">gans-awesome-applications</a><br>Curated list of awesome GAN applications and demonstrations.</li>
<li><a href="https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey" target="_blank" rel="noopener">Deep Reinforcement Learning survey</a><br>This paper list is a bit different from others. The author puts some opinion and summary on it. However, to understand the whole paper, you still have to read it by yourself!</li>
<li><a href="https://github.com/jikexueyuanwiki/tensorflow-zh" target="_blank" rel="noopener">TensorFlow 官方文档中文版</a></li>
<li><a href="https://tensortalk.com/" target="_blank" rel="noopener">TensorTalk</a><br>A place to find latest work’s codes.</li>
<li><a href="https://github.com/foolwood/benchmark_results" target="_blank" rel="noopener">OTB Results</a><br>Object tracking benchmark</li>
<li><a href="https://github.com/zhangqianhui/AdversarialNetsPapers" target="_blank" rel="noopener">Adversarial Nets Papers</a></li>
<li><a href="https://www.youtube.com/watch?v=ZHYXp3gJCaI" target="_blank" rel="noopener">Creating Human-Level AI</a></li>
<li><a href="http://cv-tricks.com/" target="_blank" rel="noopener">cv-tricks.com</a></li>
<li><a href="http://mlmodelzoo.com/" target="_blank" rel="noopener">Find deep learning models for your mobile platform</a></li>
<li><a href="http://openaccess.thecvf.com/ICCV2017.py" target="_blank" rel="noopener">ICCV 2017 Open Access Repository</a></li>
</ul>
<h3 id="PROJECTS"><a href="#PROJECTS" class="headerlink" title="PROJECTS"></a>PROJECTS</h3><ul>
<li><a href="https://github.com/AberHu/Knowledge-Distillation-Zoo" target="_blank" rel="noopener">Knowledge-Distillation-Zoo</a> Pytorch implementation of various Knowledge Distillation methods.</li>
<li><a href="https://github.com/njvisionpower/mxnet-insightface-cpp" target="_blank" rel="noopener">mxnet-insightface-cpp</a> This project implement an easy deployable face recognition pipeline with mxnet c++ framework.</li>
<li><a href="https://github.com/1996scarlet/ArcFace-Multiplex-Recognition" target="_blank" rel="noopener">Real-Time ArcFace Multiplex Recognition</a> Face Detection and Recognition using RetinaFace and ArcFace, can reach nearly 24 fps at GTX1660ti.</li>
<li><a href="https://github.com/marcomusy/vtkplotter" target="_blank" rel="noopener">vtkplotter</a> A python module for scientific visualization, analysis and animation of 3D objects and point clouds based on VTK and numpy.</li>
<li><a href="https://heremaps.github.io/pptk/" target="_blank" rel="noopener">pptk</a> The Point Processing Toolkit (pptk) is a Python package for visualizing and processing 2-d/3-d point clouds.</li>
<li><a href="https://github.com/Raveler/ffmpeg-cpp" target="_blank" rel="noopener">ffmpeg-cpp</a> A clean C++ wrapper around the ffmpeg libraries.</li>
<li><a href="https://github.com/cbsudux/Human-Pose-Estimation-101" target="_blank" rel="noopener">Human Pose Estimation 101</a> Basics of 2D and 3D Human Pose Estimation.</li>
<li><a href="https://github.com/facebookresearch/mobile-vision" target="_blank" rel="noopener">Mobile Computer Vision @ Facebook</a></li>
<li><a href="https://github.com/HHTseng/video-classification" target="_blank" rel="noopener">Video Classification</a> builds a quick and simple code for video classification (or action recognition) using UCF101 with PyTorch.</li>
<li><a href="https://github.com/Charrin/RetinaFace-Cpp" target="_blank" rel="noopener">RetinaFace-Cpp</a> RetinaFace detector with C++</li>
<li><a href="https://tensorstream.argus-ai.com/" target="_blank" rel="noopener">TensorStream</a> is a C++ library for real-time video stream (e.g. RTMP) decoding to CUDA memory</li>
<li><a href="https://github.com/fabioperez/pytorch-examples/blob/master/notebooks/PyTorch_Data_Augmentation_Image_Classification.ipynb" target="_blank" rel="noopener">Data Augmentation for Computer Vision with PyTorch</a></li>
<li><a href="https://github.com/NervanaSystems/distiller" target="_blank" rel="noopener">Neural Network Distiller</a><br>Distiller is an open-source Python package for neural network compression research.</li>
<li><a href="https://github.com//hahnyuan/nn_tools" target="_blank" rel="noopener">Neural Network Tools: Converter, Constructor and Analyser</a><br>For caffe, pytorch, tensorflow, draknet and so on.</li>
<li><a href="https://github.com/aymericdamien/TensorFlow-Examples" target="_blank" rel="noopener">TensorFlow Examples</a><br>TensorFlow Tutorial with popular machine learning algorithms implementation. This tutorial was designed for easily diving into TensorFlow, through examples.It is suitable for beginners who want to find clear and concise examples about TensorFlow. For readability, the tutorial includes both notebook and code with explanations.</li>
<li><a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials" target="_blank" rel="noopener">TensorFlow Tutorials</a><br>These tutorials are intended for beginners in Deep Learning and TensorFlow. Each tutorial covers a single topic. The source-code is well-documented. There is a YouTube video for each tutorial.</li>
<li><a href="https://github.com/BrandonJoffe/home_surveillance" target="_blank" rel="noopener">Home Surveilance with Facial Recognition</a></li>
<li><a href="https://github.com/blackecho/Deep-Learning-TensorFlow" target="_blank" rel="noopener">Deep Learning algorithms with TensorFlow</a><br>This repository is a collection of various Deep Learning algorithms implemented using the TensorFlow library. This package is intended as a command line utility you can use to quickly train and evaluate popular Deep Learning models and maybe use them as benchmark/baseline in comparison to your custom models/datasets.</li>
<li><a href="https://github.com/zsdonghao/tensorlayer" target="_blank" rel="noopener">TensorLayer</a><br>TensorLayer is designed to use by both Researchers and Engineers, it is a transparent library built on the top of Google TensorFlow. It is designed to provide a higher-level API to TensorFlow in order to speed-up experimentations and developments. TensorLayer is easy to be extended and modified. In addition, we provide many examples and tutorials to help you to go through deep learning and reinforcement learning.</li>
<li><a href="http://blog.dlib.net/2016/10/easily-create-high-quality-object.html" target="_blank" rel="noopener">Easily Create High Quality Object Detectors with Deep Learning</a><br>Using <a href="http://dlib.net/" target="_blank" rel="noopener">dlib</a> to train a CNN to detect.</li>
<li><a href="https://github.com/hugorut/neural-cli" target="_blank" rel="noopener">Command Line Neural Network</a><br>Neuralcli provides a simple command line interface to a python implementation of a simple classification neural network. Neuralcli allows a quick way and easy to get instant feedback on a hypothesis or to play around with one of the most popular concepts in machine learning today.</li>
<li><a href="https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition/" target="_blank" rel="noopener">LSTM for Human Activity Recognition</a><br>Human activity recognition using smartphones dataset and an LSTM RNN. The project is based on Tesorflow. A MXNet implementation is <a href="https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/HumanActivityRecognition" target="_blank" rel="noopener">MXNET-Scala Human Activity Recognition</a>.</li>
<li><a href="https://github.com/xingwangsfu/caffe-yolo" target="_blank" rel="noopener">YOLO in caffe</a><br>This is a caffe implementation of the YOLO:Real-Time Object Detection.</li>
<li><a href="https://github.com/zhreshold/mxnet-ssd" target="_blank" rel="noopener">SSD: Single Shot MultiBox Object Detector in mxnet</a></li>
<li><a href="https://github.com/pangyupo/mxnet_mtcnn_face_detection" target="_blank" rel="noopener">MTCNN face detection and alignment in MXNet</a><br>This is a python/mxnet implementation of <a href="https://github.com/kpzhang93/MTCNN_face_detection_alignment" target="_blank" rel="noopener">Zhang’s</a> work .</li>
<li><a href="https://github.com/Microsoft/CNTK/tree/master/Examples/Image/Detection/FastRCNN" target="_blank" rel="noopener">CNTK Examples: Image/Detection/Fast R-CNN</a></li>
<li><a href="https://github.com/RyanZotti/Self-Driving-Car" target="_blank" rel="noopener">Self Driving (Toy) Ferrari</a></li>
<li><a href="https://github.com/udacity/CarND-LaneLines-P1" target="_blank" rel="noopener">Finding Lane Lines on the Road</a></li>
<li><a href="https://github.com/tensorflow/magenta" target="_blank" rel="noopener">Magenta</a><br>Magenta is a project from the Google Brain team that asks: Can we use machine learning to create compelling art and music? If so, how? If not, why not?</li>
<li><a href="https://github.com/zhangqianhui/AdversarialNetsPapers" target="_blank" rel="noopener">Adversarial Nets Papers</a><br>The classical Papers about adversarial nets</li>
<li><a href="http://mushreco.ml/en/" target="_blank" rel="noopener">Mushreco</a><br>Make a photo of a mushroom and see which species it is. Determine over 200 different species.</li>
<li><a href="https://github.com/alexjc/neural-enhance#3-background--research" target="_blank" rel="noopener">Neural Enhance</a><br>The neural network is hallucinating details based on its training from example images. It’s not reconstructing your photo exactly as it would have been if it was HD. That’s only possible in Hollywood — but using deep learning as “Creative AI” works and it is just as cool!</li>
<li><a href="https://github.com/cvjena/cnn-models" target="_blank" rel="noopener">CNN Models by CVGJ</a><br>This repository contains convolutional neural network (CNN) models trained on ImageNet by Marcel Simon at the Computer Vision Group Jena (CVGJ) using the Caffe framework. Each model is in a separate subfolder and contains everything needed to reproduce the results. This repository focuses currently contains the batch-normalization-variants of AlexNet and VGG19 as well as the training code for Residual Networks (Resnet).</li>
<li><p><a href="http://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">YOLO2</a></p>
<blockquote>
<p>YOLOv2 uses a few tricks to improve training and increase performance. Like Overfeat and SSD we use a fully-convolutional model, but we still train on whole images, not hard negatives. Like Faster R-CNN we adjust priors on bounding boxes instead of predicting the width and height outright. However, we still predict the x and y coordinates directly. The full details are in our paper soon to be released on Arxiv, stay tuned!</p>
</blockquote>
</li>
<li><p><a href="https://github.com/AlfredXiangWu/face_verification_experiment" target="_blank" rel="noopener">Lightened CNN for Deep Face Representation</a><br>The Deep Face Representation Experiment is based on Convolution Neural Network to learn a robust feature for face verification task.</p>
</li>
<li><a href="http://blog.piekniewski.info/2016/12/05/recurrent-dreams-and-filling-in/" target="_blank" rel="noopener">Recurrent dreams and filling in</a></li>
<li><a href="https://github.com/Seanlinx/mtcnn" target="_blank" rel="noopener">MTCNN in MXnet</a></li>
<li><p><a href="https://github.com/openai/openai-gemm" target="_blank" rel="noopener">openai-gemm</a></p>
<p>Open single and half precision gemm implementations. The main speedups over cublas are with small minibatch and in fp16 data formats.</p>
</li>
<li><p><a href="https://github.com/zhaw/neural_style" target="_blank" rel="noopener">Neural Style</a></p>
<p>style transfer with mxnet</p>
</li>
<li><p><a href="https://github.com/Kyubyong/sudoku" target="_blank" rel="noopener">Can Convolutional Neural Networks Crack Sudoku Puzzles?</a></p>
</li>
<li><p><a href="https://github.com/openai/cleverhans" target="_blank" rel="noopener">cleverhans</a></p>
<p>This repository contains the source code for cleverhans , a Python library to benchmark machine learning systems’ vulnerability to adversarial examples.</p>
</li>
<li><p><a href="https://sagivtech.com/2016/11/10/post-2/" target="_blank" rel="noopener">A deep learning traffic light detector using dlib and a few images from Google street view</a></p>
</li>
<li><a href="https://github.com/taizan/PaintsChainer" target="_blank" rel="noopener">Paints Chainer</a></li>
<li><a href="https://github.com/luhaofang/CACU" target="_blank" rel="noopener">Calculate deep convolution neurAl network on Cell Unit</a></li>
<li><a href="https://github.com/AKSHAYUBHAT/DeepVideoAnalytics" target="_blank" rel="noopener">Deep Video Analytics</a><br>Deep Video Analytics provides a platform for indexing and extracting information from videos and images. Deep learning detection and recognition algorithms are used for indexing individual frames / images along with detected objects. The goal of Deep Video analytics is to become a quickly customizable platform for developing visual &amp; video analytics applications, while benefiting from seamless integration with state or the art models released by the vision research community.</li>
<li><a href="https://github.com/AlexeyAB/Yolo_mark" target="_blank" rel="noopener">Yolo_mark</a><br>Windows GUI for marking bounded boxes of objects in images for training Yolo v2</li>
<li><a href="https://github.com/AlexeyAB/darknet" target="_blank" rel="noopener">Yolo-Windows v2 - Windows version of Yolo Convolutional Neural Networks</a></li>
<li><a href="https://github.com/lucasPV/UDLF" target="_blank" rel="noopener">An Unsupervised Distance Learning Framework for Multimedia Retrieva</a></li>
<li><a href="https://github.com/hwalsuklee/awesome-deep-vision-web-demo" target="_blank" rel="noopener">awesome-deep-vision-web-demo</a></li>
<li><a href="https://github.com/luoyetx/mini-caffe" target="_blank" rel="noopener">Mini Caffe</a><br>Minimal runtime core of Caffe, Forward only, GPU support and Memory efficiency.</li>
<li><a href="https://medium.com/merantix/picasso-a-free-open-source-visualizer-for-cnns-d8ed3a35cfc5" target="_blank" rel="noopener">Picasso: A free open-source visualizer for Convolutional Neural Networks</a><br>Picasso is a free open-source (Eclipse Public License) DNN visualization tool that gives you partial occlusion and saliency maps with minimal fuss.</li>
<li><a href="https://uizard.io/research#pix2code" target="_blank" rel="noopener">pix2code: Generating Code from a Graphical User Interface Screenshot</a></li>
<li><a href="https://github.com/AlphaQi/MTCNN-light" target="_blank" rel="noopener">MTCNN-light</a><br>this repository is the implementation of MTCNN with no framework, Just need opencv and openblas. “Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Neural Networks”, implemented with C++，no framework</li>
<li><a href="https://github.com/KeyKy/mobilenet-mxnet" target="_blank" rel="noopener">MobileNet-MXNet</a><br>This is a MXNet implementation of Google’s MobileNets.</li>
<li><a href="http://dawn.cs.stanford.edu/2017/06/22/noscope/" target="_blank" rel="noopener">NoScope: 1000x Faster Deep Learning Queries over Video</a></li>
<li><a href="https://github.com/leonardvandriel/caffe2_cpp_tutorial" target="_blank" rel="noopener">Caffe2 C++ Tutorials and Examples</a></li>
<li><a href="https://github.com/kencoken/imsearch-tools" target="_blank" rel="noopener">Web Image Downloader Tools</a></li>
</ul>
<h3 id="NEWS-BLOGS"><a href="#NEWS-BLOGS" class="headerlink" title="NEWS/BLOGS"></a>NEWS/BLOGS</h3><ul>
<li><a href="https://towardsdatascience.com/the-complete-guide-to-time-series-analysis-and-forecasting-70d476bfe775" target="_blank" rel="noopener">The Complete Guide to Time Series Analysis and Forecasting</a></li>
<li><a href="https://lilianweng.github.io/" target="_blank" rel="noopener">Lil’Log</a> A very informative blog.</li>
<li><a href="https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a" target="_blank" rel="noopener">A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning</a></li>
<li><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" target="_blank" rel="noopener">Attention? Attention!</a></li>
<li><a href="https://venturebeat.com/2018/11/14/intels-neural-compute-stick-2-is-8-times-faster-than-its-predecessor/" target="_blank" rel="noopener">Intel’s Neural Compute Stick 2 is 8 times faster than its predecessor</a></li>
<li><a href="http://machinethink.net/blog/how-fast-is-my-model/" target="_blank" rel="noopener">How fast is my model?</a></li>
<li><a href="https://eli.thegreenplace.net/2018/depthwise-separable-convolutions-for-machine-learning/" target="_blank" rel="noopener">Depthwise separable convolutions for machine learning</a></li>
<li><a href="[PDF](https://distill.pub/2018/building-blocks/">The Building Blocks of Interpretability</a>&gt;)</li>
<li><a href="https://www.jeremyjordan.me/nn-learning-rate/" target="_blank" rel="noopener">Setting the learning rate of your neural network.</a></li>
<li><a href="https://medium.com/south-park-commons/otodscinos-the-root-cause-of-slow-neural-net-training-fec7295c364c" target="_blank" rel="noopener">The Root Cause of Slow Neural Net Training</a></li>
<li><a href="https://severelytheoretical.wordpress.com/2018/01/01/why-is-it-hard-to-train-deep-neural-networks-degeneracy-not-vanishing-gradients-is-the-key/" target="_blank" rel="noopener">Why is it hard to train deep neural networks? Degeneracy, not vanishing gradients, is the key</a></li>
<li><a href="http://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/" target="_blank" rel="noopener">ResNet, AlexNet, VGG, Inception: Understanding various architectures of Convolutional Networks</a></li>
<li><a href="https://www.bilibili.com/video/av16499811/" target="_blank" rel="noopener">Neural Networks For Recommender Systems</a></li>
<li><a href="https://www.technologyreview.com/" target="_blank" rel="noopener">MIT Technology Review</a><br>A good place to keep up the trends.</li>
<li><a href="https://gab41.lab41.org/" target="_blank" rel="noopener">LAB41</a><br>Lab41 is a Silicon Valley challenge lab where experts from the U.S. Intelligence Community (IC), academia, industry, and In-Q-Tel come together to gain a better understanding of how to work with — and ultimately use — big data.</li>
<li><a href="http://www.partnershiponai.org/" target="_blank" rel="noopener">Partnership on AI</a><br>Amazon, DeepMind/Google, Facebook, IBM, and Microsoft announced that they will create a non-profit organization that will work to advance public understanding of artificial intelligence technologies (AI) and formulate best practices on the challenges and opportunities within the field. Academics, non-profits, and specialists in policy and ethics will be invited to join the Board of the organization, named the Partnership on Artificial Intelligence to Benefit People and Society (Partnership on AI).</li>
<li><a href="http://weibo.com/fly51fly?from=profile&amp;wvr=6&amp;is_all=1" target="_blank" rel="noopener">爱可可-爱生活</a> 老师的推荐十分值得一看</li>
<li><a href="https://github.com/dusty-nv/jetson-inference" target="_blank" rel="noopener">Guide to deploying deep-learning inference networks and realtime object recognition tutorial for NVIDIA Jetson TX1</a></li>
<li><a href="https://medium.com/@kcimc/a-return-to-machine-learning-2de3728558eb" target="_blank" rel="noopener">A Return to Machine Learning</a><br>This post is aimed at artists and other creative people who are interested in a survey of recent developments in machine learning research that intersect with art and culture. If you’ve been following ML research recently, you might find some of the experiments interesting but will want to skip most of the explanations.</li>
<li><a href="https://medium.com/@awjuliani/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32#.1d3mpy4hd" target="_blank" rel="noopener">ResNets, HighwayNets, and DenseNets, Oh My!</a><br>This post walks through the logic behind three recent deep learning architectures: ResNet, HighwayNet, and DenseNet. Each make it more possible to successfully trainable deep networks by overcoming the limitations of traditional network design.</li>
<li><p><a href="https://www.oreilly.com/learning/how-to-build-a-robot-that-sees-with-100-and-tensorflow" target="_blank" rel="noopener">How to build a robot that “sees” with $100 and TensorFlow</a> &gt;I wanted to build a robot that could recognize objects. Years of experience building computer programs and doing test-driven development have turned me into a menace working on physical projects. In the real world, testing your buggy device can burn down your house, or at least fry your motor and force you to wait a couple of days for replacement parts to arrive.</p>
</li>
<li><p><a href="https://culurciello.github.io//tech/2016/06/10/unsup.html" target="_blank" rel="noopener">Navigating the unsupervised learning landscape</a><br>Unsupervised learning is the Holy Grail of Deep Learning. The goal of unsupervised learning is to create general systems that can be trained with little data. Very little data.</p>
</li>
<li><a href="http://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">Deconvolution and Checkerboard Artifacts</a></li>
<li><a href="http://www.mattkrzus.com/face.html" target="_blank" rel="noopener">Facial Recognition on a Jetson TX1 in Tensorflow</a><br>Here’s a way to hack facial recognition system together in relatively short time on NVIDIA’s Jetson TX1.</li>
<li><a href="https://amundtveit.com/2016/11/12/deep-learning-with-generative-and-generative-adverserial-networks-iclr-2017-discoveries/" target="_blank" rel="noopener">Deep Learning with Generative and Generative Adverserial Networks – ICLR 2017 Discoveries</a><br>This blog post gives an overview of Deep Learning with Generative and Adverserial Networks related papers submitted to ICLR 2017.</li>
<li><a href="https://amundtveit.com/2016/11/12/unsupervised-deep-learning-iclr-2017-discoveries/" target="_blank" rel="noopener">Unsupervised Deep Learning – ICLR 2017 Discoveries</a><br>This blog post gives an overview of papers related to Unsupervised Deep Learning submitted to ICLR 2017.</li>
<li><a href="https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-38dad1cf7571" target="_blank" rel="noopener">You Only Look Twice — Multi-Scale Object Detection in Satellite Imagery With Convolutional Neural Networks</a></li>
<li><a href="https://hackernoon.com/deep-learning-isnt-the-brain-e1d800ebb5a9?gi=1ef40a41a579" target="_blank" rel="noopener">Deep Learning isn’t the brain</a></li>
<li><a href="https://blog.insightdatascience.com/isee-removing-eyeglasses-from-faces-using-deep-learning-d4e7d935376f#.kmld0zlxz" target="_blank" rel="noopener">iSee: Using deep learning to remove eyeglasses from faces</a></li>
<li><a href="http://gabgoh.github.io/ThoughtVectors/" target="_blank" rel="noopener">Decoding The Thought Vector</a></li>
<li><a href="http://venturebeat.com/2016/11/29/algorithmia-will-help-you-make-ai-powered-photo-filters/" target="_blank" rel="noopener">Algorithmia will help you make your own AI-powered photo filters</a></li>
<li><a href="http://ahogrammer.com/2016/11/15/deep-learning-enables-you-to-hide-screen-when-your-boss-is-approaching/" target="_blank" rel="noopener">Deep Learning Enables You to Hide Screen when Your Boss is Approaching</a></li>
<li><a href="http://weibo.com/ttarticle/p/show?id=2309404050111117306404" target="_blank" rel="noopener">对偶学习：一种新的机器学习范式</a></li>
<li><p><a href="https://github.com/soumith/ganhacks" target="_blank" rel="noopener">How to Train a GAN? Tips and tricks to make GANs work</a></p>
<blockquote>
<p>While research in Generative Adversarial Networks (GANs) continues to improve the fundamental stability of these models, we use a bunch of tricks to train them and make them stable day to day.</p>
</blockquote>
</li>
<li><p><a href="https://medium.com/seldon-open-source-machine-learning/highlights-of-ieee-big-data-2016-nearest-neighbours-outliers-and-deep-learning-696d014d8cdf#.8xnfbuofv" target="_blank" rel="noopener">Highlights of IEEE Big Data 2016: Nearest Neighbours, Outliers and Deep Learning</a></p>
</li>
<li><a href="http://www.erogol.com/cnn-visualization-tools-techniques/" target="_blank" rel="noopener">Some CNN visualization tools and techniques</a><br>Besides this post, the others written by the author are also worthy of reading.</li>
<li><a href="http://www.deeplearningweekly.com/blog/deep-learning-2016-the-year-in-review" target="_blank" rel="noopener">Deep Learning 2016: The Year in Review</a></li>
<li><a href="https://medium.com/@Moscow25/gans-will-change-the-world-7ed6ae8515ca#.uod2m7gpp" target="_blank" rel="noopener">GANs will change the world</a></li>
<li><a href="http://colah.github.io/" target="_blank" rel="noopener">colah’s blog</a></li>
<li><a href="https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/" target="_blank" rel="noopener">Analysis of Dropout</a></li>
<li><a href="https://gab41.lab41.org/nips-2016-review-day-1-6e504bcf1451#.g2wwg6g6a" target="_blank" rel="noopener">NIPS 2016 Review</a></li>
<li><a href="http://mp.weixin.qq.com/s/-JxpVh_sAIXWueSBfzxbsA" target="_blank" rel="noopener">【榜单】GitHub 最受欢迎深度学习应用项目 Top 16（持续更新）</a></li>
<li><a href="http://blog.yhat.com/posts/why-support-vector-machine.html" target="_blank" rel="noopener">Why use SVM?</a></li>
<li><a href="http://svds.com/tensorflow-image-recognition-raspberry-pi/" target="_blank" rel="noopener">TensorFlow Image Recognition on a Raspberry Pi</a></li>
<li><a href="https://medium.com/@bfortuner/building-your-own-deep-learning-box-47b918aea1eb#.gabyza8vg" target="_blank" rel="noopener">Building Your Own Deep Learning Box</a></li>
<li><a href="https://medium.com/@ksakmann/vehicle-detection-and-tracking-using-hog-features-svm-vs-yolo-73e1ccb35866#.b6215aaoi" target="_blank" rel="noopener">Vehicle tracking using a support vector machine vs. YOLO</a></li>
<li><a href="https://blog.acolyer.org/2017/02/27/understanding-generalisation-and-transfer-learning-in-deep-neural-networks/" target="_blank" rel="noopener">Understanding, generalisation, and transfer learning in deep neural networks</a></li>
<li><a href="http://www.phoronix.com/scan.php?page=article&amp;item=nvidia-jtx2-denver&amp;num=1" target="_blank" rel="noopener">NVIDIA Announces The Jetson TX2, Powered By NVIDIA’s “Denver 2” CPU &amp; Pascal Graphics</a></li>
<li><a href="https://www.nextplatform.com/2017/03/21/can-fpgas-beat-gpus-accelerating-next-generation-deep-learning/" target="_blank" rel="noopener">Can FPGAs Beat GPUs in Accelerating Next-Generation Deep Learning?</a></li>
<li><a href="https://gab41.lab41.org/flexible-image-tagging-with-fast0tag-681c6283c9b7" target="_blank" rel="noopener">Flexible Image Tagging with Fast0Tag</a></li>
<li><a href="https://blogs.nvidia.com/blog/2016/08/30/eye-tracking-deep-learning/" target="_blank" rel="noopener">Eye Fidelity: How Deep Learning Will Help Your Smartphone Track Your Gaze</a></li>
<li><a href="https://www.slideshare.net/HJvanVeen/using-deep-learning-to-find-similar-dresses" target="_blank" rel="noopener">Using Deep Learning to Find Similar Dresses</a></li>
<li><a href="http://211.136.65.144/cache/martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf?ich_args2=63-30201313013522_2c061a5e256a1540143166bdb81879f4_10001002_9c886d2cdfc4f4d89e3e518939a83798_1fc45e3ebbbb5d0180278ba0111aceaf" target="_blank" rel="noopener">Rules of Machine Learning: Best Practices for ML Engineering</a></li>
<li><a href="https://culurciello.github.io/tech/2016/06/04/nets.html" target="_blank" rel="noopener">Neural Network Architectures</a></li>
<li><a href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4" target="_blank" rel="noopener">A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN</a></li>
<li><a href="https://zhuanlan.zhihu.com/xiaoleimlnote" target="_blank" rel="noopener">晓雷机器学习笔记</a></li>
<li><a href="https://github.com/Fdevmsy/Image_Classification_with_5_methods" target="_blank" rel="noopener">Image Classification with 5 methods</a></li>
<li><a href="http://brohrer.github.io/how_convolutional_neural_networks_work.html" target="_blank" rel="noopener">How do Convolutional Neural Networks work?</a></li>
<li><a href="https://www.zhihu.com/question/60759296/answer/180176680" target="_blank" rel="noopener">基于深度卷积神经网络进行人脸识别的原理是什么</a></li>
<li><a href="https://medium.com/@julsimon/10-deep-learning-projects-based-on-apache-mxnet-8231109f3f64" target="_blank" rel="noopener">10 Deep Learning projects based on Apache MXNet</a></li>
<li><a href="http://www.offconvex.org/" target="_blank" rel="noopener">Off the Convex Path</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/26746283" target="_blank" rel="noopener">图像风格迁移(Neural Style)简史</a></li>
<li><a href="https://blog.metaflow.fr/ml-notes-why-the-log-likelihood-24f7b6c40f83" target="_blank" rel="noopener">ML notes: Why the log-likelihood?</a></li>
<li><a href="https://blog.statsbot.co/generative-adversarial-networks-gans-engine-and-applications-f96291965b47" target="_blank" rel="noopener">Generative Adversarial Networks (GANs): Engine and Applications</a></li>
<li><a href="http://machinethink.net/blog/compressing-deep-neural-nets/" target="_blank" rel="noopener">Compressing deep neural nets</a></li>
<li><a href="https://sigmoidal.io/" target="_blank" rel="noopener">Sigmoidal</a></li>
<li><a href="https://machinelearningmastery.com/gentle-introduction-bag-words-model/" target="_blank" rel="noopener">A Gentle Introduction to the Bag-of-Words Model</a></li>
<li><a href="http://guimperarnau.com/blog/2017/03/Fantastic-GANs-and-where-to-find-them" target="_blank" rel="noopener">Fantastic GANs and where to find them</a></li>
<li><a href="http://guimperarnau.com/blog/2017/11/Fantastic-GANs-and-where-to-find-them-II" target="_blank" rel="noopener">Fantastic GANs and where to find them II</a></li>
</ul>
<h3 id="BENCHMARK-LEADERBOARD-DATASET"><a href="#BENCHMARK-LEADERBOARD-DATASET" class="headerlink" title="BENCHMARK/LEADERBOARD/DATASET"></a>BENCHMARK/LEADERBOARD/DATASET</h3><ul>
<li><a href="https://arxiv.org/abs/1910.06663" target="_blank" rel="noopener">AI Benchmark: All About Deep Learning on Smartphones in 2019</a></li>
<li><a href="https://github.com/lvis-dataset/lvis-api" target="_blank" rel="noopener">LVIS API</a> LVIS: is a new dataset for Large Vocabulary Instance Segmentation.</li>
<li><a href="https://github.com/PKU-IMRE/VERI-Wild" target="_blank" rel="noopener">VERI-Wild</a> A Large-Scale Dataset for Vehicle Re-Identification in the Wild</li>
<li><a href="https://github.com/VehicleReId/VeRidataset" target="_blank" rel="noopener">VeRi dataset</a> is a large scale image dataset for vehicle re-identification in urban traffic surveillance.</li>
<li><a href="https://www.stateoftheart.ai/" target="_blank" rel="noopener">State of the Art</a></li>
<li><a href="https://stair-lab-cit.github.io/STAIR-actions-web/" target="_blank" rel="noopener">STAIR Actions: A Video Dataset of Everyday Home Actions</a><br>STAIR Actions is a video dataset consisting of 100 everyday human action categories.</li>
<li><a href="http://cmp.felk.cvut.cz/revisitop/" target="_blank" rel="noopener">Revisiting Oxford and Paris: Large-Scale Image Retrieval Benchmarking</a></li>
<li><a href="https://epic-kitchens.github.io/2018" target="_blank" rel="noopener">EPIC-Kitchens</a><br>The largest dataset in first-person (egocentric) vision; multi-faceted non-scripted recordings in native environments - i.e. the wearers’ homes, capturing all daily activities in the kitchen over multiple days. Annotations are collected using a novel `live’ audio commentary approach.</li>
<li><a href="https://landmarkscvprw18.github.io/" target="_blank" rel="noopener">Large-Scale Landmark Recognition: A Challenge</a></li>
<li><a href="https://rebootingcomputing.ieee.org/lpirc" target="_blank" rel="noopener">Low-Power Image Recognition Challenge</a></li>
<li><a href="https://github.com//openimages/dataset" target="_blank" rel="noopener">Open Images Dataset</a><br>Open Images is a dataset of ~9 million URLs to images that have been annotated with image-level labels and bounding boxes spanning thousands of classes.</li>
<li><a href="http://cvlab.hanyang.ac.kr/tracker_benchmark/index.html" target="_blank" rel="noopener">Visual Tracker Benchmark</a><br>This website contains data and code of the benchmark evaluation of online visual tracking algorithms. Join visual-tracking Google groups for further updates, discussions, or QnAs.</li>
<li><a href="https://motchallenge.net/" target="_blank" rel="noopener">Multiple Object Tracking Benchmark</a><br>With this benchmark we would like to pave the way for a unified framework towards more meaningful quantification of multi-target tracking.</li>
<li><a href="http://host.robots.ox.ac.uk:8080/leaderboard/main_bootstrap.php" target="_blank" rel="noopener">Leaderboards for the Evaluations on PASCAL VOC Data</a></li>
<li><a href="https://github.com/openimages/dataset" target="_blank" rel="noopener">Open Images dataset</a><br>Open Images is a dataset of ~9 million URLs to images that have been annotated with labels spanning over 6000 categories.</li>
<li><a href="https://github.com/udacity/self-driving-car" target="_blank" rel="noopener">Open Sourcing 223GB of Driving Data</a><br>223GB of image frames and log data from 70 minutes of driving in Mountain View on two separate days, with one day being sunny, and the other overcast.</li>
<li><a href="http://mscoco.org" target="_blank" rel="noopener">MS COCO</a></li>
<li><a href="http://umdfaces.io/" target="_blank" rel="noopener">UMDFaces Dataset</a><br>UMDFaces is a face dataset which has 367,920 faces of 8,501 subjects. From this page you can download the entire dataset and the trained model for predicting the localization of the 21 keypoints.</li>
<li><a href="http://videonet.team/" target="_blank" rel="noopener">VideoNet</a><br>VideoNet is a new initiative to bring together the community of researchers that have put effort into creating benchmarks for video tasks.</li>
<li><a href="https://arxiv.org/abs/1702.00824" target="_blank" rel="noopener">YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in Video</a></li>
<li><a href="http://www.cvlibs.net/datasets/kitti/index.php" target="_blank" rel="noopener">KITTI Vision Benchmark Suite</a></li>
<li><a href="https://github.com/layumi/Duke_evaluation" target="_blank" rel="noopener">Duke: A New Large-scale Person Re-identification Dataset derived from DukeMTMC</a><br>Duke is a subset of the DukeMTMC for image-based re-ID, in the format of the Market-1501 dataset. The original dataset contains 85-minute high-resolution videos from 8 different cameras. Hand-drawn pedestrain bounding boxes are available.</li>
<li><a href="http://blog.mapillary.com/product/2017/05/03/mapillary-vistas-dataset.html" target="_blank" rel="noopener">Releasing the World’s Largest Street-level Imagery Dataset for Teaching Machines to See</a> &gt;Today we present the Mapillary Vistas Dataset—the world’s largest and most diverse publicly available, pixel-accurately and instance-specifically annotated street-level imagery dataset for empowering autonomous mobility and transport at the global scale.</li>
<li><a href="http://www.vision.ee.ethz.ch/webvision/" target="_blank" rel="noopener">WEBVISION DATASET</a><blockquote>
<p>The WebVision dataset is designed to facilitate the research on learning visual representation from noisy web data. Our goal is to disentangle the deep learning techniques from huge human labor on annotating large-scale vision dataset. We release this large scale web images dataset as a benchmark to advance the research on learning from web data, including weakly supervised visual representation learning, visual transfer learning, text and vision, etc.</p>
</blockquote>
</li>
<li><a href="https://github.com/NEU-Gou/DukeReID" target="_blank" rel="noopener">DukeMTMC4ReID</a><blockquote>
<p>DukeMTMC4ReID dataset is new large-scale real-world person re-id dataset based on DukeMTMC.</p>
</blockquote>
</li>
<li><a href="http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html" target="_blank" rel="noopener">Person Re-identification Datasets</a><blockquote>
<p>Person re-identification has drawn intensive attention in the computer vision society in recent decades. As far as we know, this page collects all public datasets that have been tested by person re-identification algorithms.</p>
</blockquote>
</li>
<li><a href="http://saliency.mit.edu/home.html" target="_blank" rel="noopener">MIT Saliency Benchmark</a></li>
<li><a href="https://www.adrianbulat.com/face-alignment/" target="_blank" rel="noopener">How far are we from solving the 2D &amp; 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)</a></li>
<li><a href="https://posetrack.net/workshops/iccv2017/posetrack-challenge-results.html" target="_blank" rel="noopener">PoseTrack: A Benchmark for Human Pose Estimation and Tracking</a></li>
</ul>
<h3 id="TOOLKITS"><a href="#TOOLKITS" class="headerlink" title="TOOLKITS"></a>TOOLKITS</h3><ul>
<li><a href="https://github.com/StatMixedML/XGBoostLSS" target="_blank" rel="noopener">XGBoostLSS</a> An extension of XGBoost to probabilistic forecasting</li>
<li><a href="https://github.com/lutzroeder/netron" target="_blank" rel="noopener">Netron</a> is a viewer for neural network, deep learning and machine learning models.</li>
<li><a href="https://blue-oil.org/" target="_blank" rel="noopener">Bring Deep Learning to small devices</a> An open source deep learning platform for low bit computation</li>
<li><a href="https://github.com/albu/albumentations" target="_blank" rel="noopener">Albumentations</a> fast image augmentation library and easy to use wrapper around other libraries.</li>
<li><a href="https://github.com/Tencent/FeatherCNN" target="_blank" rel="noopener">FeatherCNN</a><br>FeatherCNN is a high performance inference engine for convolutional neural networks.</li>
<li><a href="http://caffe.berkeleyvision.org/" target="_blank" rel="noopener">Caffe</a><br>Caffe is a deep learning framework made with expression, speed, and modularity in mind. It is developed by the Berkeley Vision and Learning Center (BVLC) and by community contributors. Yangqing Jia created the project during his PhD at UC Berkeley. Caffe is released under the BSD 2-Clause license.</li>
<li><a href="https://github.com/caffe2/caffe2" target="_blank" rel="noopener">Caffe2</a><br>Caffe2 is a deep learning framework made with expression, speed, and modularity in mind. It is an experimental refactoring of Caffe, and allows a more flexible way to organize computation.</li>
<li><a href="https://github.com/intel/caffe" target="_blank" rel="noopener">Caffe on Intel</a><br>This fork of BVLC/Caffe is dedicated to improving performance of this deep learning framework when running on CPU, in particular Intel® Xeon processors (HSW+) and Intel® Xeon Phi processors</li>
<li><a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener">TensorFlow</a><br>TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) that flow between them. This flexible architecture lets you deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device without rewriting code. TensorFlow also includes TensorBoard, a data visualization toolkit.</li>
<li><a href="http://mxnet.io/" target="_blank" rel="noopener">MXNet</a><br>MXNet is a deep learning framework designed for both efficiency and flexibility. It allows you to mix the flavours of symbolic programming and imperative programming to maximize efficiency and productivity. In its core, a dynamic dependency scheduler that automatically parallelizes both symbolic and imperative operations on the fly. A graph optimization layer on top of that makes symbolic execution fast and memory efficient. The library is portable and lightweight, and it scales to multiple GPUs and multiple machines.</li>
<li><a href="https://github.com/NervanaSystems/neon" target="_blank" rel="noopener">neon</a><br>neon is Nervana’s Python based Deep Learning framework and achieves the fastest performance on modern deep neural networks such as AlexNet, VGG and GoogLeNet. Designed for ease-of-use and extensibility.</li>
<li><a href="https://pdollar.github.io/toolbox/" target="_blank" rel="noopener">Piotr’s Computer Vision Matlab Toolbox</a><br>This toolbox is meant to facilitate the manipulation of images and video in Matlab. Its purpose is to complement, not replace, Matlab’s Image Processing Toolbox, and in fact it requires that the Matlab Image Toolbox be installed. Emphasis has been placed on code efficiency and code reuse. Thanks to everyone who has given me feedback - you’ve helped make this toolbox more useful and easier to use.</li>
<li><a href="https://developer.nvidia.com/" target="_blank" rel="noopener">NVIDIA Developer</a></li>
<li><a href="https://github.com/NVIDIA/caffe/tree/experimental/fp16" target="_blank" rel="noopener">nvCaffe</a><br>A special branch of caffe is used on TX1 which includes support for FP16.</li>
<li><a href="http://dlib.net/" target="_blank" rel="noopener">dlib</a><br>Dlib is a modern C++ toolkit containing machine learning algorithms and tools for creating complex software in C++ to solve real world problems. It is used in both industry and academia in a wide range of domains including robotics, embedded devices, mobile phones, and large high performance computing environments. Dlib’s open source licensing allows you to use it in any application, free of charge.</li>
<li><a href="http://opencv.org/" target="_blank" rel="noopener">OpenCV</a><br>OpenCV is released under a BSD license and hence it’s free for both academic and commercial use. It has C++, C, Python and Java interfaces and supports Windows, Linux, Mac OS, iOS and Android. OpenCV was designed for computational efficiency and with a strong focus on real-time applications.</li>
<li><a href="https://github.com/ENCP/CNNdroid" target="_blank" rel="noopener">CNNdroid</a><br>CNNdroid is an open source library for execution of trained convolutional neural networks on Android devices.</li>
<li><p><a href="https://github.com/tiny-dnn/tiny-dnn" target="_blank" rel="noopener">tiny dnn</a><br>tiny-dnn is a C++11 implementation of deep learning. It is suitable for deep learning on limited computational resource, embedded systems and IoT devices.</p>
<p>An introduction to this toolkit at《<a href="http://www.slideshare.net/ssuser756ec5/deep-learning-with-c-an-introduction-to-tinydnn" target="_blank" rel="noopener">Deep learning with C++ - an introduction to tiny-dnn》by Taiga Nomi</a></p>
</li>
<li><p><a href="https://github.com/sciencefans/CaffeMex_v2" target="_blank" rel="noopener">CaffeMex</a><br>A multi-GPU &amp; memory-reduced MAT-Caffe on LINUX and WINDOWS</p>
</li>
<li><a href="https://developers.google.com/ar/" target="_blank" rel="noopener">ARCore</a> ARCore is a platform for building augmented reality apps on Android. ARCore uses three key technologies to integrate virtual content with the real world as seen through your phone’s camera</li>
<li><a href="https://github.com/Microsoft/CNTK" target="_blank" rel="noopener">CNTK</a> Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit.</li>
<li><a href="http://onnx.ai/" target="_blank" rel="noopener">ONNX</a> ONNX is a open format to represent deep learning models. With ONNX, AI developers can more easily move models between state-of-the-art tools and choose the combination that is best for them. ONNX is developed and supported by a community of partners.</li>
<li><a href="http://pytoune.org/en/latest/" target="_blank" rel="noopener">PyToune</a> is a Keras-like framework for PyTorch and handles much of the boilerplating code needed to train neural networks.</li>
<li><a href="http://deepcognition.ai/desktop/" target="_blank" rel="noopener">Deep Learning Studio - Desktop DeepCognition.ai</a> is a single user solution that runs locally on your hardware. Desktop version allows you to train models on your GPU(s) without uploading data to the cloud. The platform supports transparent multi-GPU training for up to 4 GPUs. Additional GPUs are supported in Deep Learning Studio – Enterprise.</li>
</ul>
<h3 id="LEARNING-TRICKS-TIPS"><a href="#LEARNING-TRICKS-TIPS" class="headerlink" title="LEARNING/TRICKS/TIPS"></a>LEARNING/TRICKS/TIPS</h3><ul>
<li><a href="https://arxiv.org/abs/1804.00247" target="_blank" rel="noopener">Training Tips for the Transformer Model</a></li>
<li><a href="https://machinelearningmastery.com/deep-learning-courses/" target="_blank" rel="noopener">Deep Learning Courses</a></li>
<li><a href="http://deeplearning.stanford.edu/wiki/index.php/Backpropagation_Algorithm" target="_blank" rel="noopener">Backpropagation Algorithm</a><br>A website that explain how Backpropagation Algorithm works.</li>
<li><a href="http://www.deeplearningbook.org/" target="_blank" rel="noopener">Deep Learning (textbook authored by Ian Goodfellow and Yoshua Bengio and Aaron Courville)</a><br>The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in particular.</li>
<li><a href="http://neuralnetworksanddeeplearning.com/index.html" target="_blank" rel="noopener">Neural Networks and Deep Learning (online book authored by Michael Nielsen)</a><br>Neural Networks and Deep Learning is a free online book. The book will teach you about 1) Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data and 2) Deep learning, a powerful set of techniques for learning in neural networks. Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.</li>
<li><a href="http://szeliski.org/Book/" target="_blank" rel="noopener">Computer Vision: Algorithms and Applications</a><br>This book is largely based on the computer vision courses that Richard Szeliski has co-taught at the University of Washington (2008, 2005, 2001) and Stanford (2003) with Steve Seitz and David Fleet.</li>
<li><a href="http://210.28.132.67/weixs/project/CNNTricks/CNNTricks.html" target="_blank" rel="noopener">Must Know Tips/Tricks in Deep Neural Networks </a><br>Many implementation details for DCNNs are collected and concluded. Extensive implementation details are introduced, i.e., tricks or tips, for building and training your own deep networks.</li>
<li><a href="http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html" target="_blank" rel="noopener">The zen of gradient descent</a></li>
<li><a href="https://kevinzakka.github.io/2016/09/14/batch_normalization/" target="_blank" rel="noopener">Deriving the Gradient for the Backward Pass of Batch Normalization</a></li>
<li><a href="https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html" target="_blank" rel="noopener">Reinforcement Learning: An Introduction</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a></li>
<li><a href="https://pan.baidu.com/s/1kUUtxdl" target="_blank" rel="noopener">Regularizing neural networks by penalizing confident predictions</a></li>
<li><a href="https://cartesianfaith.com/2016/10/06/what-you-need-to-know-about-data-augmentation-for-machine-learning/" target="_blank" rel="noopener">What you need to know about data augmentation for machine learning</a><br>Plentiful high-quality data is the key to great machine learning models. But good data doesn’t grow on trees, and that scarcity can impede the development of a model. One way to get around a lack of data is to augment your dataset. Smart approaches to programmatic data augmentation can increase the size of your training set 10-fold or more. Even better, your model will often be more robust (and prevent overfitting) and can even be simpler due to a better training set.</li>
<li>[Guide to deploying deep-learning inference networks and realtime object recognition tutorial for NVIDIA Jetson TX1]</li>
<li><a href="https://medium.com/the-downlinq/the-effect-of-resolution-on-deep-neural-network-image-classification-accuracy-d1338e2782c5#.5iwdz2te8" target="_blank" rel="noopener">The Effect of Resolution on Deep Neural Network Image Classification Accuracy</a><br>The author explored the impact of both spatial resolution and training dataset size on the classification performance of deep neural networks in this post.</li>
<li><a href="https://www.zhihu.com/question/25097993" target="_blank" rel="noopener">深度学习调参的技巧</a></li>
<li><a href="https://www.zhihu.com/question/27962483" target="_blank" rel="noopener">CNN 怎么调参数</a></li>
<li><a href="https://www.zhihu.com/question/40545681" target="_blank" rel="noopener">视频多目标跟踪当前（2014,2015,2016）比较好的算法有哪些</a></li>
<li><a href="https://www.neuraldesigner.com/blog/5_algorithms_to_train_a_neural_network.html" target="_blank" rel="noopener">5 algorithms to train a neural network</a></li>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;mid=2650325586&amp;idx=1&amp;sn=69a7e8482d884dda869290581a50a6d6&amp;chksm=f235a558c5422c4eabe75f6db92fd13a3ca662d648676461914c9bfd1f4e22affe12430afce3&amp;mpshare=1&amp;scene=2&amp;srcid=1024IdcoRukOMBvDA5dPbRoV&amp;from=timeline&amp;isappinstalled=0#wechat_redirect" target="_blank" rel="noopener">Towards Good Practices for Recognition &amp; Detection</a><br>海康威视研究院 ImageNet2016 竞赛经验分享</li>
<li><a href="https://www.quora.com/What-are-the-differences-between-Random-Forest-and-Gradient-Tree-Boosting-algorithms" target="_blank" rel="noopener">What are the differences between Random Forest and Gradient Tree Boosting algorithms</a></li>
<li><a href="https://www.zhihu.com/question/43370067" target="_blank" rel="noopener">为什么现在的 CNN 模型都是在 GoogleNet、VGGNet 或者 AlexNet 上调整的</a></li>
<li><a href="https://nndl.github.io/" target="_blank" rel="noopener">神经网络与深度学习</a></li>
<li><a href="http://mp.weixin.qq.com/s/t3U_gUfe5KekrH-jDSDk_w" target="_blank" rel="noopener">ILSVRC2016 目标检测任务回顾(上)——图像目标检测(DET)</a></li>
<li><a href="http://mp.weixin.qq.com/s/mQ78KNuaHUTox3ql6rU-Nw" target="_blank" rel="noopener">ILSVRC2016 目标检测任务回顾(下)——视频目标检测(VID)</a></li>
<li><a href="https://github.com/soumith/ganhacks" target="_blank" rel="noopener">How to Train a GAN? Tips and tricks to make GANs work</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="noopener">令人拍案叫绝的 Wasserstein GAN</a></li>
<li><a href="https://courses.csail.mit.edu/6.042/spring17/mcs.pdf" target="_blank" rel="noopener">Mathematics for Computer Science</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&amp;mid=2651995256&amp;idx=1&amp;sn=f508a61ebd4792b4b407ac7418c4f1ab" target="_blank" rel="noopener">生成式对抗网络 GAN 的研究进展与展望</a></li>
<li><a href="https://www.medium.com/@nikasa1889/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807" target="_blank" rel="noopener">A guide to receptive field arithmetic for Convolutional Neural Networks</a></li>
<li><a href="http://geek.csdn.net/news/detail/191718" target="_blank" rel="noopener">见微知著：细粒度图像分析进展</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/27292838" target="_blank" rel="noopener">目标跟踪相关资源</a></li>
<li><a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/" target="_blank" rel="noopener">Intro to Neural Networks and Machine Learning</a></li>
<li><a href="http://danijar.com/tips-for-training-recurrent-neural-networks/" target="_blank" rel="noopener">Tips for Training Recurrent Neural Networks</a></li>
<li><a href="http://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/" target="_blank" rel="noopener">A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size</a></li>
<li><a href="https://www.bilibili.com/video/av12532910/" target="_blank" rel="noopener">Learning To See（机器学习计算机视觉入门）（英文字幕）</a></li>
<li><a href="https://medium.com/@slavivanov/4020854bd607" target="_blank" rel="noopener">37 Reasons why your Neural Network is not working</a></li>
<li><a href="http://theorangeduck.com/page/neural-network-not-working" target="_blank" rel="noopener">My Neural Network isn’t working! What should I do?</a></li>
<li><a href="http://www.shakirm.com/slides/DeepGenModelsTutorial.pdfg" target="_blank" rel="noopener">Tutorial on Deep Generative Models</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650740517&amp;idx=1&amp;sn=0b69d9a42f5ca18ed513cee309cc27a1&amp;chksm=871ad35bb06d5a4d9f4d30e848c023247358db64af5b6d3fe6453412d992c1c5c49d4fe3aca6#rd" target="_blank" rel="noopener">卷积神经网络：从基础技术到研究前景</a></li>
</ul>
<h2 id="SKILLS"><a href="#SKILLS" class="headerlink" title="SKILLS"></a>SKILLS</h2><h3 id="ABOUT-CAFFE"><a href="#ABOUT-CAFFE" class="headerlink" title="ABOUT CAFFE"></a>ABOUT CAFFE</h3><ul>
<li><a href="https://joshua19881228.github.io/2015-08-30-Setting-Up-Caffe/" target="_blank" rel="noopener">Set Up Caffe on Ubuntu14.04 64bit+NVIDIA GTX970M+CUDA7.0</a></li>
<li><a href="http://blog.csdn.net/joshua_1988/article/details/45048871" target="_blank" rel="noopener">VS2013 配置 Caffe 卷积神经网络工具（64 位 Windows 7）——建立工程</a></li>
<li><a href="http://blog.csdn.net/joshua_1988/article/details/45036993" target="_blank" rel="noopener">VS2013 配置 Caffe 卷积神经网络工具（64 位 Windows 7）——准备依赖库</a></li>
</ul>
<h3 id="SETTING-UP"><a href="#SETTING-UP" class="headerlink" title="SETTING UP"></a>SETTING UP</h3><ul>
<li><a href="https://joshua19881228.github.io/2015-08-29-Ndriver-Cuda/" target="_blank" rel="noopener">Installation of NVIDIA GPU Driver and CUDA Toolkit</a></li>
<li><a href="https://marcnu.github.io/2016-08-17/Tensorflow-v0.10-installed-from-scratch-Ubuntu-16.04-CUDA8.0RC-cuDNN5.1-1080GTX/" target="_blank" rel="noopener">Tensorflow v0.10 installed from scratch on Ubuntu 16.04, CUDA 8.0RC+Patch, cuDNN v5.1 with a 1080GTX</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIzMzI0NjkwMw==&amp;mid=2652210076&amp;idx=1&amp;sn=f7150cc62d68eb9fc29d3a65f9834104" target="_blank" rel="noopener">DL 小钢炮攒机心得 帮你踩坑</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/08/23/Computer_Vision/Reading_Note/2016-08-23-Reading-note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar-icon.png">
      <meta itemprop="name" content="Joshua LI">
      <meta itemprop="description" content="Do not aim for success if you want it; just do what you love and believe in, and it will come naturally.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Joshua's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/08/23/Computer_Vision/Reading_Note/2016-08-23-Reading-note/" class="post-title-link" itemprop="url">ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-08-23 00:00:00" itemprop="dateCreated datePublished" datetime="2016-08-23T00:00:00+08:00">2016-08-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-08-19 17:11:04" itemprop="dateModified" datetime="2022-08-19T17:11:04+08:00">2022-08-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Computer-Vision/" itemprop="url" rel="index"><span itemprop="name">Computer Vision</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>TITLE</strong>: ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation</p>
<p><strong>AUTHER</strong>: Adam Paszke, Abhishek Chaurasia, Sangpil Kim, Eugenio Culurciello</p>
<p><strong>ASSOCIATION</strong>: University of Warsaw, Purdue University</p>
<p><strong>FROM</strong>: <a href="http://arxiv.org/abs/1606.02147" target="_blank" rel="noopener">arXiv:1606.02147</a></p>
<h3 id="CONTRIBUTIONS"><a href="#CONTRIBUTIONS" class="headerlink" title="CONTRIBUTIONS"></a>CONTRIBUTIONS</h3><ol>
<li>A novel deep neural network architecture named ENet (efficient neural network) is propsed, which is quite efficient.</li>
<li>A serie of designing strategies is discussed.</li>
</ol>
<h3 id="Design-Choices"><a href="#Design-Choices" class="headerlink" title="Design Choices"></a>Design Choices</h3><p><strong>Network Architecture</strong></p>
<p>Readers could refer to the paper to have a look at the network architecture. The network is inspired by ResNet structure, while the authers re-design it based on the specific task of semantic segmentation and their intuitions. The intial block and basic building block (bottlenect module) is shown in the following figure. After the intial block, a comparetively large encoder is constructed using the bottleneck module. On the other hand, a smaller decoder follows the encoder.</p>
<p><img class="img-responsive center-block" src="https://raw.githubusercontent.com/joshua19881228/my_blogs/master/Computer_Vision/Reading_Note/figures/blocks.jpg" alt="" width="640"/></p>
<p><strong>Design Strategy</strong></p>
<ol>
<li><strong>Feature map resolution:</strong> Small feature map resolution has two drawbacks 1) loss of finer information of edges and 2) smaller size compared with original image. The advantage is that small feature map resolution means larger receptive field and more context for the filters. The first problem is solved by adding more feature maps or unsampling technique.</li>
<li><strong>Early downsampling:</strong> Early downsampling is very helpful for boosting the efficiency of the network while persisting the performance. The idea is that visual information is highly redundant and that initial network layers should not directly contribute to classification but act as good feature extractors.</li>
<li><strong>Decoder size:</strong> In most previous works, the encoder and decoder have the same size, for example totally symmetric. In this work, the auther uses a larger encoder and a smaller decoder. The responsibility of encoder is to operate on smaller resolution data and provide for information processing and filtering. Instead, the role of the the decoder, is to upsample the output of the encoder, only fine-tuning the details.</li>
<li><strong>Nonlinear operations</strong> In this paper some interesting observations are carried out. The auther invetigates the effect of nonlinear operations by training the network using PReLU. All layers in the main branch behave nearly exactly like regular ReLUs, while the weights of PReLU inside bottleneck modules are negative. It means that typical identity shortcut in ResNet does not work well because of the limited depth of the network.</li>
<li><strong>Information-preserving dimensionality changes:</strong> A method of performing pooling operation in parallel with a convolution of stride 2 and concatenating resulting feature maps is used to guarentee efficiency and performance, just as the intial block shows.</li>
<li><strong>Factorizing filters:</strong> Using factorizing technique can achive a kernel of larger size while using less computations. In addition, deeper network and more times of non-linear operation helps simulate richer functions.</li>
<li><strong>Dilated convolutions:</strong> Dilated convolutions is a good way of maintaining feature resolution while boosting efficiency.</li>
<li><strong>Regularization:</strong> Spatial Dropout is used to prevent overfitting.</li>
</ol>
<h3 id="ADVANTAGES"><a href="#ADVANTAGES" class="headerlink" title="ADVANTAGES"></a>ADVANTAGES</h3><ol>
<li>The network processes fast.</li>
</ol>
<h3 id="DISADVANTAGES"><a href="#DISADVANTAGES" class="headerlink" title="DISADVANTAGES"></a>DISADVANTAGES</h3><ol>
<li>The performance is comparatively inferior.</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/08/19/Computer_Vision/Reading_Note/2016-08-19-Reading-note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar-icon.png">
      <meta itemprop="name" content="Joshua LI">
      <meta itemprop="description" content="Do not aim for success if you want it; just do what you love and believe in, and it will come naturally.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Joshua's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/08/19/Computer_Vision/Reading_Note/2016-08-19-Reading-note/" class="post-title-link" itemprop="url">Reading Note: Factorized Convolutional Neural Networks</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-08-19 00:00:00" itemprop="dateCreated datePublished" datetime="2016-08-19T00:00:00+08:00">2016-08-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-08-19 17:11:04" itemprop="dateModified" datetime="2022-08-19T17:11:04+08:00">2022-08-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Computer-Vision/" itemprop="url" rel="index"><span itemprop="name">Computer Vision</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>TITLE</strong>: Factorized Convolutional Neural Networks</p>
<p><strong>AUTHER</strong>: Min Wang, Baoyuan Liu, Hassan Foroosh</p>
<p><strong>ASSOCIATION</strong>: Department of EECS, University of Central Florida, Orlando</p>
<p><strong>FROM</strong>: <a href="http://arxiv.org/abs/1608.04337" target="_blank" rel="noopener">arXiv:1608.04337</a></p>
<h3 id="CONTRIBUTIONS"><a href="#CONTRIBUTIONS" class="headerlink" title="CONTRIBUTIONS"></a>CONTRIBUTIONS</h3><ol>
<li>A new implementation of convolutional layer is proposed and only involves single in-channel convolution and linear channel projection.</li>
<li>The network using such layers can achieves similar accuracy with significantly less computaion.</li>
</ol>
<h3 id="METHOD"><a href="#METHOD" class="headerlink" title="METHOD"></a>METHOD</h3><p><strong>Convolutional Layer with Bases</strong></p>
<p><img class="img-responsive center-block" src="https://raw.githubusercontent.com/joshua19881228/my_blogs/master/Computer_Vision/Reading_Note/figures/alg2.jpg" alt="" width="640"/></p>
<p>When $b = k^2$, this layer is equivalent to the standard convolutional layer. The number of multiplication required for this layer is $hwbm(k^2 + n)$, which means that by reducing b and increasing k, we create a layer that achieves large convolutional kernel while maintaining low complexity.</p>
<p><strong>Convolutional Layer as Stacked Single Basis Layer</strong></p>
<p><img class="img-responsive center-block" src="https://raw.githubusercontent.com/joshua19881228/my_blogs/master/Computer_Vision/Reading_Note/figures/alg3.jpg" alt="" width="640"/></p>
<p>One assumption is that the number of output channels is the same as the number of input channels $m = n$, which is the case of that in ResNet. The modified layer can be considered as stacking multiple convolutional layers with single basis. Residual learning is also introduced in thie modified layer, which solves the problem of losing useful information caused by single basis.</p>
<p><strong>Topological Connections</strong></p>
<p><img class="img-responsive center-block" src="https://raw.githubusercontent.com/joshua19881228/my_blogs/master/Computer_Vision/Reading_Note/figures/alg4.jpg" alt="" width="640"/></p>
<p>A $n$-dimensional topological connections between the input and output channels in convolutional layer is proposed. Each output channel is only connected with its local neighbors rather than all input channels.</p>
<h3 id="ADVANTAGES"><a href="#ADVANTAGES" class="headerlink" title="ADVANTAGES"></a>ADVANTAGES</h3><ol>
<li>It is an interesting method of speeding up CNN as the auther claims that the network achieves accuracy of GoogLeNet while consuming 3.4 times less computaion.</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/08/17/Life_Discovery/Miscellaneous/2016-08-17-Miscellaneous/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar-icon.png">
      <meta itemprop="name" content="Joshua LI">
      <meta itemprop="description" content="Do not aim for success if you want it; just do what you love and believe in, and it will come naturally.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Joshua's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/08/17/Life_Discovery/Miscellaneous/2016-08-17-Miscellaneous/" class="post-title-link" itemprop="url">《精灵守护者》</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-08-17 00:00:00" itemprop="dateCreated datePublished" datetime="2016-08-17T00:00:00+08:00">2016-08-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-08-19 17:11:04" itemprop="dateModified" datetime="2022-08-19T17:11:04+08:00">2022-08-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Life-Discovery/" itemprop="url" rel="index"><span itemprop="name">Life Discovery</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>最近没有怎么看书学习，倒是看了一部2007年上映的日本TV动画——《精灵守护者》，改编自上桥菜穗子的儿童系列小说《守护人·旅人》中的第一部《精灵之守护人》，动画一共26话。作品是世界观很有意思，这个架空世界里有分为两个世界，肉眼所能看到的人类的世界及肉眼看不到的精灵的世界，这两个世界的存在很像平行世界，在特定条件下可以相互产生作用。</p>
<p><img class="img-responsive center-block" src="https://raw.githubusercontent.com/joshua19881228/my_blogs/master/Life_Discovery/Miscellaneous/figures/Guardian.jpg" alt="" width="480"/></p>
<p>在世界观设定中，精灵世界里的水精灵一百年产一次卵，新生的卵在人类世界孕育，孵化后化身为水精灵返回精灵世界。水精灵保证人类世界拥有充足的降水，使得动植物繁荣地繁衍生息，人类社会可以繁荣安康。水精灵的卵在人类世界孕育时会寄生在一种生物个体体内，这个生物个体被称为“精灵守护者”。当孵化临近时，卵会指引精灵守护者来到人类世界与精灵世界的衔接处，此时与水精灵相克的土精灵会来狩猎精灵守护者，只有成功避开土精灵的狩猎才能使水精灵重返精灵世界。</p>
<p>在《精灵守护者》的故事中，二皇子扎克穆被选为精灵守护者，但是由于史书的错误记载，扎克穆被认为被不祥之物附身，遭到皇室的秘密追杀。女保镖巴鲁萨因为机缘巧合成为他的监护人，带着扎克穆在民间谋生躲避皇室的追杀。随着服务于皇室的观星者揭开大旱之兆的原因，逐渐意识到史书的错误，皇室也开始与巴鲁萨一行人开始合作，争取维护扎克穆的生命和人类世界的繁荣。作为根据儿童文学改编的动画，当然是大团圆结局。</p>
<p>B站上的网友戏称这是一部没有反派登场的故事，事实也是如此，故事中的各个人物之间虽然有着各种各样的矛盾，不管是各为其主还是历史偏见，但各个角色都拥有绝对正派的世界观。观众可以从各个角色身上体察到一种正向的人格，比如二皇子扎克穆的勇敢善良，女保镖巴鲁萨对誓言和生命的信仰，观星者修伽对真理的追求，扎克穆母亲二之妃的母爱，巴鲁萨养父吉古洛对知己的忠诚……看完之后会让人有一种充满正能量的感觉。除了这些正能量，动画中的伏笔比比皆是，任何一处细节都会在后续的故事中发挥作用，每每将这些呼应串联起来的时候，都会让人大呼过瘾。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/08/09/Computer_Vision/Reading_Note/2016-08-09-Reading-Note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar-icon.png">
      <meta itemprop="name" content="Joshua LI">
      <meta itemprop="description" content="Do not aim for success if you want it; just do what you love and believe in, and it will come naturally.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Joshua's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/08/09/Computer_Vision/Reading_Note/2016-08-09-Reading-Note/" class="post-title-link" itemprop="url">Reading Note: Do semantic parts emerge in Convolutional Neural Networks</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-08-09 00:00:00" itemprop="dateCreated datePublished" datetime="2016-08-09T00:00:00+08:00">2016-08-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-08-19 17:11:04" itemprop="dateModified" datetime="2022-08-19T17:11:04+08:00">2022-08-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Computer-Vision/" itemprop="url" rel="index"><span itemprop="name">Computer Vision</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>TITLE</strong>: Do semantic parts emerge in Convolutional Neural Networks?</p>
<p><strong>AUTHER</strong>: Abel Gonzalez-Garica, David Modolo, Vittorio Ferrari</p>
<p><strong>ASSOCIATION</strong>: CLAVIN, University of Edingburgh, UK</p>
<p><strong>FROM</strong>: <a href="http://arxiv.org/abs/1607.03738" target="_blank" rel="noopener">arXiv:1607.03738</a></p>
<h3 id="CONTRIBUTIONS"><a href="#CONTRIBUTIONS" class="headerlink" title="CONTRIBUTIONS"></a>CONTRIBUTIONS</h3><ol>
<li>An extensive quantitative analysis of the association between responses of CNN filters and sematic parts </li>
</ol>
<h3 id="METHOD"><a href="#METHOD" class="headerlink" title="METHOD"></a>METHOD</h3><ol>
<li>CNNs are trained for object detection task or object classification.</li>
<li>Filters that give significant responses to certain semantic parts are selected.</li>
<li>Filters are comibned to construct a part detector if necessary.</li>
<li>A regressor is trained for part bounding-boxes.</li>
<li>Discriminative filters are selected in object classification task.</li>
</ol>
<h3 id="Observation"><a href="#Observation" class="headerlink" title="Observation"></a>Observation</h3><p>There are several interesting observatoins from the authers.</p>
<p><strong>Differences between layers.</strong> Overall, the higher the network layer, the higher the performance. It means that in higher part of the network abstract semantic contents are represented.</p>
<p><strong>Differences between part classes.</strong> Performance varies greatly across part classes. It seems that very discriminative semantic parts are well detected.</p>
<p><strong>Filter combinations.</strong> Performing part detection using a combination of filters always performs better than single best filter. It means taht a semantic part may be represented jointly by several filters.</p>
<p><strong>Filter sharing across part classes.</strong> Filters are shared across different part classes. It is clear that some filters are representative for a generic part and work well on all object classes containing it.</p>
<p><strong>The number of emerged semantic parts.</strong> Only a modest number of filters responses to semantic parts. The auther concludes that the network does contain filters combinations that can cover some part classes well, but they do not fire exclusively on the part, making them weak part detectors. Moreover, the part classes covered by the semantic filters tend to either cover a large image area, or be very discriminative for their object class.</p>
<p><strong>Discriminative filters in object classification.</strong> The filters are measured by how much they contribute to the classification score. On average, 9/256 filters are discriminative for a particular class. The total number of dicriminative filte overall 16 object classes amounts to 104. It shows that the discriminative filters are largely distributed across different object classes, with very little sharing.</p>
<p><strong>Discriminative and semantic filters.</strong> 5.5 out of the 9 discriminative filters for an object class are semantic filters.It means that only a portion of the filters learned by CNN are semantic, and many are just responding to dicriminative patches.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/16/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><span class="page-number current">17</span><a class="page-number" href="/page/18/">18</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="extend next" rel="next" href="/page/18/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Joshua LI"
      src="/images/avatar-icon.png">
  <p class="site-author-name" itemprop="name">Joshua LI</p>
  <div class="site-description" itemprop="description">Do not aim for success if you want it; just do what you love and believe in, and it will come naturally.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">239</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/joshua19881228" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;joshua19881228" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhixuan.1988.li@gmail.com" title="E-Mail → mailto:zhixuan.1988.li@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/joshua1988" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;joshua1988" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Joshua LI</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


</body>
</html>
